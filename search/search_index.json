{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CODATA/RDA Schools for Research Data Science Materials from DataKigali 2018 Materials from DataSaoPaulo 2018 Materials from DataTrieste 2019 Materials from DataSanJose 2019","title":"Home"},{"location":"#codatarda-schools-for-research-data-science","text":"Materials from DataKigali 2018 Materials from DataSaoPaulo 2018 Materials from DataTrieste 2019 Materials from DataSanJose 2019","title":"CODATA/RDA Schools for Research Data Science"},{"location":"DataKigali2018/","text":"Kigali Foundational School in Research Data Science 22 October \u2013 2 November, 2018 University of Rwanda, Kigali, Rwanda Background and purpose The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, ICTP and EAIFR, was conducted at the University of Rwanda, Kigali to introduce participants to the skills of RDS. Materials for the 2018 School of Research Data Science in Kigali Day 1 - Introduction, Open Science , UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R Day 4 - Research Data Management Day 5 - Research Data Management , Open Science Day 6 - Data Visualization Day 7 - Overview of Machine Learning - 20of 20Machine 20Learning 202018.pdf\">Fundamentals , 20Systems 202018.pdf\">Recommender Systems Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Day 10 - Computational Infrastructures Wrap-Up - Lecture 4 , School Close Out","title":"Kigali Foundational School in Research Data Science"},{"location":"DataKigali2018/#kigali-foundational-school-in-research-data-science","text":"22 October \u2013 2 November, 2018 University of Rwanda, Kigali, Rwanda","title":"Kigali Foundational School in Research Data Science"},{"location":"DataKigali2018/#background-and-purpose","text":"The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, ICTP and EAIFR, was conducted at the University of Rwanda, Kigali to introduce participants to the skills of RDS.","title":"Background and purpose"},{"location":"DataKigali2018/#materials-for-the-2018-school-of-research-data-science-in-kigali","text":"Day 1 - Introduction, Open Science , UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R Day 4 - Research Data Management Day 5 - Research Data Management , Open Science Day 6 - Data Visualization Day 7 - Overview of Machine Learning - 20of 20Machine 20Learning 202018.pdf\">Fundamentals , 20Systems 202018.pdf\">Recommender Systems Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Day 10 - Computational Infrastructures Wrap-Up - Lecture 4 , School Close Out","title":"Materials for the 2018 School of Research Data Science in Kigali"},{"location":"DataKigali2018/slides/readme/","text":"Slides folder","title":"Readme"},{"location":"DataKigali2018/slides/DataVizMaterials/readme/","text":"download the folder for data visualization materials","title":"Readme"},{"location":"DataSanJose2019/","text":"San Jos\u00e9 School of Research Data Science December 2 \u2013 13, 2018 CeNAT, San Jos\u00e9, Costa Rica Background and purpose The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, CONARE and CeNAT, was conducted at the CeNAT, San Jos\u00e9 to introduce participants to the skills of RDS. Materials for the 2019 School of Research Data Science in San Jos\u00e9 Day 1 - Introduction , Open Science , UNIX Shell Day 2 - Version Control with Git , Introduction to Python Day 3 - Introduction to Python Day 4 - Author Carpentry , Intro to Research Data Management Day 5 - Research Data Management , Data Management Plans Day 6 - 20using 20Seaborn.md\">Data Visualization Day 6 - Open Science , Information Security Day 7 - Overview of Machine Learning Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Lecture 4 Day 10 - Computational Infrastructures Wrap-Up - Lecture 5 , Lecture 6 School Close Out","title":"San Jos\u00e9 School of Research Data Science"},{"location":"DataSanJose2019/#san-jose-school-of-research-data-science","text":"December 2 \u2013 13, 2018 CeNAT, San Jos\u00e9, Costa Rica","title":"San Jos\u00e9 School of Research Data Science"},{"location":"DataSanJose2019/#background-and-purpose","text":"The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, CONARE and CeNAT, was conducted at the CeNAT, San Jos\u00e9 to introduce participants to the skills of RDS.","title":"Background and purpose"},{"location":"DataSanJose2019/#materials-for-the-2019-school-of-research-data-science-in-san-jose","text":"Day 1 - Introduction , Open Science , UNIX Shell Day 2 - Version Control with Git , Introduction to Python Day 3 - Introduction to Python Day 4 - Author Carpentry , Intro to Research Data Management Day 5 - Research Data Management , Data Management Plans Day 6 - 20using 20Seaborn.md\">Data Visualization Day 6 - Open Science , Information Security Day 7 - Overview of Machine Learning Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Lecture 4 Day 10 - Computational Infrastructures Wrap-Up - Lecture 5 , Lecture 6 School Close Out","title":"Materials for the 2019 School of Research Data Science in San Jos\u00e9"},{"location":"DataSanJose2019/CI/00-Pre-Introduction-Login/","text":"Install an SSH-Cleint Download and install a SSH Client. We recommend PuTTY, but any ssh client is acceptable. https://www.putty.org/ Direct link to executable After opening the executable you should see the following screen: Authentication for CI Exercises You will receive login credentials at the beginning of this session. To authenticate (prove you who you say you are and establish what you are authorized to do) the bridgekeeper (login nodes) requires three bits of information: From Monty Python and the Holy Grail: BRIDGEKEEPER: Hee hee heh. Stop! What... is your name?\\ ARTHUR: It is 'Arthur', King of the Britons.\\ BRIDGEKEEPER: What... is your quest?\\ ARTHUR: To seek the Holy Grail.\\ BRIDGEKEEPER: What... is the air-speed velocity of an unladen swallow?\\ ARTHUR: What do you mean? An African or European swallow?\\ BRIDGEKEEPER: Huh? I-- I don't know that. Auuuuuuuugh!\\ BEDEVERE: How do know so much about swallows?\\ ARTHUR: Well, you have to know these things when you're a king, you know. or... 1) Tell me who you are. 2) Tell me something only you know. 3) Show me someething only you have. Why do you need both a password and a key? What is the role of the password in the public-private key scheme? Where you will work You will be logging into training.osgconnect.net for the CyberInfrastructure exercises. To confirm you have the proper authentication and authorization to do the exercises tomorrow and Friday we will test logins today. Due to the local network firewall setup (another security mechanism) and key installation, we will go to Brazil first (thanks to Raphael for setting up temporary VM). First be sure you are on the wireless network Eventos CeNAT . Replace XX with your osguser ID and use the password you have been supplied with the following command. ssh -o PreferredAuthentications=password osguserXX@200.145.46.31 If you are using putty, you should fill the Host Name (or IP address) with the value 200.145.46.31 as seen below: After hitting the Open button you may see the following message: You should hit the Yes button. Login on our submission node using the following command along with the password you have been supplied. $ ssh training.osgconnect.net The authenticity of host 'training.osgconnect.net (128.135.158.220)' can't be established. ECDSA key fingerprint is SHA256:gielJSpIiZisxGna5ocHtiK+0zAqFTdcEkLBOgnDUsg. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added 'training.osgconnect.net,128.135.158.220' (ECDSA) to the list of known hosts. Enter passphrase for key '/home/osguser01/.ssh/id_rsa': You may get a message asking you to establish the authenticity of this connection. Answer \"yes\". When you login to the machine you will be in your \"home directory\". We recommend that you work in this directory as nobody else can modify the files here (what security concept we covered today does this recommendation satisfy?).","title":"00 Pre Introduction Login"},{"location":"DataSanJose2019/CI/00-Pre-Introduction-Login/#install-an-ssh-cleint","text":"Download and install a SSH Client. We recommend PuTTY, but any ssh client is acceptable. https://www.putty.org/ Direct link to executable After opening the executable you should see the following screen:","title":"Install an SSH-Cleint"},{"location":"DataSanJose2019/CI/00-Pre-Introduction-Login/#authentication-for-ci-exercises","text":"You will receive login credentials at the beginning of this session. To authenticate (prove you who you say you are and establish what you are authorized to do) the bridgekeeper (login nodes) requires three bits of information: From Monty Python and the Holy Grail: BRIDGEKEEPER: Hee hee heh. Stop! What... is your name?\\ ARTHUR: It is 'Arthur', King of the Britons.\\ BRIDGEKEEPER: What... is your quest?\\ ARTHUR: To seek the Holy Grail.\\ BRIDGEKEEPER: What... is the air-speed velocity of an unladen swallow?\\ ARTHUR: What do you mean? An African or European swallow?\\ BRIDGEKEEPER: Huh? I-- I don't know that. Auuuuuuuugh!\\ BEDEVERE: How do know so much about swallows?\\ ARTHUR: Well, you have to know these things when you're a king, you know. or... 1) Tell me who you are. 2) Tell me something only you know. 3) Show me someething only you have. Why do you need both a password and a key? What is the role of the password in the public-private key scheme?","title":"Authentication for CI Exercises"},{"location":"DataSanJose2019/CI/00-Pre-Introduction-Login/#where-you-will-work","text":"You will be logging into training.osgconnect.net for the CyberInfrastructure exercises. To confirm you have the proper authentication and authorization to do the exercises tomorrow and Friday we will test logins today. Due to the local network firewall setup (another security mechanism) and key installation, we will go to Brazil first (thanks to Raphael for setting up temporary VM). First be sure you are on the wireless network Eventos CeNAT . Replace XX with your osguser ID and use the password you have been supplied with the following command. ssh -o PreferredAuthentications=password osguserXX@200.145.46.31 If you are using putty, you should fill the Host Name (or IP address) with the value 200.145.46.31 as seen below: After hitting the Open button you may see the following message: You should hit the Yes button. Login on our submission node using the following command along with the password you have been supplied. $ ssh training.osgconnect.net The authenticity of host 'training.osgconnect.net (128.135.158.220)' can't be established. ECDSA key fingerprint is SHA256:gielJSpIiZisxGna5ocHtiK+0zAqFTdcEkLBOgnDUsg. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added 'training.osgconnect.net,128.135.158.220' (ECDSA) to the list of known hosts. Enter passphrase for key '/home/osguser01/.ssh/id_rsa': You may get a message asking you to establish the authenticity of this connection. Answer \"yes\". When you login to the machine you will be in your \"home directory\". We recommend that you work in this directory as nobody else can modify the files here (what security concept we covered today does this recommendation satisfy?).","title":"Where you will work"},{"location":"DataSanJose2019/CI/01-Introduction/","text":"High Throughput Computing and Condor Introduction Preliminaries You will receive login credentials at the beginning of this session. You might want to refer to the online Condor 8.6.13 manual . You may enjoy browsing the Condor web page . Which Condor? We will be using Condor 8.6.13, which is a recent production version of Condor. Condor has two coexisting types of releases at any given time: stable and development. Condor 8.2.X and 7.8.x are considered stable releases, and you can know they are stable because the second digits (a 2 or a 8 in these cases) are even numbers. In a given stable series, all versions have the same features (for example 7.8.0 and 7.8.1 have the same set of features) and differ only in bug fixes. Where you will work Today you will log into training.osgconnect.net for all of your exercises. Login on submission node using the directions from yesterdays security session. They can be found here . When you login to the machine you will be in your \"home directory\". We recommend that you work in this directory as nobody else can modify the files here. You can always return to your home directory by running the command $ cd ~ The Exercises Throughout the Condor exercises, you will be given a fair amount of guidance. In several spots, there are suggestions for extra exercises to do \"on your own\" or as \"challenges\". Since you aren't being graded, there is no extra credit for doing them, but we encourage you to try them out. If you prefer, you can come back to the extra credit after you've completed the basic exercises. If you simply cruise through the exercises, you'll probably have free time--we encourage you to delve in more deeply. For all of the exercises, we'll assume that you are logged into user-training.osgconnect.net. You should have received your name and password for user-training.osgconnect.net at the beginning of the Computation Infrastructures lecture.","title":"High Throughput Computing and Condor Introduction"},{"location":"DataSanJose2019/CI/01-Introduction/#high-throughput-computing-and-condor-introduction","text":"","title":"High Throughput Computing and Condor Introduction"},{"location":"DataSanJose2019/CI/01-Introduction/#preliminaries","text":"You will receive login credentials at the beginning of this session. You might want to refer to the online Condor 8.6.13 manual . You may enjoy browsing the Condor web page .","title":"Preliminaries"},{"location":"DataSanJose2019/CI/01-Introduction/#which-condor","text":"We will be using Condor 8.6.13, which is a recent production version of Condor. Condor has two coexisting types of releases at any given time: stable and development. Condor 8.2.X and 7.8.x are considered stable releases, and you can know they are stable because the second digits (a 2 or a 8 in these cases) are even numbers. In a given stable series, all versions have the same features (for example 7.8.0 and 7.8.1 have the same set of features) and differ only in bug fixes.","title":"Which Condor?"},{"location":"DataSanJose2019/CI/01-Introduction/#where-you-will-work","text":"Today you will log into training.osgconnect.net for all of your exercises. Login on submission node using the directions from yesterdays security session. They can be found here . When you login to the machine you will be in your \"home directory\". We recommend that you work in this directory as nobody else can modify the files here. You can always return to your home directory by running the command $ cd ~","title":"Where you will work"},{"location":"DataSanJose2019/CI/01-Introduction/#the-exercises","text":"Throughout the Condor exercises, you will be given a fair amount of guidance. In several spots, there are suggestions for extra exercises to do \"on your own\" or as \"challenges\". Since you aren't being graded, there is no extra credit for doing them, but we encourage you to try them out. If you prefer, you can come back to the extra credit after you've completed the basic exercises. If you simply cruise through the exercises, you'll probably have free time--we encourage you to delve in more deeply. For all of the exercises, we'll assume that you are logged into user-training.osgconnect.net. You should have received your name and password for user-training.osgconnect.net at the beginning of the Computation Infrastructures lecture.","title":"The Exercises"},{"location":"DataSanJose2019/CI/02-OurJobManager/","text":"Our Condor Installation Objective This exercise should help you understand the basics of how Condor is installed, what Condor processes (a.k.a. daemons) are running, and what they do. Login to the Condor submit computer Before you start, make sure you are logged into user-training.osgconnect.net $ hostname training.osgconnect.net You should have been given your name and password when you arrived this afternoon. If you don't know them, talk to Rob. Looking at our Condor installation How do you know what version of Condor you are using? Try condor_version : $ condor_version $CondorVersion: 8.6.13 Jan 16 2019 $ $CondorPlatform: X86_64-CentOS_7.6 $ Note that the \"CondorPlatform\" reports the type of computer we built it on, not the computer we're running on. It was built on CentOS_6.8, but you might notice that we're running on Scientific Linux 6.8, which is a free clone of Red Hat Enterprise Linux. Extra Tip: The OS version Do you know how to find the OS version? You can usually look in /etc/os-release to find out: $ cat /etc/os-release Or you can run: $ hostnamectl Where is Condor installed? # Show the location of the condor_q binary $ which condor_q /usr/bin/condor_q # Show which RPM installed Condor $ rpm -q condor condor-8.6.11-1.osg34.el6.x86_64 Condor has some configuration files that it needs to find. They are in the standard location, /etc/condor $ ls /etc/condor condor_config condor_ssh_to_job_sshd_config_template ganglia.d condor_config.local config.d Condor has some directories that it keeps records of jobs in. Remember that each submission computer keeps track of all jobs submitted to it. That's in the local directory: $ condor_config_val -v LOCAL_DIR LOCAL_DIR = /var # at: /etc/condor/condor_config, line 26 # raw: LOCAL_DIR = /var $ ls -CF /var/lib/condor execute/ spool/ spool.q1/ spool.q2/ spool.q3/ spool.q4/ spool.q5/ The spool directory is where Condor keeps the jobs you submit, while the execute directory is where Condor keeps running jobs. Since this is a submission-only computer, it should be empty. Check if Condor is running. Your output will differ slightly, but you should see condor_master with the other Condor daemons listed under it: $ ps auwx --forest | grep condor_ | grep -v grep condor 2299245 0.0 0.1 50972 7348 ? Ss Jul10 0:08 condor_master -pidfile /var/run/condor/condor_master.pid root 2299287 0.0 0.1 25924 5072 ? S Jul10 1:54 \\_ condor_procd -A /var/run/condor/procd_pipe -L /var/log/condor/ProcLog -R 1000000 -S 60 -C 499 condor 2299288 0.0 0.1 50596 7796 ? Ss Jul10 0:16 \\_ condor_shared_port -f condor 2299289 0.0 0.2 70020 9100 ? Ss Jul10 0:13 \\_ condor_collector -f condor 2299290 0.0 0.5 116132 23872 ? Ss Jul10 6:19 \\_ condor_schedd -f condor 2299291 0.0 0.1 51056 7956 ? Ss Jul10 0:59 \\_ condor_negotiator -f For this version of Condor there are four processes running: the condor_master, the condor_schedd, the condor_procd, and condor_schedd. In general, you might see many different Condor processes. Here's a list of the processes: condor_master : This program runs constantly and ensures that all other parts of Condor are running. If they hang or crash, it restarts them. condor_schedd : If this program is running, it allows jobs to be submitted from this computer--that is, your computer is a \"submit machine\". This will advertise jobs to the central manager so that it knows about them. It will contact a condor_startd on other execute machines for each job that needs to be started. condor_procd: This process helps Condor track process (from jobs) that it creates condor_collector: This program is part of the Condor central manager. It collects information about all computers in the pool as well as which users want to run jobs. It is what normally responds to the condor_status command. At the school, it is running on a different computer, and you can figure out which one: Other daemons include: condor_negotiator: This program is part of the Condor central manager. It decides what jobs should be run where. It is run on the same computer as the collector. condor_startd: If this program is running, it allows jobs to be started up on this computer--that is, your computer is an \"execute machine\". This advertises your computer to the central manager so that it knows about this computer. It will start up the jobs that run. condor_shadow: For each job that has been submitted from this computer, there is one condor_shadow running. It will watch over the job as it runs remotely. In some cases it will provide some assistance (see the standard universe later.) You may or may not see any condor_shadow processes running, depending on what is happening on the computer when you try it out. condor_shared_port: Used to assist Condor with networking by allowing multiple Condor processes to share a single network port. condor_q You can find out what jobs have been submitted on your computer with the condor_q command: $ condor_q -- Schedd: user-training.osgconnect.net : <128.135.158.195:9618?... @ 08/12/18 16:10:58 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE HOLD TOTAL JOB_IDS 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended The output that you see will be different depending on what jobs are running. Notice what we can see from this: ID : We can see each jobs cluster and process number. For the first job, the cluster is 60256 and the process is 0. OWNER : We can see who owns the job. SUBMITTED : We can see when the job was submitted RUN_TIME : We can see how long the job has been running. ST : We can see what the current state of the job is. I is idle, R is running. PRI : We can see the priority of the job. SIZE : We can see the memory consumption of the job. CMD : We can see the program that is being executed. Extra Tip What else can you find out with condor_q? Try any one of: man condor_q condor_q -help condor_q from the online manual Double bonus points How do you use the -constraint or -format options to condor_q ? When would you want them? When would you use the -l option? This might be an easier exercise to try once you submit some jobs. condor_status You can find out what computers are in your Condor pool. (A pool is similar to a cluster, but it doesn't have the connotation that all computers are dedicated full-time to computation: some may be desktop computers owned by users.) To look, use condor_status: $ condor_status -pool flock.opensciencegrid.org Name OpSys Arch State Activity LoadAv Mem ActvtyTime slot1@amundsen.grid.uchicago.edu LINUX X86_64 Owner Idle 0.000 32768 1+02:46:31 slot2@amundsen.grid.uchicago.edu LINUX X86_64 Owner Idle 0.000 32768 5+01:05:58 slot1@c2 LINUX X86_64 Unclaimed Idle 0.000 48289 3+10:04:49 slot1@dhcp-10-1-202-3 LINUX X86_64 Unclaimed Idle 0.000 3251 0+08:10:13 slot1_1@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 6144 0+01:09:46 slot1_2@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 6144 0+00:46:46 slot1_3@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 2048 0+00:53:08 slot1_4@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 1024 0+05:48:14 slot1_5@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.000 6144 0+00:16:48 slot1_6@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 2816 0+13:16:34 ... Let's look at exactly what you can see: Name : The name of the computer. Sometimes this gets chopped off, like above. OpSys : The operating system, though not at the granularity you may wish: It says \"Linux\" instead of which distribution and version of Linux. Arch : The architecture, such as INTEL or PPC. State : The state is often Claimed (when it is running a Condor job) or Unclaimed (when it is not running a Condor job). It can be in a few other states as well, such as Matched. Activity : This is usually something like Busy or Idle. Sometimes you may see a computer that is Claimed, but no job has yet begun on the computer. Then it is Claimed/Idle. Hopefully this doesn't last very long. LoadAv : The load average on the computer. Mem : The computers memory in megabytes. ActvtyTime : How long the computer has been doing what it's been doing. Extra credit What else can you find out with condor_status? Try any one of: man condor_status condor_status -help condor_status from the online manual Note in particular the options like -master and -schedd . When would these be useful? When would the -l option be useful?","title":"Our Condor Installation"},{"location":"DataSanJose2019/CI/02-OurJobManager/#our-condor-installation","text":"","title":"Our Condor Installation"},{"location":"DataSanJose2019/CI/02-OurJobManager/#objective","text":"This exercise should help you understand the basics of how Condor is installed, what Condor processes (a.k.a. daemons) are running, and what they do.","title":"Objective"},{"location":"DataSanJose2019/CI/02-OurJobManager/#login-to-the-condor-submit-computer","text":"Before you start, make sure you are logged into user-training.osgconnect.net $ hostname training.osgconnect.net You should have been given your name and password when you arrived this afternoon. If you don't know them, talk to Rob.","title":"Login to the Condor submit computer"},{"location":"DataSanJose2019/CI/02-OurJobManager/#looking-at-our-condor-installation","text":"How do you know what version of Condor you are using? Try condor_version : $ condor_version $CondorVersion: 8.6.13 Jan 16 2019 $ $CondorPlatform: X86_64-CentOS_7.6 $ Note that the \"CondorPlatform\" reports the type of computer we built it on, not the computer we're running on. It was built on CentOS_6.8, but you might notice that we're running on Scientific Linux 6.8, which is a free clone of Red Hat Enterprise Linux.","title":"Looking at our Condor installation"},{"location":"DataSanJose2019/CI/02-OurJobManager/#extra-tip-the-os-version","text":"Do you know how to find the OS version? You can usually look in /etc/os-release to find out: $ cat /etc/os-release Or you can run: $ hostnamectl Where is Condor installed? # Show the location of the condor_q binary $ which condor_q /usr/bin/condor_q # Show which RPM installed Condor $ rpm -q condor condor-8.6.11-1.osg34.el6.x86_64 Condor has some configuration files that it needs to find. They are in the standard location, /etc/condor $ ls /etc/condor condor_config condor_ssh_to_job_sshd_config_template ganglia.d condor_config.local config.d Condor has some directories that it keeps records of jobs in. Remember that each submission computer keeps track of all jobs submitted to it. That's in the local directory: $ condor_config_val -v LOCAL_DIR LOCAL_DIR = /var # at: /etc/condor/condor_config, line 26 # raw: LOCAL_DIR = /var $ ls -CF /var/lib/condor execute/ spool/ spool.q1/ spool.q2/ spool.q3/ spool.q4/ spool.q5/ The spool directory is where Condor keeps the jobs you submit, while the execute directory is where Condor keeps running jobs. Since this is a submission-only computer, it should be empty. Check if Condor is running. Your output will differ slightly, but you should see condor_master with the other Condor daemons listed under it: $ ps auwx --forest | grep condor_ | grep -v grep condor 2299245 0.0 0.1 50972 7348 ? Ss Jul10 0:08 condor_master -pidfile /var/run/condor/condor_master.pid root 2299287 0.0 0.1 25924 5072 ? S Jul10 1:54 \\_ condor_procd -A /var/run/condor/procd_pipe -L /var/log/condor/ProcLog -R 1000000 -S 60 -C 499 condor 2299288 0.0 0.1 50596 7796 ? Ss Jul10 0:16 \\_ condor_shared_port -f condor 2299289 0.0 0.2 70020 9100 ? Ss Jul10 0:13 \\_ condor_collector -f condor 2299290 0.0 0.5 116132 23872 ? Ss Jul10 6:19 \\_ condor_schedd -f condor 2299291 0.0 0.1 51056 7956 ? Ss Jul10 0:59 \\_ condor_negotiator -f For this version of Condor there are four processes running: the condor_master, the condor_schedd, the condor_procd, and condor_schedd. In general, you might see many different Condor processes. Here's a list of the processes: condor_master : This program runs constantly and ensures that all other parts of Condor are running. If they hang or crash, it restarts them. condor_schedd : If this program is running, it allows jobs to be submitted from this computer--that is, your computer is a \"submit machine\". This will advertise jobs to the central manager so that it knows about them. It will contact a condor_startd on other execute machines for each job that needs to be started. condor_procd: This process helps Condor track process (from jobs) that it creates condor_collector: This program is part of the Condor central manager. It collects information about all computers in the pool as well as which users want to run jobs. It is what normally responds to the condor_status command. At the school, it is running on a different computer, and you can figure out which one: Other daemons include: condor_negotiator: This program is part of the Condor central manager. It decides what jobs should be run where. It is run on the same computer as the collector. condor_startd: If this program is running, it allows jobs to be started up on this computer--that is, your computer is an \"execute machine\". This advertises your computer to the central manager so that it knows about this computer. It will start up the jobs that run. condor_shadow: For each job that has been submitted from this computer, there is one condor_shadow running. It will watch over the job as it runs remotely. In some cases it will provide some assistance (see the standard universe later.) You may or may not see any condor_shadow processes running, depending on what is happening on the computer when you try it out. condor_shared_port: Used to assist Condor with networking by allowing multiple Condor processes to share a single network port.","title":"Extra Tip: The OS version"},{"location":"DataSanJose2019/CI/02-OurJobManager/#condor_q","text":"You can find out what jobs have been submitted on your computer with the condor_q command: $ condor_q -- Schedd: user-training.osgconnect.net : <128.135.158.195:9618?... @ 08/12/18 16:10:58 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE HOLD TOTAL JOB_IDS 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended The output that you see will be different depending on what jobs are running. Notice what we can see from this: ID : We can see each jobs cluster and process number. For the first job, the cluster is 60256 and the process is 0. OWNER : We can see who owns the job. SUBMITTED : We can see when the job was submitted RUN_TIME : We can see how long the job has been running. ST : We can see what the current state of the job is. I is idle, R is running. PRI : We can see the priority of the job. SIZE : We can see the memory consumption of the job. CMD : We can see the program that is being executed.","title":"condor_q"},{"location":"DataSanJose2019/CI/02-OurJobManager/#extra-tip","text":"What else can you find out with condor_q? Try any one of: man condor_q condor_q -help condor_q from the online manual","title":"Extra Tip"},{"location":"DataSanJose2019/CI/02-OurJobManager/#double-bonus-points","text":"How do you use the -constraint or -format options to condor_q ? When would you want them? When would you use the -l option? This might be an easier exercise to try once you submit some jobs.","title":"Double bonus points"},{"location":"DataSanJose2019/CI/02-OurJobManager/#condor_status","text":"You can find out what computers are in your Condor pool. (A pool is similar to a cluster, but it doesn't have the connotation that all computers are dedicated full-time to computation: some may be desktop computers owned by users.) To look, use condor_status: $ condor_status -pool flock.opensciencegrid.org Name OpSys Arch State Activity LoadAv Mem ActvtyTime slot1@amundsen.grid.uchicago.edu LINUX X86_64 Owner Idle 0.000 32768 1+02:46:31 slot2@amundsen.grid.uchicago.edu LINUX X86_64 Owner Idle 0.000 32768 5+01:05:58 slot1@c2 LINUX X86_64 Unclaimed Idle 0.000 48289 3+10:04:49 slot1@dhcp-10-1-202-3 LINUX X86_64 Unclaimed Idle 0.000 3251 0+08:10:13 slot1_1@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 6144 0+01:09:46 slot1_2@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 6144 0+00:46:46 slot1_3@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 2048 0+00:53:08 slot1_4@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 1024 0+05:48:14 slot1_5@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.000 6144 0+00:16:48 slot1_6@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 2816 0+13:16:34 ... Let's look at exactly what you can see: Name : The name of the computer. Sometimes this gets chopped off, like above. OpSys : The operating system, though not at the granularity you may wish: It says \"Linux\" instead of which distribution and version of Linux. Arch : The architecture, such as INTEL or PPC. State : The state is often Claimed (when it is running a Condor job) or Unclaimed (when it is not running a Condor job). It can be in a few other states as well, such as Matched. Activity : This is usually something like Busy or Idle. Sometimes you may see a computer that is Claimed, but no job has yet begun on the computer. Then it is Claimed/Idle. Hopefully this doesn't last very long. LoadAv : The load average on the computer. Mem : The computers memory in megabytes. ActvtyTime : How long the computer has been doing what it's been doing.","title":"condor_status"},{"location":"DataSanJose2019/CI/02-OurJobManager/#extra-credit","text":"What else can you find out with condor_status? Try any one of: man condor_status condor_status -help condor_status from the online manual Note in particular the options like -master and -schedd . When would these be useful? When would the -l option be useful?","title":"Extra credit"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/","text":"Submitting your first Condor job Objective The objective of this exercise to have you run and understand your first Condor job, as well as run small sets of jobs in a parameter sweep. This is an important exercise because it is the basis for everything that follows. If there is anything you don't understand in this exercise, please ask before you continue on. Because this is an important foundation, please seriously consider doing the \u201cOn Your Own\u201d section. First you need a job Before you can submit a job to Condor, you need a job. We will quickly write a small program in C. If you aren't an expert C programmer, fear not. We will hold your hand throughout this process. Create a file called simple.c using your favorite editor. Put it anywhere you like in your home directory. In that file, put the following text. Copy and paste is a good choice: $ mkdir -p ~/condor-test $ cd ~/condor-test Use your preferred text editor to create this C program. (Shown below with nano.) $ nano simple.c Paste in the following C code. #include <stdio.h> int main(int argc, char **argv) { int sleep_time; int input; int failure; if (argc != 3) { printf(\"Usage: simple &lt;sleep-time&gt; &lt;integer&gt;\\n\"); failure = 1; } else { sleep_time = atoi(argv[1]); input = atoi(argv[2]); printf(\"Thinking really hard for %d seconds...\\n\", sleep_time); sleep(sleep_time); printf(\"We calculated: %d\\n\", input * 2); failure = 0; } return failure; } Now compile that program: $ gcc -o simple simple.c $ ls -lh simple -rwxrwxr-x 1 roy roy 595K Jun 20 11:12 simple Finally, run the program and tell it to sleep for four seconds and calculate 10 * 2: $ ./simple 4 10 Thinking really hard for 4 seconds... We calculated: 20 Great! You just had a job run locally on the machine you are logged into (user-training.osgconnect.net). The next step is to run this job on a remote computer - and this is a job you can tell Condor to run! Although it clearly isn't an interesting job, it models some of the aspects of a real scientific program: it takes a while to run and it does a calculation. Think back to the lecture. I said that our first step was to have a job to run. Now we'll work on running it in Condor, and eventually running lots of copies of it. Submitting your job Now that you have a job, you just have to tell Condor to run it. Put the following text into a file called submit : Universe = vanilla Executable = simple Arguments = 4 10 +ProjectName = \"ConnectTrain\" Log = simple.log Output = simple.out Error = simple.error requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ON_EXIT Queue Let's examine each of these lines: Universe: The vanilla universe means a plain old job. Later on, we'll encounter some special universes. Executable: The name of your program Arguments: These are the arguments you want. They will be the same arguments we typed above. Log: This is the name of a file where Condor will record information about your job's execution. While it's not required, it is a really good idea to have a log. If something goes wrong you can refer to this log to help figure out the problem. Output: Where Condor should put the standard output from your job. Error: Where Condor should put the standard error from your job. Our job isn't likely to have any, but we'll put it there to be safe. should_transfer_files: Tell Condor that it should transfer files, instead of relying on a shared filesystem. While your home directories (on the glite-tutor computers) are mounted on NFS, you do not have user accounts on the worker nodes, so your jobs cannot access files on NFS. In addition, NFS isn't available between the local UI computers and the remote worker nodes. Therefore we will have Condor transfer files to the remote computer. when_to_transfer_output: A technical detail about when files should be transported back to the computer from which you submitted your job. Don't worry about the details for now. If you're really curious, you can read all the details in the Condor manual . Next, tell Condor to run your job: $ condor_submit submit Submitting job(s). 1 job(s) submitted to cluster 16. Now, watch your job run (insert your username in the command below instead of USER . If you forgot your username use the whoami command. Note that most of your output will be different than the example, the important column to watch is the ST column - the job state): # Note the job state of 'I' means the job is idle - not yet running $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:08 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 2056.0 osguser99 7/19 03:40 0+00:00:00 I 0 0.0 simple 4 10 Total for query: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended Total for all users: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended # After some time your job will enter the 'R' state which means it is currently running $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:14 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 2056.0 osguser99 7/19 03:40 0+00:00:02 R 0 0.0 simple 4 10 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended # When your job disappears from the queue that means it completed. $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:21 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD Total for query: 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Total for all users: 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Tip : While you are waiting for your job to run and complete you can check out \"A few tips and tricks\" to learn how to user condor_q more effectively. When my job was done, it was no longer listed. Because I told Condor to log information about my job, I can see what happened: $ cat simple.log 000 (032.000.000) 08/18 15:18:13 Job submitted from host: <10.0.0.252:9645> ... 001 (032.000.000) 08/18 15:18:32 Job executing on host: <172.16.200.1:9250> ... 006 (032.000.000) 08/18 15:18:32 Image size of job updated: 7 0 - MemoryUsage of job (MB) 0 - ResidentSetSize of job (KB) ... 005 (032.000.000) 08/18 15:18:33 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 56 - Run Bytes Sent By Job 7059 - Run Bytes Received By Job 56 - Total Bytes Sent By Job 7059 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 15 7 17605109 Memory (MB) : 0 1 1900 That looks good: the job started up quickly, though you will often see slightly slower startups. Condor doesn't optimize for fast job startup, but for high throughput, The job ran for four seconds. Now take a look at the job's output: $ cat simple.out Thinking really hard for 4 seconds... We calculated: 20 Excellent! We ran our sophisticated scientific job on a Condor pool! We've only run one job though. Can we run more? Doing a parameter sweep If you only ever had to run a single job, you probably wouldn't need Condor. But we would like to have our program calculate a whole set of values for different inputs. How can we do that? Let's change our submit file to look like this: Universe = vanilla Executable = simple +ProjectName = \"ConnectTrain\" Arguments = 4 10 Log = simple.$(Process).log Output = simple.$(Process).out Error = simple.$(Process).error requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ON_EXIT Queue Arguments = 4 11 Queue Arguments = 4 12 Queue There are two important differences to notice here. First, the Log, Output and Error lines have the $(Process) macro in them. This means that the output and error files will be named according to the process number of the job. You'll see what this looks like in a moment. Second, we told Condor to run the same job an extra two times by adding extra Arguments and Queue statements. We are doing a parameter sweep on the values 10, 11, and 12. Let's see what happens: $ condor_submit submit Submitting job(s)... 3 job(s) submitted to cluster 18. $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 34.0 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 0 34 34.1 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 1 34 34.2 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 2 34 3 jobs; 0 completed, 0 removed, 4 idle, 0 running, 0 held, 0 suspended $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 34.0 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 0 34 34.1 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 1 34 34.2 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 2 34 3 jobs; 0 completed, 0 removed, 0 idle, 4 running, 0 held, 0 suspended $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended $ ls simple*out simple.0.out simple.1.out simple.2.out simple.out $ cat simple.0.out Thinking really hard for 4 seconds... We calculated: 20 $ cat simple.1.out Thinking really hard for 4 seconds... We calculated: 22 $ cat simple.2.out Thinking really hard for 4 seconds... We calculated: 24 Notice that we had three jobs with the same cluster number, but different process numbers. They have the same cluster number because they were all submitted from the same submit file. When the jobs ran, they created three different output files, each with the desired output. You are now ready to submit lots of jobs! Although this example was simple, Condor has many, many options so you can get a wide variety of behaviors. You can find many of these if you look at the documentation for condor_submit . On your own Now that you've gotten your feet wet, try a few things on your own. Just one log file There's no reason to have a separate log file for each job. Change your submit file so that it uses a single log file. Does it all still work? New outputs for each run You might have noticed that the output files were over-written when you re-ran the jobs. (That is, simple.1.out was just re-written.) That was okay for a simple exercise, but it might be very bad if you had wanted to keep around the results. Maybe you changed a parameter or rebuilt your program, and you want to compare the outputs. Just like you used $(Process) , you can also use $(Cluster) . This will be a number from your job ID. For example, it would be 34 from the above example. Change your submit file to use $(Cluster) and $(Process) . If you do two job submissions, will you have separate output files? Lots of jobs Instead of specifying the Arguments multiple times with multiple queue statements, try this: Arguments = $(Process) $(Cluster) queue 10 What does it mean? What happens? Does it work as you expect? (An aside: you might wish to be able to do math, something like $(Process)+1 . Unfortunately, you can't do that.) Challenges If you have time and feel comfortable with the technical background, try these extra challenges. You'll need to peruse the Condor manual (particularly the manual page for condor_submit ) to find answers. Feel free to ask Rob--he'd love to give you hints! Make another scientific program (probably just modify simple.c) that takes its input from a file. Now submit 3 copies of this program where each input file is in a separate directory. Use the initialdir option described in the manual . This will let you specify a directory for the input to the program. You can run specify the initialdir with $(Process) . You can specify extra files to copy with transfer_input_files . Now you're really learning the basics of running something like a real scientific job! Condor can send you email when a job finishes. How can you control this? You know that your job should never run for more than four hours. If it does, then the job should be killed because there is a problem. How can you tell Condor to do this for you?","title":"Submitting your first Condor job"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#submitting-your-first-condor-job","text":"","title":"Submitting your first Condor job"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#objective","text":"The objective of this exercise to have you run and understand your first Condor job, as well as run small sets of jobs in a parameter sweep. This is an important exercise because it is the basis for everything that follows. If there is anything you don't understand in this exercise, please ask before you continue on. Because this is an important foundation, please seriously consider doing the \u201cOn Your Own\u201d section.","title":"Objective"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#first-you-need-a-job","text":"Before you can submit a job to Condor, you need a job. We will quickly write a small program in C. If you aren't an expert C programmer, fear not. We will hold your hand throughout this process. Create a file called simple.c using your favorite editor. Put it anywhere you like in your home directory. In that file, put the following text. Copy and paste is a good choice: $ mkdir -p ~/condor-test $ cd ~/condor-test Use your preferred text editor to create this C program. (Shown below with nano.) $ nano simple.c Paste in the following C code. #include <stdio.h> int main(int argc, char **argv) { int sleep_time; int input; int failure; if (argc != 3) { printf(\"Usage: simple &lt;sleep-time&gt; &lt;integer&gt;\\n\"); failure = 1; } else { sleep_time = atoi(argv[1]); input = atoi(argv[2]); printf(\"Thinking really hard for %d seconds...\\n\", sleep_time); sleep(sleep_time); printf(\"We calculated: %d\\n\", input * 2); failure = 0; } return failure; } Now compile that program: $ gcc -o simple simple.c $ ls -lh simple -rwxrwxr-x 1 roy roy 595K Jun 20 11:12 simple Finally, run the program and tell it to sleep for four seconds and calculate 10 * 2: $ ./simple 4 10 Thinking really hard for 4 seconds... We calculated: 20 Great! You just had a job run locally on the machine you are logged into (user-training.osgconnect.net). The next step is to run this job on a remote computer - and this is a job you can tell Condor to run! Although it clearly isn't an interesting job, it models some of the aspects of a real scientific program: it takes a while to run and it does a calculation. Think back to the lecture. I said that our first step was to have a job to run. Now we'll work on running it in Condor, and eventually running lots of copies of it.","title":"First you need a job"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#submitting-your-job","text":"Now that you have a job, you just have to tell Condor to run it. Put the following text into a file called submit : Universe = vanilla Executable = simple Arguments = 4 10 +ProjectName = \"ConnectTrain\" Log = simple.log Output = simple.out Error = simple.error requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ON_EXIT Queue Let's examine each of these lines: Universe: The vanilla universe means a plain old job. Later on, we'll encounter some special universes. Executable: The name of your program Arguments: These are the arguments you want. They will be the same arguments we typed above. Log: This is the name of a file where Condor will record information about your job's execution. While it's not required, it is a really good idea to have a log. If something goes wrong you can refer to this log to help figure out the problem. Output: Where Condor should put the standard output from your job. Error: Where Condor should put the standard error from your job. Our job isn't likely to have any, but we'll put it there to be safe. should_transfer_files: Tell Condor that it should transfer files, instead of relying on a shared filesystem. While your home directories (on the glite-tutor computers) are mounted on NFS, you do not have user accounts on the worker nodes, so your jobs cannot access files on NFS. In addition, NFS isn't available between the local UI computers and the remote worker nodes. Therefore we will have Condor transfer files to the remote computer. when_to_transfer_output: A technical detail about when files should be transported back to the computer from which you submitted your job. Don't worry about the details for now. If you're really curious, you can read all the details in the Condor manual . Next, tell Condor to run your job: $ condor_submit submit Submitting job(s). 1 job(s) submitted to cluster 16. Now, watch your job run (insert your username in the command below instead of USER . If you forgot your username use the whoami command. Note that most of your output will be different than the example, the important column to watch is the ST column - the job state): # Note the job state of 'I' means the job is idle - not yet running $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:08 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 2056.0 osguser99 7/19 03:40 0+00:00:00 I 0 0.0 simple 4 10 Total for query: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended Total for all users: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended # After some time your job will enter the 'R' state which means it is currently running $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:14 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 2056.0 osguser99 7/19 03:40 0+00:00:02 R 0 0.0 simple 4 10 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended # When your job disappears from the queue that means it completed. $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:21 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD Total for query: 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Total for all users: 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Tip : While you are waiting for your job to run and complete you can check out \"A few tips and tricks\" to learn how to user condor_q more effectively. When my job was done, it was no longer listed. Because I told Condor to log information about my job, I can see what happened: $ cat simple.log 000 (032.000.000) 08/18 15:18:13 Job submitted from host: <10.0.0.252:9645> ... 001 (032.000.000) 08/18 15:18:32 Job executing on host: <172.16.200.1:9250> ... 006 (032.000.000) 08/18 15:18:32 Image size of job updated: 7 0 - MemoryUsage of job (MB) 0 - ResidentSetSize of job (KB) ... 005 (032.000.000) 08/18 15:18:33 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 56 - Run Bytes Sent By Job 7059 - Run Bytes Received By Job 56 - Total Bytes Sent By Job 7059 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 15 7 17605109 Memory (MB) : 0 1 1900 That looks good: the job started up quickly, though you will often see slightly slower startups. Condor doesn't optimize for fast job startup, but for high throughput, The job ran for four seconds. Now take a look at the job's output: $ cat simple.out Thinking really hard for 4 seconds... We calculated: 20 Excellent! We ran our sophisticated scientific job on a Condor pool! We've only run one job though. Can we run more?","title":"Submitting your job"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#doing-a-parameter-sweep","text":"If you only ever had to run a single job, you probably wouldn't need Condor. But we would like to have our program calculate a whole set of values for different inputs. How can we do that? Let's change our submit file to look like this: Universe = vanilla Executable = simple +ProjectName = \"ConnectTrain\" Arguments = 4 10 Log = simple.$(Process).log Output = simple.$(Process).out Error = simple.$(Process).error requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ON_EXIT Queue Arguments = 4 11 Queue Arguments = 4 12 Queue There are two important differences to notice here. First, the Log, Output and Error lines have the $(Process) macro in them. This means that the output and error files will be named according to the process number of the job. You'll see what this looks like in a moment. Second, we told Condor to run the same job an extra two times by adding extra Arguments and Queue statements. We are doing a parameter sweep on the values 10, 11, and 12. Let's see what happens: $ condor_submit submit Submitting job(s)... 3 job(s) submitted to cluster 18. $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 34.0 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 0 34 34.1 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 1 34 34.2 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 2 34 3 jobs; 0 completed, 0 removed, 4 idle, 0 running, 0 held, 0 suspended $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 34.0 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 0 34 34.1 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 1 34 34.2 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 2 34 3 jobs; 0 completed, 0 removed, 0 idle, 4 running, 0 held, 0 suspended $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended $ ls simple*out simple.0.out simple.1.out simple.2.out simple.out $ cat simple.0.out Thinking really hard for 4 seconds... We calculated: 20 $ cat simple.1.out Thinking really hard for 4 seconds... We calculated: 22 $ cat simple.2.out Thinking really hard for 4 seconds... We calculated: 24 Notice that we had three jobs with the same cluster number, but different process numbers. They have the same cluster number because they were all submitted from the same submit file. When the jobs ran, they created three different output files, each with the desired output. You are now ready to submit lots of jobs! Although this example was simple, Condor has many, many options so you can get a wide variety of behaviors. You can find many of these if you look at the documentation for condor_submit .","title":"Doing a parameter sweep"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#on-your-own","text":"Now that you've gotten your feet wet, try a few things on your own.","title":"On your own"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#just-one-log-file","text":"There's no reason to have a separate log file for each job. Change your submit file so that it uses a single log file. Does it all still work?","title":"Just one log file"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#new-outputs-for-each-run","text":"You might have noticed that the output files were over-written when you re-ran the jobs. (That is, simple.1.out was just re-written.) That was okay for a simple exercise, but it might be very bad if you had wanted to keep around the results. Maybe you changed a parameter or rebuilt your program, and you want to compare the outputs. Just like you used $(Process) , you can also use $(Cluster) . This will be a number from your job ID. For example, it would be 34 from the above example. Change your submit file to use $(Cluster) and $(Process) . If you do two job submissions, will you have separate output files?","title":"New outputs for each run"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#lots-of-jobs","text":"Instead of specifying the Arguments multiple times with multiple queue statements, try this: Arguments = $(Process) $(Cluster) queue 10 What does it mean? What happens? Does it work as you expect? (An aside: you might wish to be able to do math, something like $(Process)+1 . Unfortunately, you can't do that.)","title":"Lots of jobs"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#challenges","text":"If you have time and feel comfortable with the technical background, try these extra challenges. You'll need to peruse the Condor manual (particularly the manual page for condor_submit ) to find answers. Feel free to ask Rob--he'd love to give you hints! Make another scientific program (probably just modify simple.c) that takes its input from a file. Now submit 3 copies of this program where each input file is in a separate directory. Use the initialdir option described in the manual . This will let you specify a directory for the input to the program. You can run specify the initialdir with $(Process) . You can specify extra files to copy with transfer_input_files . Now you're really learning the basics of running something like a real scientific job! Condor can send you email when a job finishes. How can you control this? You know that your job should never run for more than four hours. If it does, then the job should be killed because there is a problem. How can you tell Condor to do this for you?","title":"Challenges"},{"location":"DataSanJose2019/CI/04-TipsandTricks/","text":"A few tips and tricks Objective This exercise will teach you a few nifty commands to help you use Condor more easily. Tips for condor_q condor_q can show you your job ClassAd. Recall back to the lecture and the discussion of ClassAds. For instance, you can look at the ClassAd for a single job: $ condor_q -l YOUR_JOB_CLUSTER_NUMBER MaxHosts = 1 User = \"kagross@frontal.cci.ucad.sn\" OnExitHold = false CoreSize = 0 MachineAttrCpus0 = 1 WantRemoteSyscalls = false MyType = \"Job\" Rank = 0.0 CumulativeSuspensionTime = 0 MinHosts = 1 PeriodicHold = false PeriodicRemove = false Err = \"simple.49.error\" ProcId = 49 EnteredCurrentStatus = 1408374244 UserLog = \"/home/kagross/condor-test/s ... output trimmed ... There are some interesting parts you can check out. How many CPUs is the job requesting. (This can be more than one, but for the exercises we will do today it will be 1) $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep RequestCpus RequestCpus = 1 Where is the user log for this job? This is helpful when you assist someone else in debugging and they're not sure. $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep UserLog UserLog = \"/home/kagross/condor-test/simple.47.log\" What are the job's requirements? Condor automatically fills some in for you to make sure your job runs on a reasonable computer in our cluster, but you can override any of these. I've broken the output into multiple lines to explain it to you. $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep Requirements Requirements =( TARGET.Arch == \"X86_64\" ) <em># Run on a 64-bit computer && ( TARGET.OpSys == \"LINUX\" ) <em># Make sure you run on Linux && ( TARGET.Disk >= RequestDisk ) <em># Make sure the default disk Condor is on has enough disk space. && ( TARGET.Memory >= RequestMemory ) <em># Make sure the computer has enough memory && ( TARGET.HasFileTransfer ) <em># Only run on a computer that can accept your files. What else can you find that's interesting in the ClassAd? Removing jobs If you submit a job that you realize has a problem, you can remove it with condor_rm . For example: $ condor_q -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 29.0 roy 6/21 15:23 0+00:00:00 I 0 0.7 simple 60 10 1 jobs; 0 completed, 0 removed, 2 idle, 0 running, 0 held, 0 suspended $ condor_rm YOUR_JOB_CLUSTER_NUMBER Job 29.0 marked for removal $ condor_q -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended A few tips: You can remove all of your jobs with the -all option. You can't remove other users jobs. There are fancy options to condor_rm . Historical information You can see information about jobs that completed and are no longer in the queue with the condor_history command. It's rare that you want to see all the jobs, so try looking at jobs for just you: $ condor_history USER For example: $ condor_history kagross 9.9 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 9 9 9.8 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 8 9 9.11 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 11 9 9.7 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 7 9 9.5 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 5 9 9.6 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 6 9 9.3 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 3 9 9.2 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 2 9 9.1 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 1 9 9.0 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 9.4 kagross 7/31 12:44 0+00:00:01 C 7/31 12:44 /home/kagross/simple 4 9 8.0 kagross 7/31 12:42 0+00:00:07 C 7/31 12:42 /home/kagross/simple 4 10 ...","title":"A few tips and tricks"},{"location":"DataSanJose2019/CI/04-TipsandTricks/#a-few-tips-and-tricks","text":"","title":"A few tips and tricks"},{"location":"DataSanJose2019/CI/04-TipsandTricks/#objective","text":"This exercise will teach you a few nifty commands to help you use Condor more easily.","title":"Objective"},{"location":"DataSanJose2019/CI/04-TipsandTricks/#tips-for-condor_q","text":"condor_q can show you your job ClassAd. Recall back to the lecture and the discussion of ClassAds. For instance, you can look at the ClassAd for a single job: $ condor_q -l YOUR_JOB_CLUSTER_NUMBER MaxHosts = 1 User = \"kagross@frontal.cci.ucad.sn\" OnExitHold = false CoreSize = 0 MachineAttrCpus0 = 1 WantRemoteSyscalls = false MyType = \"Job\" Rank = 0.0 CumulativeSuspensionTime = 0 MinHosts = 1 PeriodicHold = false PeriodicRemove = false Err = \"simple.49.error\" ProcId = 49 EnteredCurrentStatus = 1408374244 UserLog = \"/home/kagross/condor-test/s ... output trimmed ... There are some interesting parts you can check out. How many CPUs is the job requesting. (This can be more than one, but for the exercises we will do today it will be 1) $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep RequestCpus RequestCpus = 1 Where is the user log for this job? This is helpful when you assist someone else in debugging and they're not sure. $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep UserLog UserLog = \"/home/kagross/condor-test/simple.47.log\" What are the job's requirements? Condor automatically fills some in for you to make sure your job runs on a reasonable computer in our cluster, but you can override any of these. I've broken the output into multiple lines to explain it to you. $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep Requirements Requirements =( TARGET.Arch == \"X86_64\" ) <em># Run on a 64-bit computer && ( TARGET.OpSys == \"LINUX\" ) <em># Make sure you run on Linux && ( TARGET.Disk >= RequestDisk ) <em># Make sure the default disk Condor is on has enough disk space. && ( TARGET.Memory >= RequestMemory ) <em># Make sure the computer has enough memory && ( TARGET.HasFileTransfer ) <em># Only run on a computer that can accept your files. What else can you find that's interesting in the ClassAd?","title":"Tips for condor_q"},{"location":"DataSanJose2019/CI/04-TipsandTricks/#removing-jobs","text":"If you submit a job that you realize has a problem, you can remove it with condor_rm . For example: $ condor_q -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 29.0 roy 6/21 15:23 0+00:00:00 I 0 0.7 simple 60 10 1 jobs; 0 completed, 0 removed, 2 idle, 0 running, 0 held, 0 suspended $ condor_rm YOUR_JOB_CLUSTER_NUMBER Job 29.0 marked for removal $ condor_q -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended A few tips: You can remove all of your jobs with the -all option. You can't remove other users jobs. There are fancy options to condor_rm .","title":"Removing jobs"},{"location":"DataSanJose2019/CI/04-TipsandTricks/#historical-information","text":"You can see information about jobs that completed and are no longer in the queue with the condor_history command. It's rare that you want to see all the jobs, so try looking at jobs for just you: $ condor_history USER For example: $ condor_history kagross 9.9 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 9 9 9.8 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 8 9 9.11 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 11 9 9.7 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 7 9 9.5 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 5 9 9.6 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 6 9 9.3 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 3 9 9.2 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 2 9 9.1 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 1 9 9.0 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 9.4 kagross 7/31 12:44 0+00:00:01 C 7/31 12:44 /home/kagross/simple 4 9 8.0 kagross 7/31 12:42 0+00:00:07 C 7/31 12:42 /home/kagross/simple 4 10 ...","title":"Historical information"},{"location":"DataSanJose2019/CI/05-ScriptingJob/","text":"Using scripting languages Objective The objective of this exercise is to demonstrate that you can submit jobs to Condor in any language, including scripting languages. At this point, you might be asking yourself, \"This is all well and good, but I don't write programs in C. Can I use other languages?\" Absolutely. Let's assume you like to write program in Bourne shell. Make sure your program begins with #!/bin/sh , and you're good to go. Save this example code into a file called simple.sh using nano or your favorite editor. #!/bin/sh if [ $# -ne 2 ]; then echo \"Usage: simple.sh sleep-time integer\" exit 1 fi echo \"Thinking really hard for $1 seconds..\" sleep $1 answer=$(( $2 * 2 )) echo \"We calculated $answer.\" exit 0 This script will not be executable without changing the permissions. $ chmod 755 simple.sh Can you write a submit file to run this job? This should be easy--the script is your Executable , not /bin/sh . You may also want to change the name of your submit.log , submit.out , and submit.err in your submit file to be sure they are not written over when you run this submission. Challenge Rewrite this script in Perl or Python (if you're comfortable with one of those languages). Does it still work for you?","title":"Using scripting languages"},{"location":"DataSanJose2019/CI/05-ScriptingJob/#using-scripting-languages","text":"","title":"Using scripting languages"},{"location":"DataSanJose2019/CI/05-ScriptingJob/#objective","text":"The objective of this exercise is to demonstrate that you can submit jobs to Condor in any language, including scripting languages. At this point, you might be asking yourself, \"This is all well and good, but I don't write programs in C. Can I use other languages?\" Absolutely. Let's assume you like to write program in Bourne shell. Make sure your program begins with #!/bin/sh , and you're good to go. Save this example code into a file called simple.sh using nano or your favorite editor. #!/bin/sh if [ $# -ne 2 ]; then echo \"Usage: simple.sh sleep-time integer\" exit 1 fi echo \"Thinking really hard for $1 seconds..\" sleep $1 answer=$(( $2 * 2 )) echo \"We calculated $answer.\" exit 0 This script will not be executable without changing the permissions. $ chmod 755 simple.sh Can you write a submit file to run this job? This should be easy--the script is your Executable , not /bin/sh . You may also want to change the name of your submit.log , submit.out , and submit.err in your submit file to be sure they are not written over when you run this submission.","title":"Objective"},{"location":"DataSanJose2019/CI/05-ScriptingJob/#challenge","text":"Rewrite this script in Perl or Python (if you're comfortable with one of those languages). Does it still work for you?","title":"Challenge"},{"location":"DataSanJose2019/CI/06-RJob/","text":"Using scripting languages Objective The objective of this exercise is to demonstrate that you can submit jobs to Condor in any language, including scripting languages. At this point, you might be asking yourself, \"This is all well and good, but I don't write programs in C. Can I use other languages?\" Absolutely. Let's assume you like to write program in Bourne shell. Make sure your program begins with #!/bin/sh , and you're good to go. Save this example code into a file called simple.sh using nano or your favorite editor. #!/bin/sh if [ $# -ne 2 ]; then echo \"Usage: simple.sh sleep-time integer\" exit 1 fi echo \"Thinking really hard for $1 seconds..\" sleep $1 answer=$(( $2 * 2 )) echo \"We calculated $answer.\" exit 0 This script will not be executable without changing the permissions. $ chmod 755 simple.sh Can you write a submit file to run this job? This should be easy--the script is your Executable , not /bin/sh . You may also want to change the name of your submit.log , submit.out , and submit.err in your submit file to be sure they are not written over when you run this submission. Challenge Rewrite this script in Perl or Python (if you're comfortable with one of those languages). Does it still work for you?","title":"Using scripting languages"},{"location":"DataSanJose2019/CI/06-RJob/#using-scripting-languages","text":"","title":"Using scripting languages"},{"location":"DataSanJose2019/CI/06-RJob/#objective","text":"The objective of this exercise is to demonstrate that you can submit jobs to Condor in any language, including scripting languages. At this point, you might be asking yourself, \"This is all well and good, but I don't write programs in C. Can I use other languages?\" Absolutely. Let's assume you like to write program in Bourne shell. Make sure your program begins with #!/bin/sh , and you're good to go. Save this example code into a file called simple.sh using nano or your favorite editor. #!/bin/sh if [ $# -ne 2 ]; then echo \"Usage: simple.sh sleep-time integer\" exit 1 fi echo \"Thinking really hard for $1 seconds..\" sleep $1 answer=$(( $2 * 2 )) echo \"We calculated $answer.\" exit 0 This script will not be executable without changing the permissions. $ chmod 755 simple.sh Can you write a submit file to run this job? This should be easy--the script is your Executable , not /bin/sh . You may also want to change the name of your submit.log , submit.out , and submit.err in your submit file to be sure they are not written over when you run this submission.","title":"Objective"},{"location":"DataSanJose2019/CI/06-RJob/#challenge","text":"Rewrite this script in Perl or Python (if you're comfortable with one of those languages). Does it still work for you?","title":"Challenge"},{"location":"DataSanJose2019/CI/07-WorkingwithFiles/","text":"Working with data in files Objective The objective of this exercise is to teach you how to provide files as input to your job, and get output as files back from your job. Data Movement So far, we've done really simple examples where the entire input to the program is just on the command-line. What do you do if you have data files to deal with? Let's walk through a short example. First, let's make a program, call it analyze.sh that analyzes a text file that it is provided on the command-line. #!/bin/sh if [ $# -ne 1 ]; then echo \"Usage: analyze.sh <filename>\" exit 1 fi echo \"About to do a deep analysis of $1...\" echo \"First, we convert it to all upper case (see $1.upper)\" tr \"[:lower:]\" \"[:upper:]\" < $1 > $1.upper echo \"Next, we find the 10 most common words (see $1.10)\" cat $1 | tr \"[:upper:]\" \"[:lower:]\" | tr -cs \"[:alpha:]\" \"\\n\" | sort | uniq -c | sort --key=1,7 -n -r | head -10 > $1.10 sleep 5 You also need a file to analyze. Put the following text into a file called gettysburg . Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this. But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us -- that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion -- that we here highly resolve that these dead shall not have died in vain -- that this nation, under God, shall have a new birth of freedom -- and that government of the people, by the people, for the people, shall not perish from the earth. Our submit file looks nearly identical to what we had before, except for the one bolded line that specifies the data file to transfer. Put the following text into a file called submit.speech . Universe = vanilla Executable = analyze.sh Output = analyze.out Error = analyze.error Log = analyze.log Arguments = gettysburg +ProjectName = \"ConnectTrain\" requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") ShouldTransferFiles = Yes WhenToTransferOutput = ON_EXIT transfer_input_files = gettysburg queue Notice that you just had to specify the input files and not the output files. Condor will automatically transfer back any new files, so you don't have to worry about it. Nifty, huh? Now run the job. $ condor_submit submit.speech Submitting job(s). 1 job(s) submitted to cluster 37. $ ls -lh gettys* -rw-rw-r--. 1 kagross kagross 1.5K Aug 18 15:41 gettysburg -rw-r--r--. 1 kagross kagross 120 Aug 18 15:42 gettysburg.10 -rw-r--r--. 1 kagross kagross 1.5K Aug 18 15:42 gettysburg.upper You got your files! Check them out--do they look okay? On your own Create several text files, then submit jobs (preferably from a single submit file) to analyze each of them. If you're at a loss to create some text files, here are a few for you. Walkthrough of the Original Text Game \"Adventure\" The Story of Captain Midnight The Universal Geek Code Tao of Programming Instead of downloading these files and transferring them directly, can you change your transfer-input-files to use a URL and have Condor download them for you? Give this a try.","title":"Working with data in files"},{"location":"DataSanJose2019/CI/07-WorkingwithFiles/#working-with-data-in-files","text":"","title":"Working with data in files"},{"location":"DataSanJose2019/CI/07-WorkingwithFiles/#objective","text":"The objective of this exercise is to teach you how to provide files as input to your job, and get output as files back from your job.","title":"Objective"},{"location":"DataSanJose2019/CI/07-WorkingwithFiles/#data-movement","text":"So far, we've done really simple examples where the entire input to the program is just on the command-line. What do you do if you have data files to deal with? Let's walk through a short example. First, let's make a program, call it analyze.sh that analyzes a text file that it is provided on the command-line. #!/bin/sh if [ $# -ne 1 ]; then echo \"Usage: analyze.sh <filename>\" exit 1 fi echo \"About to do a deep analysis of $1...\" echo \"First, we convert it to all upper case (see $1.upper)\" tr \"[:lower:]\" \"[:upper:]\" < $1 > $1.upper echo \"Next, we find the 10 most common words (see $1.10)\" cat $1 | tr \"[:upper:]\" \"[:lower:]\" | tr -cs \"[:alpha:]\" \"\\n\" | sort | uniq -c | sort --key=1,7 -n -r | head -10 > $1.10 sleep 5 You also need a file to analyze. Put the following text into a file called gettysburg . Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this. But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us -- that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion -- that we here highly resolve that these dead shall not have died in vain -- that this nation, under God, shall have a new birth of freedom -- and that government of the people, by the people, for the people, shall not perish from the earth. Our submit file looks nearly identical to what we had before, except for the one bolded line that specifies the data file to transfer. Put the following text into a file called submit.speech . Universe = vanilla Executable = analyze.sh Output = analyze.out Error = analyze.error Log = analyze.log Arguments = gettysburg +ProjectName = \"ConnectTrain\" requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") ShouldTransferFiles = Yes WhenToTransferOutput = ON_EXIT transfer_input_files = gettysburg queue Notice that you just had to specify the input files and not the output files. Condor will automatically transfer back any new files, so you don't have to worry about it. Nifty, huh? Now run the job. $ condor_submit submit.speech Submitting job(s). 1 job(s) submitted to cluster 37. $ ls -lh gettys* -rw-rw-r--. 1 kagross kagross 1.5K Aug 18 15:41 gettysburg -rw-r--r--. 1 kagross kagross 120 Aug 18 15:42 gettysburg.10 -rw-r--r--. 1 kagross kagross 1.5K Aug 18 15:42 gettysburg.upper You got your files! Check them out--do they look okay?","title":"Data Movement"},{"location":"DataSanJose2019/CI/07-WorkingwithFiles/#on-your-own","text":"Create several text files, then submit jobs (preferably from a single submit file) to analyze each of them. If you're at a loss to create some text files, here are a few for you. Walkthrough of the Original Text Game \"Adventure\" The Story of Captain Midnight The Universal Geek Code Tao of Programming Instead of downloading these files and transferring them directly, can you change your transfer-input-files to use a URL and have Condor download them for you? Give this a try.","title":"On your own"},{"location":"DataSanJose2019/CI/08-Mandlebrot/","text":"A brief detour through the Mandlebrot set Before we dive into a more complicated DAG, let's get a more interesting job. I'm tired of this lazy, sleepy job that only does trivial mathematics. Let's make pretty pictures! We have a small program that draws pictures of the Mandlebrot set. You can read about the Mandlebrot set on Wikipedia , or you can simply appreciate the pretty pictures. It's a fractal. We have a simple program that can draw the Mandlebrot set. It's called goatbrot , A simple invocation of goatbrot You can generate the Mandlebrot set with two simple commands. Generate a PPM image of the Mandlebrot set: $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000000_000000.ppm -c 0,0 -w 3 -s 1000,1000 Add the Fast Fourier Transform and ImageMagick packages: $ module load fftw Convert it to a JPEG image and write into your home directory: $ convert tile_000000_000000.ppm ~/mandle.gif Open a new terminal window and move the file to local machine for viewing (substitute your username in place of USER ): $ scp USER@training.osgconnect.net:/home/USER/mandle.gif ./ Point Browser at the file URL: firefox ./mandle.gif The goatbroat program takes several parameters. Let's break them down: -i 1000 The number of iterations. Bigger numbers generate more accurate images but are slower to run. -o tile_000000_000000.ppm The output file to generate. -c 0,0 The center point of the image. Here it is the point (0,0). -w 3 The width of the image. Here is 3. -s 1000,1000 The size of the final image. Here we generate a picture that is 1000 pixels wide and 1000 pixels tall. Dividing goatbrot up The Mandlebrot set can take a while to create, particularly if you make the iterations large or the image size large. What if we broke the creation of the image into multiple invocations then stitched them together? Once we do that, we can run the each goatbroat in parallel in our cluster. Here's an example you can run by hand. Run goatbroat 4 times : $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000000_000000.ppm -c -0.75,0.75 -w 1.5 -s 500,500 $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000000_000001.ppm -c 0.75,0.75 -w 1.5 -s 500,500 $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000001_000000.ppm -c -0.75,-0.75 -w 1.5 -s 500,500 $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000001_000001.ppm -c 0.75,-0.75 -w 1.5 -s 500,500 Stitch them together : $ montage tile_000000_000000.ppm tile_000000_000001.ppm tile_000001_000000.ppm tile_000001_000001.ppm -mode Concatenate -tile 2x2 ~/mandle.gif This will produce the same image as above. We broke the image space into a 2 by 2 grid and ran goatbrot on each section of the grid. The montage program simply stitches the files together. Try it! Run the commands above, make sure you can create the Mandlebrot image. When you create the image, you might wonder how you can view it. The same way we did above, or more simply by moving the file to a web accessible location. cp mandle.gif ~/public point your browser at the stash web server: http://stash.osgconnect.net/~USER","title":"A brief detour through the Mandlebrot set"},{"location":"DataSanJose2019/CI/08-Mandlebrot/#a-brief-detour-through-the-mandlebrot-set","text":"Before we dive into a more complicated DAG, let's get a more interesting job. I'm tired of this lazy, sleepy job that only does trivial mathematics. Let's make pretty pictures! We have a small program that draws pictures of the Mandlebrot set. You can read about the Mandlebrot set on Wikipedia , or you can simply appreciate the pretty pictures. It's a fractal. We have a simple program that can draw the Mandlebrot set. It's called goatbrot ,","title":"A brief detour through the Mandlebrot set"},{"location":"DataSanJose2019/CI/08-Mandlebrot/#a-simple-invocation-of-goatbrot","text":"You can generate the Mandlebrot set with two simple commands. Generate a PPM image of the Mandlebrot set: $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000000_000000.ppm -c 0,0 -w 3 -s 1000,1000 Add the Fast Fourier Transform and ImageMagick packages: $ module load fftw Convert it to a JPEG image and write into your home directory: $ convert tile_000000_000000.ppm ~/mandle.gif Open a new terminal window and move the file to local machine for viewing (substitute your username in place of USER ): $ scp USER@training.osgconnect.net:/home/USER/mandle.gif ./ Point Browser at the file URL: firefox ./mandle.gif The goatbroat program takes several parameters. Let's break them down: -i 1000 The number of iterations. Bigger numbers generate more accurate images but are slower to run. -o tile_000000_000000.ppm The output file to generate. -c 0,0 The center point of the image. Here it is the point (0,0). -w 3 The width of the image. Here is 3. -s 1000,1000 The size of the final image. Here we generate a picture that is 1000 pixels wide and 1000 pixels tall.","title":"A simple invocation of goatbrot"},{"location":"DataSanJose2019/CI/08-Mandlebrot/#dividing-goatbrot-up","text":"The Mandlebrot set can take a while to create, particularly if you make the iterations large or the image size large. What if we broke the creation of the image into multiple invocations then stitched them together? Once we do that, we can run the each goatbroat in parallel in our cluster. Here's an example you can run by hand. Run goatbroat 4 times : $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000000_000000.ppm -c -0.75,0.75 -w 1.5 -s 500,500 $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000000_000001.ppm -c 0.75,0.75 -w 1.5 -s 500,500 $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000001_000000.ppm -c -0.75,-0.75 -w 1.5 -s 500,500 $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000001_000001.ppm -c 0.75,-0.75 -w 1.5 -s 500,500 Stitch them together : $ montage tile_000000_000000.ppm tile_000000_000001.ppm tile_000001_000000.ppm tile_000001_000001.ppm -mode Concatenate -tile 2x2 ~/mandle.gif This will produce the same image as above. We broke the image space into a 2 by 2 grid and ran goatbrot on each section of the grid. The montage program simply stitches the files together.","title":"Dividing goatbrot up"},{"location":"DataSanJose2019/CI/08-Mandlebrot/#try-it","text":"Run the commands above, make sure you can create the Mandlebrot image. When you create the image, you might wonder how you can view it. The same way we did above, or more simply by moving the file to a web accessible location. cp mandle.gif ~/public point your browser at the stash web server: http://stash.osgconnect.net/~USER","title":"Try it!"},{"location":"DataSanJose2019/CI/09-SimpleDAG/","text":"Coordinating set of jobs: A simple DAG Objective The objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job. What is DAGMan? Your tutorial leader will introduce you to DAGMan and DAGs. In short, DAGMan lets you submit complex sequences of jobs as long as they can be expressed as a directed acylic graph. For example, you may wish to run a large parameter sweep but before the sweep run you need to prepare your data. After the sweep runs, you need to collate the results. DAGMan has many abilities such as throttling jobs, recovery from failures, and more. More information about DAGMan can be found at in the Condor manual . Submitting a simple DAG We're going to go back to the \"simple\" example that we did first. (The one with the job that slept and multiplied a number by 2.) Make sure that you have a submit file has only one queue command in it, as when we first wrote it. And we will just run vanilla universe jobs for now, though we could equally well run standard universe jobs. Universe = vanilla Executable = simple Arguments = 4 10 +ProjectName = \"ConnectTrain\" Log = simple.log Output = simple.out Error = simple.error requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ON_EXIT Queue Make sure you've built the simple program. If you need to, go back to the instructions for your first job to do it again. We are going to get a bit more sophisticated in submitting our jobs now. Let's have three windows open. In one window you'll submit the job. In the second you will watch the queue. And in the third you will watch what DAGMan does. First we will create the most minimal DAG that can be created: a DAG with just one node. Put the text below into a file named simple.dag . job simple submit In your first window, submit the DAG: $ condor_submit_dag simple.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : simple.dag.condor.sub Log of DAGMan debugging messages : simple.dag.dagman.out Log of Condor library output : simple.dag.lib.out Log of Condor library error messages : simple.dag.lib.err Log of the life of condor_dagman itself : simple.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 61. ----------------------------------------------------------------------- In the second window, watch the queue (press Ctrl+C when finished watching to kill this process): $ watch -n 10 condor_q USER -nobatch -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:00:01 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:01:25 R 0 0.3 condor_dagman 62.0 roy 6/21 22:51 0+00:00:00 I 0 0.7 simple 4 10 2 jobs; 0 completed, 0 removed, 1 idle, 1 running, 0 held, 0 suspended -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:03:47 R 0 0.3 condor_dagman 62.0 roy 6/21 22:51 0+00:00:03 R 0 0.7 simple 4 10 2 jobs; 0 completed, 0 removed, 0 idle, 2 running, 0 held, 0 suspended -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Ctrl-C In the third window, watch what DAGMan does: $ tail -f --lines=500 simple.dag.dagman.out 6/21/12 22:51:13 Setting maximum accepts per cycle 8. 06/21/12 22:51:13 ****************************************************** 06/21/12 22:51:13 ** condor_scheduniv_exec.61.0 (CONDOR_DAGMAN) STARTING UP 06/21/12 22:51:13 ** /usr/bin/condor_dagman 06/21/12 22:51:13 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/21/12 22:51:13 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/21/12 22:51:13 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/21/12 22:51:13 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/21/12 22:51:13 ** PID = 5812 06/21/12 22:51:13 ** Log last touched 6/21 22:51:00 06/21/12 22:51:13 ****************************************************** 06/21/12 22:51:13 Using config source: /etc/condor/condor_config 06/21/12 22:51:13 Using local config sources: 06/21/12 22:51:13 /etc/condor/config.d/00-chtc-global.conf 06/21/12 22:51:13 /etc/condor/config.d/01-chtc-submit.conf 06/21/12 22:51:13 /etc/condor/config.d/02-chtc-flocking.conf 06/21/12 22:51:13 /etc/condor/config.d/03-chtc-jobrouter.conf 06/21/12 22:51:13 /etc/condor/config.d/04-chtc-blacklist.conf 06/21/12 22:51:13 /etc/condor/config.d/99-osg-ss-group.conf 06/21/12 22:51:13 /etc/condor/config.d/99-roy-extras.conf 06/21/12 22:51:13 /etc/condor/condor_config.local 06/21/12 22:51:13 DaemonCore: command socket at <128.104.100.55:60417> 06/21/12 22:51:13 DaemonCore: private command socket at <128.104.100.55:60417> 06/21/12 22:51:13 Setting maximum accepts per cycle 8. 06/21/12 22:51:13 DAGMAN_USE_STRICT setting: 0 06/21/12 22:51:13 DAGMAN_VERBOSITY setting: 3 06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880 06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_ENABLE setting: False 06/21/12 22:51:13 DAGMAN_SUBMIT_DELAY setting: 0 06/21/12 22:51:13 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6 06/21/12 22:51:13 DAGMAN_STARTUP_CYCLE_DETECT setting: False 06/21/12 22:51:13 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 5 06/21/12 22:51:13 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5 06/21/12 22:51:13 allow_events (DAGMAN_IGNORE_DUPLICATE_JOB_EXECUTION, DAGMAN_ALLOW_EVENTS) setting: 114 06/21/12 22:51:13 DAGMAN_RETRY_SUBMIT_FIRST setting: True 06/21/12 22:51:13 DAGMAN_RETRY_NODE_FIRST setting: False 06/21/12 22:51:13 DAGMAN_MAX_JOBS_IDLE setting: 0 06/21/12 22:51:13 DAGMAN_MAX_JOBS_SUBMITTED setting: 0 06/21/12 22:51:15 DAGMAN_MAX_PRE_SCRIPTS setting: 0 06/21/12 22:51:15 DAGMAN_MAX_POST_SCRIPTS setting: 0 06/21/12 22:51:15 DAGMAN_ALLOW_LOG_ERROR setting: False 06/21/12 22:51:15 DAGMAN_MUNGE_NODE_NAMES setting: True 06/21/12 22:51:15 DAGMAN_PROHIBIT_MULTI_JOBS setting: False 06/21/12 22:51:15 DAGMAN_SUBMIT_DEPTH_FIRST setting: False 06/21/12 22:51:15 DAGMAN_ALWAYS_RUN_POST setting: True 06/21/12 22:51:15 DAGMAN_ABORT_DUPLICATES setting: True 06/21/12 22:51:15 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: True 06/21/12 22:51:15 DAGMAN_PENDING_REPORT_INTERVAL setting: 600 06/21/12 22:51:15 DAGMAN_AUTO_RESCUE setting: True 06/21/12 22:51:15 DAGMAN_MAX_RESCUE_NUM setting: 100 06/21/12 22:51:15 DAGMAN_WRITE_PARTIAL_RESCUE setting: True 06/21/12 22:51:15 DAGMAN_DEFAULT_NODE_LOG setting: null 06/21/12 22:51:15 DAGMAN_GENERATE_SUBDAG_SUBMITS setting: True 06/21/12 22:51:15 ALL_DEBUG setting: 06/21/12 22:51:15 DAGMAN_DEBUG setting: 06/21/12 22:51:15 argv[0] == \"condor_scheduniv_exec.61.0\" 06/21/12 22:51:15 argv[1] == \"-Lockfile\" 06/21/12 22:51:15 argv[2] == \"simple.dag.lock\" 06/21/12 22:51:15 argv[3] == \"-AutoRescue\" 06/21/12 22:51:15 argv[4] == \"1\" 06/21/12 22:51:15 argv[5] == \"-DoRescueFrom\" 06/21/12 22:51:15 argv[6] == \"0\" 06/21/12 22:51:15 argv[7] == \"-Dag\" 06/21/12 22:51:15 argv[8] == \"simple.dag\" 06/21/12 22:51:15 argv[9] == \"-CsdVersion\" 06/21/12 22:51:15 argv[10] == \"$CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $\" 06/21/12 22:51:15 argv[11] == \"-Force\" 06/21/12 22:51:15 argv[12] == \"-Dagman\" 06/21/12 22:51:15 argv[13] == \"/usr/bin/condor_dagman\" 06/21/12 22:51:15 Default node log file is: </home/roy/condor/simple.dag.nodes.log> 06/21/12 22:51:15 DAG Lockfile will be written to simple.dag.lock 06/21/12 22:51:15 DAG Input file is simple.dag 06/21/12 22:51:15 Parsing 1 dagfiles 06/21/12 22:51:15 Parsing simple.dag ... 06/21/12 22:51:15 Dag contains 1 total jobs 06/21/12 22:51:15 Sleeping for 12 seconds to ensure ProcessId uniqueness 06/21/12 22:51:27 Bootstrapping... 06/21/12 22:51:27 Number of pre-completed nodes: 0 06/21/12 22:51:27 Registering condor_event_timer... 06/21/12 22:51:28 Sleeping for one second for log file consistency 06/21/12 22:51:29 MultiLogFiles: truncating log file /home/roy/condor/simple.log 06/21/12 22:51:29 Submitting Condor Node Simple job(s)... # Here's where the job is submitted 06/21/12 22:51:29 submitting: condor_submit -a dag_node_name' '=' 'Simple -a +DAGManJobId' '=' '61 -a DAGManJobId' '=' '61 -a submit_event_notes' '=' 'DAG' 'Node:' 'Simple -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"\" submit 06/21/12 22:51:30 From submit: Submitting job(s). 06/21/12 22:51:30 From submit: 1 job(s) submitted to cluster 62. 06/21/12 22:51:30 assigned Condor ID (62.0.0) 06/21/12 22:51:30 Just submitted 1 job this cycle... 06/21/12 22:51:30 Currently monitoring 1 Condor log file(s) 06/21/12 22:51:30 Event: ULOG_SUBMIT for Condor Node Simple (62.0.0) 06/21/12 22:51:30 Number of idle job procs: 1 06/21/12 22:51:30 Of 1 nodes total: 06/21/12 22:51:30 Done Pre Queued Post Ready Un-Ready Failed 06/21/12 22:51:30 === === === === === === === 06/21/12 22:51:30 0 0 1 0 0 0 0 06/21/12 22:51:30 0 job proc(s) currently held 06/21/12 22:55:05 Currently monitoring 1 Condor log file(s) # Here's where DAGMan noticed that the job is running 06/21/12 22:55:05 Event: ULOG_EXECUTE for Condor Node Simple (62.0.0) 06/21/12 22:55:05 Number of idle job procs: 0 06/21/12 22:55:10 Currently monitoring 1 Condor log file(s) 06/21/12 22:55:10 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0) 06/21/12 22:56:05 Currently monitoring 1 Condor log file(s) 06/21/12 22:56:05 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0) # Here's where DAGMan noticed that the job finished. 06/21/12 22:56:05 Event: ULOG_JOB_TERMINATED for Condor Node Simple (62.0.0) 06/21/12 22:56:05 Node Simple job proc (62.0.0) completed successfully. 06/21/12 22:56:05 Node Simple job completed 06/21/12 22:56:05 Number of idle job procs: 0 06/21/12 22:56:05 Of 1 nodes total: 06/21/12 22:56:05 Done Pre Queued Post Ready Un-Ready Failed 06/21/12 22:56:05 === === === === === === === 06/21/12 22:56:05 1 0 0 0 0 0 0 06/21/12 22:56:05 0 job proc(s) currently held # Here's where DAGMan noticed that all the work is done. 06/21/12 22:56:05 All jobs Completed! 06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/21/12 22:56:05 Note: 0 total job deferrals because of node category throttles 06/21/12 22:56:05 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/21/12 22:56:05 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/21/12 22:56:05 **** condor_scheduniv_exec.61.0 (condor_DAGMAN) pid 5812 EXITING WITH STATUS 0 Now verify your results: $ cat simple.log 000 (062.000.000) 06/21 22:51:30 Job submitted from host: <128.104.100.55:9618?sock=28867_10e4_2> DAG Node: Simple ... 001 (062.000.000) 06/21 22:55:00 Job executing on host: <128.104.58.36:46761> ... 006 (062.000.000) 06/21 22:55:09 Image size of job updated: 750 3 - MemoryUsage of job (MB) 2324 - ResidentSetSize of job (KB) ... 006 (062.000.000) 06/21 22:56:00 Image size of job updated: 780 3 - MemoryUsage of job (MB) 2324 - ResidentSetSize of job (KB) ... 005 (062.000.000) 06/21 22:56:00 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 57 - Run Bytes Sent By Job 608490 - Run Bytes Received By Job 57 - Total Bytes Sent By Job 608490 - Total Bytes Received By Job Partitionable Resources : Usage Request Cpus : 1 Disk (KB) : 750 750 Memory (MB) : 3 3 ... $ cat simple.out Thinking really hard for 4 seconds... We calculated: 20 Looking at DAGMan's various files, we see that DAGMan itself ran as a Condor job (specifically, a scheduler universe job). $ ls simple.dag.* simple.dag.condor.sub simple.dag.dagman.log simple.dag.dagman.out simple.dag.lib.err simple.dag.lib.out $ cat simple.dag.condor.sub # Filename: simple.dag.condor.sub # Generated by condor_submit_dag simple.dag universe = scheduler executable = /usr/bin/condor_dagman getenv = True output = simple.dag.lib.out error = simple.dag.lib.err log = simple.dag.dagman.log remove_kill_sig = SIGUSR1 +OtherJobRemoveRequirements = \"DAGManJobId == $(cluster)\" # Note: default on_exit_remove expression: # ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2)) # attempts to ensure that DAGMan is automatically # requeued by the schedd if it exits abnormally or # is killed (e.g., during a reboot). on_exit_remove = ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2)) copy_to_spool = False arguments = \"-f -l . -Lockfile simple.dag.lock -AutoRescue 1 -DoRescueFrom 0 -Dag simple.dag -CsdVersion $CondorVersion:' '7.7.6' 'Apr' '16' '2012' 'BuildID:' '34175' 'PRE-RELEASE-UWCS' '$ -Force -Dagman /usr/bin/condor_dagman\" environment = _CONDOR_DAGMAN_LOG=simple.dag.dagman.out;_CONDOR_MAX_DAGMAN_LOG=0 queue Clean up some of these files: $ rm simple.dag.* On your own Why does DAGman run as a Condor job? Look at the submit file for DAGMan: what does on_exit_remove do? Why is this here?","title":"Coordinating set of jobs: A simple DAG"},{"location":"DataSanJose2019/CI/09-SimpleDAG/#coordinating-set-of-jobs-a-simple-dag","text":"","title":"Coordinating set of jobs: A simple DAG"},{"location":"DataSanJose2019/CI/09-SimpleDAG/#objective","text":"The objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job.","title":"Objective"},{"location":"DataSanJose2019/CI/09-SimpleDAG/#what-is-dagman","text":"Your tutorial leader will introduce you to DAGMan and DAGs. In short, DAGMan lets you submit complex sequences of jobs as long as they can be expressed as a directed acylic graph. For example, you may wish to run a large parameter sweep but before the sweep run you need to prepare your data. After the sweep runs, you need to collate the results. DAGMan has many abilities such as throttling jobs, recovery from failures, and more. More information about DAGMan can be found at in the Condor manual .","title":"What is DAGMan?"},{"location":"DataSanJose2019/CI/09-SimpleDAG/#submitting-a-simple-dag","text":"We're going to go back to the \"simple\" example that we did first. (The one with the job that slept and multiplied a number by 2.) Make sure that you have a submit file has only one queue command in it, as when we first wrote it. And we will just run vanilla universe jobs for now, though we could equally well run standard universe jobs. Universe = vanilla Executable = simple Arguments = 4 10 +ProjectName = \"ConnectTrain\" Log = simple.log Output = simple.out Error = simple.error requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ON_EXIT Queue Make sure you've built the simple program. If you need to, go back to the instructions for your first job to do it again. We are going to get a bit more sophisticated in submitting our jobs now. Let's have three windows open. In one window you'll submit the job. In the second you will watch the queue. And in the third you will watch what DAGMan does. First we will create the most minimal DAG that can be created: a DAG with just one node. Put the text below into a file named simple.dag . job simple submit In your first window, submit the DAG: $ condor_submit_dag simple.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : simple.dag.condor.sub Log of DAGMan debugging messages : simple.dag.dagman.out Log of Condor library output : simple.dag.lib.out Log of Condor library error messages : simple.dag.lib.err Log of the life of condor_dagman itself : simple.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 61. ----------------------------------------------------------------------- In the second window, watch the queue (press Ctrl+C when finished watching to kill this process): $ watch -n 10 condor_q USER -nobatch -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:00:01 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:01:25 R 0 0.3 condor_dagman 62.0 roy 6/21 22:51 0+00:00:00 I 0 0.7 simple 4 10 2 jobs; 0 completed, 0 removed, 1 idle, 1 running, 0 held, 0 suspended -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:03:47 R 0 0.3 condor_dagman 62.0 roy 6/21 22:51 0+00:00:03 R 0 0.7 simple 4 10 2 jobs; 0 completed, 0 removed, 0 idle, 2 running, 0 held, 0 suspended -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Ctrl-C In the third window, watch what DAGMan does: $ tail -f --lines=500 simple.dag.dagman.out 6/21/12 22:51:13 Setting maximum accepts per cycle 8. 06/21/12 22:51:13 ****************************************************** 06/21/12 22:51:13 ** condor_scheduniv_exec.61.0 (CONDOR_DAGMAN) STARTING UP 06/21/12 22:51:13 ** /usr/bin/condor_dagman 06/21/12 22:51:13 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/21/12 22:51:13 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/21/12 22:51:13 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/21/12 22:51:13 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/21/12 22:51:13 ** PID = 5812 06/21/12 22:51:13 ** Log last touched 6/21 22:51:00 06/21/12 22:51:13 ****************************************************** 06/21/12 22:51:13 Using config source: /etc/condor/condor_config 06/21/12 22:51:13 Using local config sources: 06/21/12 22:51:13 /etc/condor/config.d/00-chtc-global.conf 06/21/12 22:51:13 /etc/condor/config.d/01-chtc-submit.conf 06/21/12 22:51:13 /etc/condor/config.d/02-chtc-flocking.conf 06/21/12 22:51:13 /etc/condor/config.d/03-chtc-jobrouter.conf 06/21/12 22:51:13 /etc/condor/config.d/04-chtc-blacklist.conf 06/21/12 22:51:13 /etc/condor/config.d/99-osg-ss-group.conf 06/21/12 22:51:13 /etc/condor/config.d/99-roy-extras.conf 06/21/12 22:51:13 /etc/condor/condor_config.local 06/21/12 22:51:13 DaemonCore: command socket at <128.104.100.55:60417> 06/21/12 22:51:13 DaemonCore: private command socket at <128.104.100.55:60417> 06/21/12 22:51:13 Setting maximum accepts per cycle 8. 06/21/12 22:51:13 DAGMAN_USE_STRICT setting: 0 06/21/12 22:51:13 DAGMAN_VERBOSITY setting: 3 06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880 06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_ENABLE setting: False 06/21/12 22:51:13 DAGMAN_SUBMIT_DELAY setting: 0 06/21/12 22:51:13 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6 06/21/12 22:51:13 DAGMAN_STARTUP_CYCLE_DETECT setting: False 06/21/12 22:51:13 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 5 06/21/12 22:51:13 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5 06/21/12 22:51:13 allow_events (DAGMAN_IGNORE_DUPLICATE_JOB_EXECUTION, DAGMAN_ALLOW_EVENTS) setting: 114 06/21/12 22:51:13 DAGMAN_RETRY_SUBMIT_FIRST setting: True 06/21/12 22:51:13 DAGMAN_RETRY_NODE_FIRST setting: False 06/21/12 22:51:13 DAGMAN_MAX_JOBS_IDLE setting: 0 06/21/12 22:51:13 DAGMAN_MAX_JOBS_SUBMITTED setting: 0 06/21/12 22:51:15 DAGMAN_MAX_PRE_SCRIPTS setting: 0 06/21/12 22:51:15 DAGMAN_MAX_POST_SCRIPTS setting: 0 06/21/12 22:51:15 DAGMAN_ALLOW_LOG_ERROR setting: False 06/21/12 22:51:15 DAGMAN_MUNGE_NODE_NAMES setting: True 06/21/12 22:51:15 DAGMAN_PROHIBIT_MULTI_JOBS setting: False 06/21/12 22:51:15 DAGMAN_SUBMIT_DEPTH_FIRST setting: False 06/21/12 22:51:15 DAGMAN_ALWAYS_RUN_POST setting: True 06/21/12 22:51:15 DAGMAN_ABORT_DUPLICATES setting: True 06/21/12 22:51:15 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: True 06/21/12 22:51:15 DAGMAN_PENDING_REPORT_INTERVAL setting: 600 06/21/12 22:51:15 DAGMAN_AUTO_RESCUE setting: True 06/21/12 22:51:15 DAGMAN_MAX_RESCUE_NUM setting: 100 06/21/12 22:51:15 DAGMAN_WRITE_PARTIAL_RESCUE setting: True 06/21/12 22:51:15 DAGMAN_DEFAULT_NODE_LOG setting: null 06/21/12 22:51:15 DAGMAN_GENERATE_SUBDAG_SUBMITS setting: True 06/21/12 22:51:15 ALL_DEBUG setting: 06/21/12 22:51:15 DAGMAN_DEBUG setting: 06/21/12 22:51:15 argv[0] == \"condor_scheduniv_exec.61.0\" 06/21/12 22:51:15 argv[1] == \"-Lockfile\" 06/21/12 22:51:15 argv[2] == \"simple.dag.lock\" 06/21/12 22:51:15 argv[3] == \"-AutoRescue\" 06/21/12 22:51:15 argv[4] == \"1\" 06/21/12 22:51:15 argv[5] == \"-DoRescueFrom\" 06/21/12 22:51:15 argv[6] == \"0\" 06/21/12 22:51:15 argv[7] == \"-Dag\" 06/21/12 22:51:15 argv[8] == \"simple.dag\" 06/21/12 22:51:15 argv[9] == \"-CsdVersion\" 06/21/12 22:51:15 argv[10] == \"$CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $\" 06/21/12 22:51:15 argv[11] == \"-Force\" 06/21/12 22:51:15 argv[12] == \"-Dagman\" 06/21/12 22:51:15 argv[13] == \"/usr/bin/condor_dagman\" 06/21/12 22:51:15 Default node log file is: </home/roy/condor/simple.dag.nodes.log> 06/21/12 22:51:15 DAG Lockfile will be written to simple.dag.lock 06/21/12 22:51:15 DAG Input file is simple.dag 06/21/12 22:51:15 Parsing 1 dagfiles 06/21/12 22:51:15 Parsing simple.dag ... 06/21/12 22:51:15 Dag contains 1 total jobs 06/21/12 22:51:15 Sleeping for 12 seconds to ensure ProcessId uniqueness 06/21/12 22:51:27 Bootstrapping... 06/21/12 22:51:27 Number of pre-completed nodes: 0 06/21/12 22:51:27 Registering condor_event_timer... 06/21/12 22:51:28 Sleeping for one second for log file consistency 06/21/12 22:51:29 MultiLogFiles: truncating log file /home/roy/condor/simple.log 06/21/12 22:51:29 Submitting Condor Node Simple job(s)... # Here's where the job is submitted 06/21/12 22:51:29 submitting: condor_submit -a dag_node_name' '=' 'Simple -a +DAGManJobId' '=' '61 -a DAGManJobId' '=' '61 -a submit_event_notes' '=' 'DAG' 'Node:' 'Simple -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"\" submit 06/21/12 22:51:30 From submit: Submitting job(s). 06/21/12 22:51:30 From submit: 1 job(s) submitted to cluster 62. 06/21/12 22:51:30 assigned Condor ID (62.0.0) 06/21/12 22:51:30 Just submitted 1 job this cycle... 06/21/12 22:51:30 Currently monitoring 1 Condor log file(s) 06/21/12 22:51:30 Event: ULOG_SUBMIT for Condor Node Simple (62.0.0) 06/21/12 22:51:30 Number of idle job procs: 1 06/21/12 22:51:30 Of 1 nodes total: 06/21/12 22:51:30 Done Pre Queued Post Ready Un-Ready Failed 06/21/12 22:51:30 === === === === === === === 06/21/12 22:51:30 0 0 1 0 0 0 0 06/21/12 22:51:30 0 job proc(s) currently held 06/21/12 22:55:05 Currently monitoring 1 Condor log file(s) # Here's where DAGMan noticed that the job is running 06/21/12 22:55:05 Event: ULOG_EXECUTE for Condor Node Simple (62.0.0) 06/21/12 22:55:05 Number of idle job procs: 0 06/21/12 22:55:10 Currently monitoring 1 Condor log file(s) 06/21/12 22:55:10 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0) 06/21/12 22:56:05 Currently monitoring 1 Condor log file(s) 06/21/12 22:56:05 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0) # Here's where DAGMan noticed that the job finished. 06/21/12 22:56:05 Event: ULOG_JOB_TERMINATED for Condor Node Simple (62.0.0) 06/21/12 22:56:05 Node Simple job proc (62.0.0) completed successfully. 06/21/12 22:56:05 Node Simple job completed 06/21/12 22:56:05 Number of idle job procs: 0 06/21/12 22:56:05 Of 1 nodes total: 06/21/12 22:56:05 Done Pre Queued Post Ready Un-Ready Failed 06/21/12 22:56:05 === === === === === === === 06/21/12 22:56:05 1 0 0 0 0 0 0 06/21/12 22:56:05 0 job proc(s) currently held # Here's where DAGMan noticed that all the work is done. 06/21/12 22:56:05 All jobs Completed! 06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/21/12 22:56:05 Note: 0 total job deferrals because of node category throttles 06/21/12 22:56:05 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/21/12 22:56:05 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/21/12 22:56:05 **** condor_scheduniv_exec.61.0 (condor_DAGMAN) pid 5812 EXITING WITH STATUS 0 Now verify your results: $ cat simple.log 000 (062.000.000) 06/21 22:51:30 Job submitted from host: <128.104.100.55:9618?sock=28867_10e4_2> DAG Node: Simple ... 001 (062.000.000) 06/21 22:55:00 Job executing on host: <128.104.58.36:46761> ... 006 (062.000.000) 06/21 22:55:09 Image size of job updated: 750 3 - MemoryUsage of job (MB) 2324 - ResidentSetSize of job (KB) ... 006 (062.000.000) 06/21 22:56:00 Image size of job updated: 780 3 - MemoryUsage of job (MB) 2324 - ResidentSetSize of job (KB) ... 005 (062.000.000) 06/21 22:56:00 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 57 - Run Bytes Sent By Job 608490 - Run Bytes Received By Job 57 - Total Bytes Sent By Job 608490 - Total Bytes Received By Job Partitionable Resources : Usage Request Cpus : 1 Disk (KB) : 750 750 Memory (MB) : 3 3 ... $ cat simple.out Thinking really hard for 4 seconds... We calculated: 20 Looking at DAGMan's various files, we see that DAGMan itself ran as a Condor job (specifically, a scheduler universe job). $ ls simple.dag.* simple.dag.condor.sub simple.dag.dagman.log simple.dag.dagman.out simple.dag.lib.err simple.dag.lib.out $ cat simple.dag.condor.sub # Filename: simple.dag.condor.sub # Generated by condor_submit_dag simple.dag universe = scheduler executable = /usr/bin/condor_dagman getenv = True output = simple.dag.lib.out error = simple.dag.lib.err log = simple.dag.dagman.log remove_kill_sig = SIGUSR1 +OtherJobRemoveRequirements = \"DAGManJobId == $(cluster)\" # Note: default on_exit_remove expression: # ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2)) # attempts to ensure that DAGMan is automatically # requeued by the schedd if it exits abnormally or # is killed (e.g., during a reboot). on_exit_remove = ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2)) copy_to_spool = False arguments = \"-f -l . -Lockfile simple.dag.lock -AutoRescue 1 -DoRescueFrom 0 -Dag simple.dag -CsdVersion $CondorVersion:' '7.7.6' 'Apr' '16' '2012' 'BuildID:' '34175' 'PRE-RELEASE-UWCS' '$ -Force -Dagman /usr/bin/condor_dagman\" environment = _CONDOR_DAGMAN_LOG=simple.dag.dagman.out;_CONDOR_MAX_DAGMAN_LOG=0 queue Clean up some of these files: $ rm simple.dag.*","title":"Submitting a simple DAG"},{"location":"DataSanJose2019/CI/09-SimpleDAG/#on-your-own","text":"Why does DAGman run as a Condor job? Look at the submit file for DAGMan: what does on_exit_remove do? Why is this here?","title":"On your own"},{"location":"DataSanJose2019/CI/10-ComplexDAG/","text":"A More Complex DAG Objective The objective of this exercise is to run a real set of jobs with DAGMan. Make your job submission files We'll run our goatbrot example. If you didn't read about it yet, please do so now . We are going to make a DAG with four simultaneous jobs ( goatbrot ) and one final node to stitch them together ( montage ). This means we have five jobs. We're going to run goatbrot with more iterations (100,000) so it will take longer to run. You can create your five jobs. The goatbrot jobs very similar to each other, but they have slightly different parameters (arguments) and output files. I have placed the goatbrot executable in my public directory: /stash/user/rquick/public/goatbrot-master/goatbrot goatbrot1.sub executable = /stash/user/rquick/public/goatbrot-master/goatbrot arguments = -i 100000 -c -0.75,0.75 -w 1.5 -s 500,500 -o tile_0_0.ppm log = goatbrot.log output = goatbrot.out.0.0 error = goatbrot.err.0.0 requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ONEXIT queue goatbrot2.sub executable = /stash/user/rquick/public/goatbrot-master/goatbrot arguments = -i 100000 -c 0.75,0.75 -w 1.5 -s 500,500 -o tile_0_1.ppm log = goatbrot.log output = goatbrot.out.0.1 error = goatbrot.err.0.1 requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ONEXIT queue goatbrot3.sub executable = /stash/user/rquick/public/goatbrot-master/goatbrot arguments = -i 100000 -c -0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_0.ppm log = goatbrot.log output = goatbrot.out.1.0 error = goatbrot.err.1.0 requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ONEXIT queue goatbrot4.sub executable = /stash/user/rquick/public/goatbrot-master/goatbrot arguments = -i 100000 -c 0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_1.ppm log = goatbrot.log output = goatbrot.out.1.1 error = goatbrot.err.1.1 requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ONEXIT queue montage.sub You should notice a few things about the montage submission file: The transfer_input_files statement refers to the files created by the other jobs. We do not transfer the montage program because it is on OASIS. universe = vanilla executable = wrapper_montage.sh arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.gif should_transfer_files = YES when_to_transfer_output = ONEXIT transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm transfer_executable = true output = montage.out error = montage.err log = montage.log requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue wrapper_montage.sh Because we are using OASIS, we will need to create a wrapper script to load the ImageMagick module so that we can use it to create the montage. Put the following lines into wrapper_montage.sh : source /cvmfs/oasis.opensciencegrid.org/osg/modules/lmod/current/init/bash module load imagemagick montage tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.gif Make your DAG In a file called goatbrot.dag , you have your DAG specification: JOB g1 goatbrot1.sub JOB g2 goatbrot2.sub JOB g3 goatbrot3.sub JOB g4 goatbrot4.sub JOB montage montage.sub PARENT g1 g2 g3 g4 CHILD montage Ask yourself: do you know how we ensure that all the goatbrot commands can run simultaneously and all of them will complete before we run the montage job? Running the DAG Submit your DAG: $ condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 71. ----------------------------------------------------------------------- Watch your DAG Watch with condor_q: $ watch condor_q USER -nobatch Here we see DAGMan running: -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:10 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended DAGMan has submitted the goatbrot jobs, but they haven't started running yet (note that the I status stands for Idle): -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:10 R 0 0.3 condor_dagman 69.0 kagross 8/19 11:38 0+00:00:00 I 0 0.0 goatbrot -i 100000 70.0 kagross 8/19 11:38 0+00:00:00 I 0 0.0 goatbrot -i 100000 71.0 kagross 8/19 11:38 0+00:00:00 I 0 0.0 goatbrot -i 100000 72.0 kagross 8/19 11:38 0+00:00:00 I 0 0.0 goatbrot -i 100000 6 jobs; 0 completed, 0 removed, 4 idle, 2 running, 0 held, 0 suspended They're running! (All four jobs are in state R - running) -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:15 R 0 0.3 condor_dagman 69.0 kagross 8/19 11:38 0+00:00:05 R 0 0.0 goatbrot -i 100000 70.0 kagross 8/19 11:38 0+00:00:05 R 0 0.0 goatbrot -i 100000 71.0 kagross 8/19 11:38 0+00:00:05 R 0 0.0 goatbrot -i 100000 72.0 kagross 8/19 11:38 0+00:00:05 R 0 0.0 goatbrot -i 100000 5 jobs; 0 completed, 0 removed, 0 idle, 5 running, 0 held, 0 suspended Two of the jobs have finished, while the others are still running (remember that completed jobs disappear from condor_q output): -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:20 R 0 0.3 condor_dagman 71.0 kagross 8/19 11:38 0+00:00:10 R 0 0.0 goatbrot -i 100000 72.0 kagross 8/19 11:38 0+00:00:10 R 0 0.0 goatbrot -i 100000 3 jobs; 0 completed, 0 removed, 0 idle, 3 running, 0 held, 0 suspended They finished, but DAGMan hasn't noticed yet. It only checks periodically: -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:30 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended DAGMan submitted and ran the montage job. It ran so fast I didn't capture it running. DAGMan will finish up soon -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:01:01 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Now it's all done: -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Examine your results. For some reason, goatbrot prints everything to stderr, not stdout. $ cat goatbrot.err.0.0 Complex image: Center: -0.75 + 0.75i Width: 1.5 Height: 1.5 Upper Left: -1.5 + 1.5i Lower Right: 0 + 0i Output image: Filename: tile_0_0.ppm Width, Height: 500, 500 Theme: beej Antialiased: no Mandelbrot: Max Iterations: 100000 Continuous: no Goatbrot: Multithreading: not supported in this build Completed: 100.0% Examine your log files ( goatbrot.log and montage.log ) and DAGMan output file ( goatbrot.dag.dagman.out ). Do they look as you expect? Can you see the progress of the DAG in the DAGMan output file? Does your final Mandlebrot image ( mandle.gif ) look correct? To view it we can use Stash. $ cp mandle.gif ~/stash/public/ And now you can go to http://stash.osgconnect.net/~USER . You will see mandle.gif listed. You can click on it to view it. Clean up your results. Be careful about deleting the goatbrot.dag. files, you do not want to delete the goatbrot.dag file, just goatbrot.dag. . $ rm goatbrot.dag.* $ rm goatbrot.out.* $ rm goatbrot.err.* On your own. Re-run your DAG. When jobs are running, try condor_q -dag . What does it do differently? Challenge, if you have time: Make a bigger DAG by making more tiles in the same area.","title":"A More Complex DAG"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#a-more-complex-dag","text":"","title":"A More Complex DAG"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#objective","text":"The objective of this exercise is to run a real set of jobs with DAGMan.","title":"Objective"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#make-your-job-submission-files","text":"We'll run our goatbrot example. If you didn't read about it yet, please do so now . We are going to make a DAG with four simultaneous jobs ( goatbrot ) and one final node to stitch them together ( montage ). This means we have five jobs. We're going to run goatbrot with more iterations (100,000) so it will take longer to run. You can create your five jobs. The goatbrot jobs very similar to each other, but they have slightly different parameters (arguments) and output files. I have placed the goatbrot executable in my public directory: /stash/user/rquick/public/goatbrot-master/goatbrot","title":"Make your job submission files"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#goatbrot1sub","text":"executable = /stash/user/rquick/public/goatbrot-master/goatbrot arguments = -i 100000 -c -0.75,0.75 -w 1.5 -s 500,500 -o tile_0_0.ppm log = goatbrot.log output = goatbrot.out.0.0 error = goatbrot.err.0.0 requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ONEXIT queue","title":"goatbrot1.sub"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#goatbrot2sub","text":"executable = /stash/user/rquick/public/goatbrot-master/goatbrot arguments = -i 100000 -c 0.75,0.75 -w 1.5 -s 500,500 -o tile_0_1.ppm log = goatbrot.log output = goatbrot.out.0.1 error = goatbrot.err.0.1 requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ONEXIT queue","title":"goatbrot2.sub"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#goatbrot3sub","text":"executable = /stash/user/rquick/public/goatbrot-master/goatbrot arguments = -i 100000 -c -0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_0.ppm log = goatbrot.log output = goatbrot.out.1.0 error = goatbrot.err.1.0 requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ONEXIT queue","title":"goatbrot3.sub"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#goatbrot4sub","text":"executable = /stash/user/rquick/public/goatbrot-master/goatbrot arguments = -i 100000 -c 0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_1.ppm log = goatbrot.log output = goatbrot.out.1.1 error = goatbrot.err.1.1 requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ONEXIT queue","title":"goatbrot4.sub"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#montagesub","text":"You should notice a few things about the montage submission file: The transfer_input_files statement refers to the files created by the other jobs. We do not transfer the montage program because it is on OASIS. universe = vanilla executable = wrapper_montage.sh arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.gif should_transfer_files = YES when_to_transfer_output = ONEXIT transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm transfer_executable = true output = montage.out error = montage.err log = montage.log requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue","title":"montage.sub"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#wrapper_montagesh","text":"Because we are using OASIS, we will need to create a wrapper script to load the ImageMagick module so that we can use it to create the montage. Put the following lines into wrapper_montage.sh : source /cvmfs/oasis.opensciencegrid.org/osg/modules/lmod/current/init/bash module load imagemagick montage tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.gif","title":"wrapper_montage.sh"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#make-your-dag","text":"In a file called goatbrot.dag , you have your DAG specification: JOB g1 goatbrot1.sub JOB g2 goatbrot2.sub JOB g3 goatbrot3.sub JOB g4 goatbrot4.sub JOB montage montage.sub PARENT g1 g2 g3 g4 CHILD montage Ask yourself: do you know how we ensure that all the goatbrot commands can run simultaneously and all of them will complete before we run the montage job?","title":"Make your DAG"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#running-the-dag","text":"Submit your DAG: $ condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 71. -----------------------------------------------------------------------","title":"Running the DAG"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#watch-your-dag","text":"Watch with condor_q: $ watch condor_q USER -nobatch Here we see DAGMan running: -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:10 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended DAGMan has submitted the goatbrot jobs, but they haven't started running yet (note that the I status stands for Idle): -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:10 R 0 0.3 condor_dagman 69.0 kagross 8/19 11:38 0+00:00:00 I 0 0.0 goatbrot -i 100000 70.0 kagross 8/19 11:38 0+00:00:00 I 0 0.0 goatbrot -i 100000 71.0 kagross 8/19 11:38 0+00:00:00 I 0 0.0 goatbrot -i 100000 72.0 kagross 8/19 11:38 0+00:00:00 I 0 0.0 goatbrot -i 100000 6 jobs; 0 completed, 0 removed, 4 idle, 2 running, 0 held, 0 suspended They're running! (All four jobs are in state R - running) -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:15 R 0 0.3 condor_dagman 69.0 kagross 8/19 11:38 0+00:00:05 R 0 0.0 goatbrot -i 100000 70.0 kagross 8/19 11:38 0+00:00:05 R 0 0.0 goatbrot -i 100000 71.0 kagross 8/19 11:38 0+00:00:05 R 0 0.0 goatbrot -i 100000 72.0 kagross 8/19 11:38 0+00:00:05 R 0 0.0 goatbrot -i 100000 5 jobs; 0 completed, 0 removed, 0 idle, 5 running, 0 held, 0 suspended Two of the jobs have finished, while the others are still running (remember that completed jobs disappear from condor_q output): -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:20 R 0 0.3 condor_dagman 71.0 kagross 8/19 11:38 0+00:00:10 R 0 0.0 goatbrot -i 100000 72.0 kagross 8/19 11:38 0+00:00:10 R 0 0.0 goatbrot -i 100000 3 jobs; 0 completed, 0 removed, 0 idle, 3 running, 0 held, 0 suspended They finished, but DAGMan hasn't noticed yet. It only checks periodically: -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:30 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended DAGMan submitted and ran the montage job. It ran so fast I didn't capture it running. DAGMan will finish up soon -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:01:01 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Now it's all done: -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Examine your results. For some reason, goatbrot prints everything to stderr, not stdout. $ cat goatbrot.err.0.0 Complex image: Center: -0.75 + 0.75i Width: 1.5 Height: 1.5 Upper Left: -1.5 + 1.5i Lower Right: 0 + 0i Output image: Filename: tile_0_0.ppm Width, Height: 500, 500 Theme: beej Antialiased: no Mandelbrot: Max Iterations: 100000 Continuous: no Goatbrot: Multithreading: not supported in this build Completed: 100.0% Examine your log files ( goatbrot.log and montage.log ) and DAGMan output file ( goatbrot.dag.dagman.out ). Do they look as you expect? Can you see the progress of the DAG in the DAGMan output file? Does your final Mandlebrot image ( mandle.gif ) look correct? To view it we can use Stash. $ cp mandle.gif ~/stash/public/ And now you can go to http://stash.osgconnect.net/~USER . You will see mandle.gif listed. You can click on it to view it. Clean up your results. Be careful about deleting the goatbrot.dag. files, you do not want to delete the goatbrot.dag file, just goatbrot.dag. . $ rm goatbrot.dag.* $ rm goatbrot.out.* $ rm goatbrot.err.*","title":"Watch your DAG"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#on-your-own","text":"Re-run your DAG. When jobs are running, try condor_q -dag . What does it do differently? Challenge, if you have time: Make a bigger DAG by making more tiles in the same area.","title":"On your own."},{"location":"DataSanJose2019/CI/11-HandlingFailure/","text":"Handling a DAG that fails Objective The objective of this exercise is to help you learn how DAGMan deals with job failures. DAGMan is built to help you recover from such failures. DAGMan can handle a situation where some of the nodes in a DAG fails. DAGMan will run as many nodes as possible, then create a \"rescue DAG\". A rescue DAG allows you to fix the problem and then resume your job where it left off. Recall that DAGMan decides that a jobs fails if its exit code is non-zero. Let's modify our montage job so that it fails. Work in the same directory where you did the last DAG. Edit montage.sub to add a -h to the arguments. It will look like this (the change is bolded): universe = vanilla executable = wrapper_montage.sh arguments = -h tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.jpg should_transfer_files = YES when_to_transfer_output = ONEXIT transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm transfer_executable = true output = montage.out error = montage.err log = goat.log requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue Submit the DAG again: $ condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 77. ----------------------------------------------------------------------- Use watch to watch the jobs until they finish. In a separate window, use tail --lines=500 -f goatbrot.dag.dagman.out to watch what DAGMan does. 06/22/12 17:57:41 Setting maximum accepts per cycle 8. 06/22/12 17:57:41 ****************************************************** 06/22/12 17:57:41 ** condor_scheduniv_exec.77.0 (CONDOR_DAGMAN) STARTING UP 06/22/12 17:57:41 ** /usr/bin/condor_dagman 06/22/12 17:57:41 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/22/12 17:57:41 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/22/12 17:57:41 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/22/12 17:57:41 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/22/12 17:57:41 ** PID = 26867 06/22/12 17:57:41 ** Log last touched time unavailable (No such file or directory) 06/22/12 17:57:41 ****************************************************** 06/22/12 17:57:41 Using config source: /etc/condor/condor_config 06/22/12 17:57:41 Using local config sources: 06/22/12 17:57:41 /etc/condor/config.d/00-chtc-global.conf 06/22/12 17:57:41 /etc/condor/config.d/01-chtc-submit.conf 06/22/12 17:57:41 /etc/condor/config.d/02-chtc-flocking.conf 06/22/12 17:57:41 /etc/condor/config.d/03-chtc-jobrouter.conf 06/22/12 17:57:41 /etc/condor/config.d/04-chtc-blacklist.conf 06/22/12 17:57:41 /etc/condor/config.d/99-osg-ss-group.conf 06/22/12 17:57:41 /etc/condor/config.d/99-roy-extras.conf 06/22/12 17:57:41 /etc/condor/condor_config.local ... output trimmed ... 06/22/12 18:08:42 Event: ULOG_EXECUTE for Condor Node montage (82.0.0) 06/22/12 18:08:42 Number of idle job procs: 0 06/22/12 18:08:42 Event: ULOG_IMAGE_SIZE for Condor Node montage (82.0.0) 06/22/12 18:08:42 Event: ULOG_JOB_TERMINATED for Condor Node montage (82.0.0) 06/22/12 18:08:42 Node montage job proc (82.0.0) failed with status 1. 06/22/12 18:08:42 Number of idle job procs: 0 06/22/12 18:08:42 Of 5 nodes total: 06/22/12 18:08:42 Done Pre Queued Post Ready Un-Ready Failed 06/22/12 18:08:42 === === === === === === === 06/22/12 18:08:42 4 0 0 0 0 0 1 06/22/12 18:08:42 0 job proc(s) currently held 06/22/12 18:08:42 Aborting DAG... 06/22/12 18:08:42 Writing Rescue DAG to goatbrot.dag.rescue001... 06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/22/12 18:08:42 Note: 0 total job deferrals because of node category throttles 06/22/12 18:08:42 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/22/12 18:08:42 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/22/12 18:08:42 **** condor_scheduniv_exec.77.0 (condor_DAGMAN) pid 26867 EXITING WITH STATUS 1 DAGMan notices that one of the jobs failed because it's exit code was non-zero. DAGMan ran as much of the DAG as possible and logged enough information to continue the run when the situation is resolved. Do you see the part where it wrote the resuce DAG? Look at the rescue DAG. It's called a partial DAG: it indicates what part of the DAG has already been completed. When you re-submit the original DAG, DAGMan will notice the rescue DAG and use it in combination with the original DAG. (The rescue DAG used to be the full DAG with nodes marked as done and you would ask DAGMan to run the new rescue DAG. For your simplicity DAGMan lets you resubmit the original DAG and it reads both files.) $ cat goatbrot.dag.rescue001 # Rescue DAG file, created after running # the goatbrot.dag DAG file # Created 6/22/2012 23:08:42 UTC # Rescue DAG version: 2.0.1 (partial) # # Total number of Nodes: 5 # Nodes premarked DONE: 4 # Nodes that failed: 1 # montage,<ENDLIST> DONE g1 DONE g2 DONE g3 DONE g4 From the comment near the top, we know that the montage node failed. Let's fix it by getting rid of the offending -h argument. Change montage.sub to look like: universe = vanilla executable = wrapper_montage.sh arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.jpg should_transfer_files = YES when_to_transfer_output = ONEXIT transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm transfer_executable = true output = montage.out error = montage.err log = goat.log requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue Now we can re-submit our original DAG and DAGMan will pick up where it left off. It will automatically notice the rescue DAG If you didn't fix the problem, DAGMan would generate another rescue DAG. $ condor_submit_dag goatbrot.dag Running rescue DAG 1 ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 83. ----------------------------------------------------------------------- $ tail -f goatbrot.dag.dagman.out 06/23/12 11:30:53 ****************************************************** 06/23/12 11:30:53 ** condor_scheduniv_exec.83.0 (CONDOR_DAGMAN) STARTING UP 06/23/12 11:30:53 ** /usr/bin/condor_dagman 06/23/12 11:30:53 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/23/12 11:30:53 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/23/12 11:30:53 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/23/12 11:30:53 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/23/12 11:30:53 ** PID = 28576 06/23/12 11:30:53 ** Log last touched 6/22 18:08:42 06/23/12 11:30:53 ****************************************************** 06/23/12 11:30:53 Using config source: /etc/condor/condor_config ... Here is where DAGMAN notices that there is a rescue DAG: 06/23/12 11:30:53 Parsing 1 dagfiles 06/23/12 11:30:53 Parsing goatbrot.dag ... 06/23/12 11:30:53 Found rescue DAG number 1; running goatbrot.dag.rescue001 in combination with normal DAG file 06/23/12 11:30:53 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 06/23/12 11:30:53 USING RESCUE DAG goatbrot.dag.rescue001 06/23/12 11:30:53 Dag contains 5 total jobs Shortly thereafter it sees that four jobs have already finished: 06/23/12 11:31:05 Bootstrapping... 06/23/12 11:31:05 Number of pre-completed nodes: 4 06/23/12 11:31:05 Registering condor_event_timer... 06/23/12 11:31:06 Sleeping for one second for log file consistency 06/23/12 11:31:07 MultiLogFiles: truncating log file /home/roy/condor/goatbrot/montage.log Here is where DAGMan resubmits the montage job and waits for it to complete: 06/23/12 11:31:07 Submitting Condor Node montage job(s)... 06/23/12 11:31:07 submitting: condor_submit -a dag_node_name' '=' 'montage -a +DAGManJobId' '=' '83 -a DAGManJobId' '=' '83 -a submit_event_notes' '=' 'DAG' 'Node:' 'montage -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"g1,g2,g3,g4\" montage.sub 06/23/12 11:31:07 From submit: Submitting job(s). 06/23/12 11:31:07 From submit: 1 job(s) submitted to cluster 84. 06/23/12 11:31:07 assigned Condor ID (84.0.0) 06/23/12 11:31:07 Just submitted 1 job this cycle... 06/23/12 11:31:07 Currently monitoring 1 Condor log file(s) 06/23/12 11:31:07 Event: ULOG_SUBMIT for Condor Node montage (84.0.0) 06/23/12 11:31:07 Number of idle job procs: 1 06/23/12 11:31:07 Of 5 nodes total: 06/23/12 11:31:07 Done Pre Queued Post Ready Un-Ready Failed 06/23/12 11:31:07 === === === === === === === 06/23/12 11:31:07 4 0 1 0 0 0 0 06/23/12 11:31:07 0 job proc(s) currently held 06/23/12 11:40:22 Currently monitoring 1 Condor log file(s) 06/23/12 11:40:22 Event: ULOG_EXECUTE for Condor Node montage (84.0.0) 06/23/12 11:40:22 Number of idle job procs: 0 06/23/12 11:40:22 Event: ULOG_IMAGE_SIZE for Condor Node montage (84.0.0) 06/23/12 11:40:22 Event: ULOG_JOB_TERMINATED for Condor Node montage (84.0.0) This is where the montage finished: 06/23/12 11:40:22 Node montage job proc (84.0.0) completed successfully. 06/23/12 11:40:22 Node montage job completed 06/23/12 11:40:22 Number of idle job procs: 0 06/23/12 11:40:22 Of 5 nodes total: 06/23/12 11:40:22 Done Pre Queued Post Ready Un-Ready Failed 06/23/12 11:40:22 === === === === === === === 06/23/12 11:40:22 5 0 0 0 0 0 0 06/23/12 11:40:22 0 job proc(s) currently held And here DAGMan decides that the work is all done: 06/23/12 11:40:22 All jobs Completed! 06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/23/12 11:40:22 Note: 0 total job deferrals because of node category throttles 06/23/12 11:40:22 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/23/12 11:40:22 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/23/12 11:40:22 **** condor_scheduniv_exec.83.0 (condor_DAGMAN) pid 28576 EXITING WITH STATUS 0</pre> Success! Now go ahead and clean up. Challenge If you have time, add an extra node to the DAG. Copy our original simple program, but make it exit with a 1 instead of a 0. DAGMan would consider this a failure, but you'll tell DAGMan that it's really a success. This is reasonable--many real world programs use a variety of return codes, and you might need to help DAGMan distinguish success from failure. Write a POST script that checks the return value. Check the Condor manual to see how to describe your post script.","title":"Handling a DAG that fails"},{"location":"DataSanJose2019/CI/11-HandlingFailure/#handling-a-dag-that-fails","text":"","title":"Handling a DAG that fails"},{"location":"DataSanJose2019/CI/11-HandlingFailure/#objective","text":"The objective of this exercise is to help you learn how DAGMan deals with job failures. DAGMan is built to help you recover from such failures. DAGMan can handle a situation where some of the nodes in a DAG fails. DAGMan will run as many nodes as possible, then create a \"rescue DAG\". A rescue DAG allows you to fix the problem and then resume your job where it left off. Recall that DAGMan decides that a jobs fails if its exit code is non-zero. Let's modify our montage job so that it fails. Work in the same directory where you did the last DAG. Edit montage.sub to add a -h to the arguments. It will look like this (the change is bolded): universe = vanilla executable = wrapper_montage.sh arguments = -h tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.jpg should_transfer_files = YES when_to_transfer_output = ONEXIT transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm transfer_executable = true output = montage.out error = montage.err log = goat.log requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue Submit the DAG again: $ condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 77. ----------------------------------------------------------------------- Use watch to watch the jobs until they finish. In a separate window, use tail --lines=500 -f goatbrot.dag.dagman.out to watch what DAGMan does. 06/22/12 17:57:41 Setting maximum accepts per cycle 8. 06/22/12 17:57:41 ****************************************************** 06/22/12 17:57:41 ** condor_scheduniv_exec.77.0 (CONDOR_DAGMAN) STARTING UP 06/22/12 17:57:41 ** /usr/bin/condor_dagman 06/22/12 17:57:41 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/22/12 17:57:41 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/22/12 17:57:41 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/22/12 17:57:41 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/22/12 17:57:41 ** PID = 26867 06/22/12 17:57:41 ** Log last touched time unavailable (No such file or directory) 06/22/12 17:57:41 ****************************************************** 06/22/12 17:57:41 Using config source: /etc/condor/condor_config 06/22/12 17:57:41 Using local config sources: 06/22/12 17:57:41 /etc/condor/config.d/00-chtc-global.conf 06/22/12 17:57:41 /etc/condor/config.d/01-chtc-submit.conf 06/22/12 17:57:41 /etc/condor/config.d/02-chtc-flocking.conf 06/22/12 17:57:41 /etc/condor/config.d/03-chtc-jobrouter.conf 06/22/12 17:57:41 /etc/condor/config.d/04-chtc-blacklist.conf 06/22/12 17:57:41 /etc/condor/config.d/99-osg-ss-group.conf 06/22/12 17:57:41 /etc/condor/config.d/99-roy-extras.conf 06/22/12 17:57:41 /etc/condor/condor_config.local ... output trimmed ... 06/22/12 18:08:42 Event: ULOG_EXECUTE for Condor Node montage (82.0.0) 06/22/12 18:08:42 Number of idle job procs: 0 06/22/12 18:08:42 Event: ULOG_IMAGE_SIZE for Condor Node montage (82.0.0) 06/22/12 18:08:42 Event: ULOG_JOB_TERMINATED for Condor Node montage (82.0.0) 06/22/12 18:08:42 Node montage job proc (82.0.0) failed with status 1. 06/22/12 18:08:42 Number of idle job procs: 0 06/22/12 18:08:42 Of 5 nodes total: 06/22/12 18:08:42 Done Pre Queued Post Ready Un-Ready Failed 06/22/12 18:08:42 === === === === === === === 06/22/12 18:08:42 4 0 0 0 0 0 1 06/22/12 18:08:42 0 job proc(s) currently held 06/22/12 18:08:42 Aborting DAG... 06/22/12 18:08:42 Writing Rescue DAG to goatbrot.dag.rescue001... 06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/22/12 18:08:42 Note: 0 total job deferrals because of node category throttles 06/22/12 18:08:42 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/22/12 18:08:42 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/22/12 18:08:42 **** condor_scheduniv_exec.77.0 (condor_DAGMAN) pid 26867 EXITING WITH STATUS 1 DAGMan notices that one of the jobs failed because it's exit code was non-zero. DAGMan ran as much of the DAG as possible and logged enough information to continue the run when the situation is resolved. Do you see the part where it wrote the resuce DAG? Look at the rescue DAG. It's called a partial DAG: it indicates what part of the DAG has already been completed. When you re-submit the original DAG, DAGMan will notice the rescue DAG and use it in combination with the original DAG. (The rescue DAG used to be the full DAG with nodes marked as done and you would ask DAGMan to run the new rescue DAG. For your simplicity DAGMan lets you resubmit the original DAG and it reads both files.) $ cat goatbrot.dag.rescue001 # Rescue DAG file, created after running # the goatbrot.dag DAG file # Created 6/22/2012 23:08:42 UTC # Rescue DAG version: 2.0.1 (partial) # # Total number of Nodes: 5 # Nodes premarked DONE: 4 # Nodes that failed: 1 # montage,<ENDLIST> DONE g1 DONE g2 DONE g3 DONE g4 From the comment near the top, we know that the montage node failed. Let's fix it by getting rid of the offending -h argument. Change montage.sub to look like: universe = vanilla executable = wrapper_montage.sh arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.jpg should_transfer_files = YES when_to_transfer_output = ONEXIT transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm transfer_executable = true output = montage.out error = montage.err log = goat.log requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue Now we can re-submit our original DAG and DAGMan will pick up where it left off. It will automatically notice the rescue DAG If you didn't fix the problem, DAGMan would generate another rescue DAG. $ condor_submit_dag goatbrot.dag Running rescue DAG 1 ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 83. ----------------------------------------------------------------------- $ tail -f goatbrot.dag.dagman.out 06/23/12 11:30:53 ****************************************************** 06/23/12 11:30:53 ** condor_scheduniv_exec.83.0 (CONDOR_DAGMAN) STARTING UP 06/23/12 11:30:53 ** /usr/bin/condor_dagman 06/23/12 11:30:53 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/23/12 11:30:53 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/23/12 11:30:53 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/23/12 11:30:53 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/23/12 11:30:53 ** PID = 28576 06/23/12 11:30:53 ** Log last touched 6/22 18:08:42 06/23/12 11:30:53 ****************************************************** 06/23/12 11:30:53 Using config source: /etc/condor/condor_config ... Here is where DAGMAN notices that there is a rescue DAG: 06/23/12 11:30:53 Parsing 1 dagfiles 06/23/12 11:30:53 Parsing goatbrot.dag ... 06/23/12 11:30:53 Found rescue DAG number 1; running goatbrot.dag.rescue001 in combination with normal DAG file 06/23/12 11:30:53 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 06/23/12 11:30:53 USING RESCUE DAG goatbrot.dag.rescue001 06/23/12 11:30:53 Dag contains 5 total jobs Shortly thereafter it sees that four jobs have already finished: 06/23/12 11:31:05 Bootstrapping... 06/23/12 11:31:05 Number of pre-completed nodes: 4 06/23/12 11:31:05 Registering condor_event_timer... 06/23/12 11:31:06 Sleeping for one second for log file consistency 06/23/12 11:31:07 MultiLogFiles: truncating log file /home/roy/condor/goatbrot/montage.log Here is where DAGMan resubmits the montage job and waits for it to complete: 06/23/12 11:31:07 Submitting Condor Node montage job(s)... 06/23/12 11:31:07 submitting: condor_submit -a dag_node_name' '=' 'montage -a +DAGManJobId' '=' '83 -a DAGManJobId' '=' '83 -a submit_event_notes' '=' 'DAG' 'Node:' 'montage -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"g1,g2,g3,g4\" montage.sub 06/23/12 11:31:07 From submit: Submitting job(s). 06/23/12 11:31:07 From submit: 1 job(s) submitted to cluster 84. 06/23/12 11:31:07 assigned Condor ID (84.0.0) 06/23/12 11:31:07 Just submitted 1 job this cycle... 06/23/12 11:31:07 Currently monitoring 1 Condor log file(s) 06/23/12 11:31:07 Event: ULOG_SUBMIT for Condor Node montage (84.0.0) 06/23/12 11:31:07 Number of idle job procs: 1 06/23/12 11:31:07 Of 5 nodes total: 06/23/12 11:31:07 Done Pre Queued Post Ready Un-Ready Failed 06/23/12 11:31:07 === === === === === === === 06/23/12 11:31:07 4 0 1 0 0 0 0 06/23/12 11:31:07 0 job proc(s) currently held 06/23/12 11:40:22 Currently monitoring 1 Condor log file(s) 06/23/12 11:40:22 Event: ULOG_EXECUTE for Condor Node montage (84.0.0) 06/23/12 11:40:22 Number of idle job procs: 0 06/23/12 11:40:22 Event: ULOG_IMAGE_SIZE for Condor Node montage (84.0.0) 06/23/12 11:40:22 Event: ULOG_JOB_TERMINATED for Condor Node montage (84.0.0) This is where the montage finished: 06/23/12 11:40:22 Node montage job proc (84.0.0) completed successfully. 06/23/12 11:40:22 Node montage job completed 06/23/12 11:40:22 Number of idle job procs: 0 06/23/12 11:40:22 Of 5 nodes total: 06/23/12 11:40:22 Done Pre Queued Post Ready Un-Ready Failed 06/23/12 11:40:22 === === === === === === === 06/23/12 11:40:22 5 0 0 0 0 0 0 06/23/12 11:40:22 0 job proc(s) currently held And here DAGMan decides that the work is all done: 06/23/12 11:40:22 All jobs Completed! 06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/23/12 11:40:22 Note: 0 total job deferrals because of node category throttles 06/23/12 11:40:22 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/23/12 11:40:22 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/23/12 11:40:22 **** condor_scheduniv_exec.83.0 (condor_DAGMAN) pid 28576 EXITING WITH STATUS 0</pre> Success! Now go ahead and clean up.","title":"Objective"},{"location":"DataSanJose2019/CI/11-HandlingFailure/#challenge","text":"If you have time, add an extra node to the DAG. Copy our original simple program, but make it exit with a 1 instead of a 0. DAGMan would consider this a failure, but you'll tell DAGMan that it's really a success. This is reasonable--many real world programs use a variety of return codes, and you might need to help DAGMan distinguish success from failure. Write a POST script that checks the return value. Check the Condor manual to see how to describe your post script.","title":"Challenge"},{"location":"DataSanJose2019/CI/12-VariableSubstitution/","text":"Simpler DAGs with variable substitutions Objective The objective of this exercise is to help you write simpler DAGs by using variable substitutions in your submit files. If you look at the DAG we made, you might find it a bit tedious because each goatbrot job has a separate Condor submit file. They're nearly identical except for a couple of parameters. Can we make it simpler? Yes, we can! Declare your variables First you need to declare your variables in your submit file. Make one submit file for all of your goatbrot jobs. Here's what it looks like. Call it goatbrot.sub : executable = /tmp/goatbrot-master/goatbrot arguments = -i 100000 -c $(CENTERX),$(CENTERY) -w 1.5 -s 500,500 -o tile_$(TILEY)_$(TILEX).ppm log = goatbrot.log output = goatbrot.out.$(TILEY).$(TILEX) error = goatbrot.err.$(TILEY).$(TILEX) should_transfer_files = YES when_to_transfer_output = ONEXIT requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue Then you need to change your DAG to use VARS for variable substitution. Here's what one of the jobs would look like: JOB g1 goatbrot.sub VARS g1 CENTERX=\"-0.75\" VARS g1 CENTERY=\"0.75\" VARS g1 TILEX=\"0\" VARS g1 TILEY=\"0\" Edit your DAG similarly for all of your goatbrot jobs. If you need help, check the Condor manual for for a description of how to use VARS . What happens?","title":"Simpler DAGs with variable substitutions"},{"location":"DataSanJose2019/CI/12-VariableSubstitution/#simpler-dags-with-variable-substitutions","text":"","title":"Simpler DAGs with variable substitutions"},{"location":"DataSanJose2019/CI/12-VariableSubstitution/#objective","text":"The objective of this exercise is to help you write simpler DAGs by using variable substitutions in your submit files. If you look at the DAG we made, you might find it a bit tedious because each goatbrot job has a separate Condor submit file. They're nearly identical except for a couple of parameters. Can we make it simpler? Yes, we can!","title":"Objective"},{"location":"DataSanJose2019/CI/12-VariableSubstitution/#declare-your-variables","text":"First you need to declare your variables in your submit file. Make one submit file for all of your goatbrot jobs. Here's what it looks like. Call it goatbrot.sub : executable = /tmp/goatbrot-master/goatbrot arguments = -i 100000 -c $(CENTERX),$(CENTERY) -w 1.5 -s 500,500 -o tile_$(TILEY)_$(TILEX).ppm log = goatbrot.log output = goatbrot.out.$(TILEY).$(TILEX) error = goatbrot.err.$(TILEY).$(TILEX) should_transfer_files = YES when_to_transfer_output = ONEXIT requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue Then you need to change your DAG to use VARS for variable substitution. Here's what one of the jobs would look like: JOB g1 goatbrot.sub VARS g1 CENTERX=\"-0.75\" VARS g1 CENTERY=\"0.75\" VARS g1 TILEX=\"0\" VARS g1 TILEY=\"0\" Edit your DAG similarly for all of your goatbrot jobs. If you need help, check the Condor manual for for a description of how to use VARS . What happens?","title":"Declare your variables"},{"location":"DataSanJose2019/CI/13-DisciplineTutorials/","text":"Follow your Interest Exercises During this portion of the exercise you'll be able to find a tutorial that fits your interest and play with submission on OSG Connect. You can see the available tutorials by logging on to login.osgconnect.net and running the following command: $ tutorial list Some suggestions to get you started: The details of the tutorials will be in the README.md files with each tutorial. Bioinformatics Tutorials Molecule Docking $ tutorial AutoDockVina Genetic Sequence Analysis $ tutorial blast Statistical Tutorials Use R to calculate Pi $ tutorial R Matlab $ tutorial matlab-HelloWorld Molecular Dynamics Tutorials NAMD Simulations $ tutorial namd GROMACS $ tutorial gromacs High Energy Physics Tutorials Calculate ntuples with root $ tutorial root Programming Tutorials Python Virtual Environment $ tutorial python-virtualenv SWIFT Parallel Programming $ tutorial swift Advanced HTC Concepts Pegasus Workflows $ tutorial pegasus Scaling on the Open Science Grid Pegasus Workflows $ tutorial scaling Feel free to explore the OSG Connect Tutorials on your own.","title":"Follow your Interest Exercises"},{"location":"DataSanJose2019/CI/13-DisciplineTutorials/#follow-your-interest-exercises","text":"During this portion of the exercise you'll be able to find a tutorial that fits your interest and play with submission on OSG Connect. You can see the available tutorials by logging on to login.osgconnect.net and running the following command: $ tutorial list Some suggestions to get you started: The details of the tutorials will be in the README.md files with each tutorial.","title":"Follow your Interest Exercises"},{"location":"DataSanJose2019/CI/13-DisciplineTutorials/#bioinformatics-tutorials","text":"Molecule Docking $ tutorial AutoDockVina Genetic Sequence Analysis $ tutorial blast","title":"Bioinformatics Tutorials"},{"location":"DataSanJose2019/CI/13-DisciplineTutorials/#statistical-tutorials","text":"Use R to calculate Pi $ tutorial R Matlab $ tutorial matlab-HelloWorld","title":"Statistical Tutorials"},{"location":"DataSanJose2019/CI/13-DisciplineTutorials/#molecular-dynamics-tutorials","text":"NAMD Simulations $ tutorial namd GROMACS $ tutorial gromacs","title":"Molecular Dynamics Tutorials"},{"location":"DataSanJose2019/CI/13-DisciplineTutorials/#high-energy-physics-tutorials","text":"Calculate ntuples with root $ tutorial root","title":"High Energy Physics Tutorials"},{"location":"DataSanJose2019/CI/13-DisciplineTutorials/#programming-tutorials","text":"Python Virtual Environment $ tutorial python-virtualenv SWIFT Parallel Programming $ tutorial swift","title":"Programming Tutorials"},{"location":"DataSanJose2019/CI/13-DisciplineTutorials/#advanced-htc-concepts","text":"Pegasus Workflows $ tutorial pegasus Scaling on the Open Science Grid Pegasus Workflows $ tutorial scaling Feel free to explore the OSG Connect Tutorials on your own.","title":"Advanced HTC Concepts"},{"location":"DataSanJose2019/CI/14-Containers/","text":"Singularity Containers in OSG Objective Singularity is a container system to allow users full control over their enviroment. You can create your own container image which your job will execute within, or choose from a set of pre-defined images. For more information about Singularity, please see: Singularity Home Page The following talk describes Singularity for scientific computing: Singularity Talk Derek Weitzel wrote a blog post about Singularity on OSG, which provides a good introduction on how to create images and run them, but does not cover all the functionality described further down: Singularity on the OSG Default Image The default setup is to auto load an image on sites which support Singularity. Every job which lands on such a site, will have a container started just for that job, and then run within that container. Most users will not even know that their jobs are run within a container, but it will provide them with a consistent environment across OSG sites. The current default container is based on EL6 and contains a basic set of tools expected from OSG compute nodes. The image is loaded from /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el6 and the definition file is available in GitHub https://github.com/opensciencegrid/osgvo-el6 . If you want to steer a job to run on a default Singularity instance, use HAS_SINGULARITY == True in the job requirements. For example: universe = vanilla executable = job.sh requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") && HAS_SINGULARITY == TRUE should_transfer_files = IF_NEEDED when_to_transfer_output = ON_EXIT output = out error = err log = log queue To instruct the system to load a different image, use the +SingularityImage attribute in your job submit file. For example, to run your job under EL7: universe = vanilla executable = job.sh requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") && HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7\" +SingularityBindCVMFS = True should_transfer_files = IF_NEEDED when_to_transfer_output = ON_EXIT output = out error = err log = log queue The user support team maintains a set of images. These contain a basic set of tools and libraries. The images are are: Image Location Defintion Description EL 6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el6:latest GitHub A basic Enterprise Linux (CentOS) 6 based image. This is currently our default image EL 7 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest GitHub A basic Enterprise Linux (CentOS) 7 based image. Ubuntu Xenial /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-xenial:latest GitHub A good image if you prefer Ubuntu over EL flavors TensorFlow /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:latest GitHub Base on the TensorFlow base image, with a few OSG package added TensorFlow GPU /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:latest GitHub Used for running TensorFlow jobs on OSG GPU resources Exloring Images on the Submit Host Images can be explored interactively on the submit hosts by starting it in \"shell\" mode. The recommended command line, similar to how containers are started for jobs, is: singularity shell \\ --home $PWD:/srv \\ --pwd /srv \\ --bind /cvmfs \\ --scratch /var/tmp \\ --scratch /tmp \\ --contain --ipc --pid \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-xenial:latest Custom Images OSG Connect provides tooling for users to create, publish and load custom images. This is useful if your job requires some very specific software setup. Creating a Custom Image If you want to use an image you have created yourself, the image should be defined as a Docker image and published in the Docker Hub . The reason we use Docker as a source image repository is that it allows us to easily import the images into our own distribution system (see below). To get started, create a Docker user, sign in to the hub, and create a new repository. You will end up with an identifier of the namespace/repository_name format. Create an image locally using a Dockerfile and the docker build . We suggest you base the image on one of the provided OSG images. For example, if you want to base the image on our Ubuntu Xenial image, first download the Dockerfile from the GitHub repository . Edit the Dockerfile to fit your requirements. Then build the image with tag matching your Docker Hub repository: docker build -t namespace/repository_name . Once you have a successful build, push it to the hub: docker push namespace/repository_name Then register the image as described in the next section. If you prefer, you can base you image on images not already published by OSG, but if you do this, we recommend that you as one of the steps create the /cvmfs directory. This will enable the container to access tools and data published on /cvmfs. In your Dockerfile , add: # required directories RUN mkdir -p /cvmfs See one of the provided image defintions for a full example. If you do not want /cvmfs mounted in the container, please add +SingularityBindCVMFS = False to your job submit file. Distributing Custom Images Via CVMFS In order to be able to efficiently distribute the container images to a large of distributed compute hosts, OSG has choosen to host the images under CVMFS . Any image publically available in Docker can be included for automatic syncing into the CVMFS repository. The result is an unpacked image under /cvmfs/singularity.opensciencegrid.org/ To get your images included, please either create a git pull request against docker_images.txt in the cvmfs-singularity-sync repository , or contact user-support@opensciencegrid.org and we can help you. Once your image has been registered, new versions pushed to Docker Hub will automatically be detected and CVMFS will be updated accordingly. Source Paged sourced from https://support.opensciencegrid.org/support/solutions/articles/12000024676-singularity-containers.","title":"Singularity Containers in OSG"},{"location":"DataSanJose2019/CI/14-Containers/#singularity-containers-in-osg","text":"","title":"Singularity Containers in OSG"},{"location":"DataSanJose2019/CI/14-Containers/#objective","text":"Singularity is a container system to allow users full control over their enviroment. You can create your own container image which your job will execute within, or choose from a set of pre-defined images. For more information about Singularity, please see: Singularity Home Page The following talk describes Singularity for scientific computing: Singularity Talk Derek Weitzel wrote a blog post about Singularity on OSG, which provides a good introduction on how to create images and run them, but does not cover all the functionality described further down: Singularity on the OSG","title":"Objective"},{"location":"DataSanJose2019/CI/14-Containers/#default-image","text":"The default setup is to auto load an image on sites which support Singularity. Every job which lands on such a site, will have a container started just for that job, and then run within that container. Most users will not even know that their jobs are run within a container, but it will provide them with a consistent environment across OSG sites. The current default container is based on EL6 and contains a basic set of tools expected from OSG compute nodes. The image is loaded from /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el6 and the definition file is available in GitHub https://github.com/opensciencegrid/osgvo-el6 . If you want to steer a job to run on a default Singularity instance, use HAS_SINGULARITY == True in the job requirements. For example: universe = vanilla executable = job.sh requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") && HAS_SINGULARITY == TRUE should_transfer_files = IF_NEEDED when_to_transfer_output = ON_EXIT output = out error = err log = log queue To instruct the system to load a different image, use the +SingularityImage attribute in your job submit file. For example, to run your job under EL7: universe = vanilla executable = job.sh requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") && HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7\" +SingularityBindCVMFS = True should_transfer_files = IF_NEEDED when_to_transfer_output = ON_EXIT output = out error = err log = log queue The user support team maintains a set of images. These contain a basic set of tools and libraries. The images are are: Image Location Defintion Description EL 6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el6:latest GitHub A basic Enterprise Linux (CentOS) 6 based image. This is currently our default image EL 7 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest GitHub A basic Enterprise Linux (CentOS) 7 based image. Ubuntu Xenial /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-xenial:latest GitHub A good image if you prefer Ubuntu over EL flavors TensorFlow /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:latest GitHub Base on the TensorFlow base image, with a few OSG package added TensorFlow GPU /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:latest GitHub Used for running TensorFlow jobs on OSG GPU resources","title":"Default Image"},{"location":"DataSanJose2019/CI/14-Containers/#exloring-images-on-the-submit-host","text":"Images can be explored interactively on the submit hosts by starting it in \"shell\" mode. The recommended command line, similar to how containers are started for jobs, is: singularity shell \\ --home $PWD:/srv \\ --pwd /srv \\ --bind /cvmfs \\ --scratch /var/tmp \\ --scratch /tmp \\ --contain --ipc --pid \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-xenial:latest","title":"Exloring Images on the Submit Host"},{"location":"DataSanJose2019/CI/14-Containers/#custom-images","text":"OSG Connect provides tooling for users to create, publish and load custom images. This is useful if your job requires some very specific software setup.","title":"Custom Images"},{"location":"DataSanJose2019/CI/14-Containers/#creating-a-custom-image","text":"If you want to use an image you have created yourself, the image should be defined as a Docker image and published in the Docker Hub . The reason we use Docker as a source image repository is that it allows us to easily import the images into our own distribution system (see below). To get started, create a Docker user, sign in to the hub, and create a new repository. You will end up with an identifier of the namespace/repository_name format. Create an image locally using a Dockerfile and the docker build . We suggest you base the image on one of the provided OSG images. For example, if you want to base the image on our Ubuntu Xenial image, first download the Dockerfile from the GitHub repository . Edit the Dockerfile to fit your requirements. Then build the image with tag matching your Docker Hub repository: docker build -t namespace/repository_name . Once you have a successful build, push it to the hub: docker push namespace/repository_name Then register the image as described in the next section. If you prefer, you can base you image on images not already published by OSG, but if you do this, we recommend that you as one of the steps create the /cvmfs directory. This will enable the container to access tools and data published on /cvmfs. In your Dockerfile , add: # required directories RUN mkdir -p /cvmfs See one of the provided image defintions for a full example. If you do not want /cvmfs mounted in the container, please add +SingularityBindCVMFS = False to your job submit file.","title":"Creating a Custom Image"},{"location":"DataSanJose2019/CI/14-Containers/#distributing-custom-images-via-cvmfs","text":"In order to be able to efficiently distribute the container images to a large of distributed compute hosts, OSG has choosen to host the images under CVMFS . Any image publically available in Docker can be included for automatic syncing into the CVMFS repository. The result is an unpacked image under /cvmfs/singularity.opensciencegrid.org/ To get your images included, please either create a git pull request against docker_images.txt in the cvmfs-singularity-sync repository , or contact user-support@opensciencegrid.org and we can help you. Once your image has been registered, new versions pushed to Docker Hub will automatically be detected and CVMFS will be updated accordingly.","title":"Distributing Custom Images Via CVMFS"},{"location":"DataSanJose2019/CI/14-Containers/#source","text":"Paged sourced from https://support.opensciencegrid.org/support/solutions/articles/12000024676-singularity-containers.","title":"Source"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/","text":"Visualisation using Seaborn The notes listed here are based on this DataCamp tutorial on Seaborn by Karlijn Willems and this CODATA-RDA module on visualisation by Sara El Jadid . During this module we'll be making use of Seaborn , which provides a high-level interface to draw statistical graphics. Seaborn vs Matplotlib Seaborn is complimentary to Matplotlib and it specifically targets statistical data visualization. But it goes even further than that: Seaborn extends Matplotlib and that\u2019s why it can address the two biggest frustrations of working with Matplotlib. Or, as Michael Waskom says in the \u201c introduction to Seaborn \u201d: \u201cIf matplotlib \u201ctries to make easy things easy and hard things possible\u201d, seaborn tries to make a well-defined set of hard things easy too.\u201d One of these hard things or frustrations had to do with the default Matplotlib parameters. Seaborn works with different parameters, which undoubtedly speaks to those users that don\u2019t use the default looks of the Matplotlib plots. During this module we'll also be making some use of Pandas to extract features of the data that we need. Getting started In the first instance please get yourself set up with a notebook on the Google colab site. Please go to https://colab.research.google.com/notebooks/welcome.ipynb and then click on File and New Python 3 notebook. OR log into the Kabre jupyter server and then click on New and then New Python 3. We'll start with importing a set of libraries that will be useful for us and the gapminder data set. import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set(style=\"darkgrid\") The last line is a choice about how things look - you may want to leave that out. Now we'll read in the data. We will again use the gapminder data set but with the columns labelled differently. Please do not use the version you have used previously. The version is stored on a github repository which has been shortened using bit.ly. url=\"http://bit.ly/2PbVBcR\" #url=\"https://raw.githubusercontent.com/CODATA-RDA-DataScienceSchools/Materials/master/docs/DataSanJose2019/slides/Visualisation/gapminder.csv\" gapminder=pd.read_csv(url) So we are using pandas to read in the data set. The gapminder data set is a Data Frame . Exploring the gapminder data set The gapminder data set is a set of socioeconomic data about populations, GDP per capita and expected life span for a large number of countries over a number of years. We can have a look at it using the head command. gapminder.head() You should get something like this. Unnamed: 0 country continent year lifeExp pop gdpPercap 0 1 Afghanistan Asia 1952 28.801 8425333 779.445314 1 2 Afghanistan Asia 1957 30.332 9240934 820.853030 2 3 Afghanistan Asia 1962 31.997 10267083 853.100710 3 4 Afghanistan Asia 1967 34.020 11537966 836.197138 4 5 Afghanistan Asia 1972 36.088 13079460 739.981106 So it is a combination of categorical data (countries and continents) and quantitative data (year, lifeExp etc.). It's also nice (though unrealistic) that it doesn't have missing values or malformed data e.g. Ireland is written sometimes as \"Ireland\" and sometimes \"ireland\" and sometimes \"Republic of Ireland\" or even \"Eire\"!! Dealing with those kinds of issues is not what we're going to focus on here. We can do a statistical summary of the numerical data using the describe function gapminder.describe() Unnamed: 0 year lifeExp pop gdpPercap count 1704.000000 1704.00000 1704.000000 1.704000e+03 1704.000000 mean 852.500000 1979.50000 59.474439 2.960121e+07 7215.327081 std 492.046746 17.26533 12.917107 1.061579e+08 9857.454543 min 1.000000 1952.00000 23.599000 6.001100e+04 241.165877 25% 426.750000 1965.75000 48.198000 2.793664e+06 1202.060309 50% 852.500000 1979.50000 60.712500 7.023596e+06 3531.846989 75% 1278.250000 1993.25000 70.845500 1.958522e+07 9325.462346 max 1704.000000 2007.00000 82.603000 1.318683e+09 113523.132900 One can find the names of the continents by executing the following. list(set(gapminder.continent)) We note that gapminder.continent gives us the list with the column corresponding to the continent entry. The set command converts the list into a set (which only has unique entries) and then list turns that back into a list again. We can list these entries alphabetically from the command sorted(list(set(gapminder.continent))) Exercise What does the function sorted do? Do the same for the countries. The gsapminder data set also presents lots of questions such as Is there a relationship between gdpPercap (roughly a measure of the average wealth of each person in a country) and their average life span? Is the average life span changing over time? How does picture change over different countries or comntinents? Visualisation allows us to explore all of this! Getting started with seaborn Let us start with doing box plots which count the number of entries that we have for each continent. sns.countplot(x=\"continent\", data=gapminder) You should get the folllowing. Note that generically seaborn generally looks like this. sns. (x= , y= , ... , data =< a data frame>) We use countplot here to just count entries and plot them as a box plot. Exercise What happens if we do the following? sns.countplot(x=\"Continent\", data=gapminder) What does that tell us? Printing out You can save a figure as a PNG or as a PDF then in the same cell as the command you run to plot use the savefig command. sns.countplot(x=\"continent\", data=gapminder) plt.savefig(\"Histogram.png\") plt.savefig(\"Histogram.pdf\") Looking at 1-d distributions We can use the command catplot to just look at the distribution of life expenctancies. sns.catplot(y=\"lifeExp\", data=gapminder) You should get something like this. The points are jittered i.e. randomly moved in the horizontal axis to make things clearer. We can switch that off if we wish. sns.catplot(y=\"lifeExp\", data=gapminder,jitter=False) This scatterplot is not very informative! We can create a box plot of the data sns.boxplot(y=\"lifeExp\", data=gapminder) This should give the following. Exercise One can use another type of plot called a violin plot which tries to summarise the distribution better than a boxplot. The width of the violin plot represents how big the distribution is at that value. It is quite useful for picking out multi-modal (one with a distribution that has more than one peak). The command in seaborn is violinplot. Try and implement this for this data. Diving deeper into the data Just looking at the life expectancy for all of the countries isn't very informative. The first thing we can do is ask how does this vary across continents. Seaborn does this easily by introducing an x-axis which is the continent. Again, let's try with just the points. sns.catplot(x=\"continent\", y=\"lifeExp\", data=gapminder) Exercise Repeat this using box plots (and violin plots if you wish). Repeat the above steps using GDP per capita (gdpPercap) instead of life expectancy. You can also try the swarmplot function as another way to represent this data. Can we just draw a distribution or a histogram? What about dividing it as continents? Yes! But we'll get to that in a bit. Ordering Plotting the box plots with the continents in alphabetical order is quite easy. orderedContinents = sorted(list(set(gapminder.continent))) orderedContinents orderedContinents is a list with the continents ordered. sns.boxplot(x=\"continent\", y=\"lifeExp\", order=orderedContinents, data=gapminder) On the other hand we may want to order the box plots in ascending order of the medians of the life expectancy. This is more involved but is a good exercise in manipulating the data. #Create an empty dictionary medianLifeExps = {} # Loop through all the continents for val in gapminder.continent: # Create a new key which is the median life expectancy of that continent # gapminder[gapminder.continent == val] pulls out the continent in that loop # the .lifeExp.median() part then computes the median # of the remaining life expectancy data key = gapminder[gapminder.continent==val].lifeExp.median() # create a new entry in the dictionary with the continent as the value and the key # as the median. medianLifeExps[key] = val # Create a sorted list of the medians (in ascending order) sortedKeys = sorted(medianLifeExps) # Finally return the list of continents in that order orderedMedianContinents = [] for m in orderedMedians: orderedMedianContinents.append(medianLifeExps[m]) sns.boxplot(x=\"continent\", y=\"lifeExp\", order=orderedMedianContinents, data=gapminder) Exercise Do the same plot for GDP per capita. Line and scatter plots Instead of having a categorical variable on the horizontal access we now do scatter and line plots. Let's start with the whole data set. sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data=gapminder) This is hard to grasp as a whole, so we'll just consider one country - China. We can select data corresponding to China by the following. gapminder[gapminder.country==\"China\"] sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) Exercise Do the same for your country - if it isn't listed in gapminder then pick another. A line plot works in the same way. It's possible to overlay these in the same cell. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) Expressing more variables with different attributes It is hard to make out the points from the lines. We can change the colour (color) of the points accordingly. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",color=\"red\",data= gapminder[gapminder.country==\"China\"]) We can use the colour of the points to represent a different column - e.g. the year. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\", data= gapminder[gapminder.country==\"China\"]) The problem here is that only a certain number of years have been picked out. We need to tell seaborn how many years there are and how to set a palette of colours for this (the default palette has six colours). # Find the number of years (why only set?) n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) # We use a rainbow-like palette but there are others. sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"]) We can use the size of the point to represent an additional column - the population. n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",size=\"pop\", palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"]) The problem now is that there is too much detail in the legend - so we'll switch that off. n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",size=\"pop\", palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"], legend=False) How useful is adding these attributes? Exercise Pick another country and try this out. Costa Rica is interesting. Does anybody have an explanation? Summarising We will now examine how life expenctancy has varied over time. sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"country\",data= gapminder,legend=False) There are so many countries here I haven't even tried to construct a palette! Looking at this many countries are generally increasing but some are not following that trend. We could explore those outliers but here we will focus on trying to summarise what is going on (is a particular continent not going with the trend of increasing life span over time?) To do this we need to use another pandas command groupby which creates a new data frame for particular variables. Once we have created that new data frame we can then plot the data. # First create a new data frame which has the medians, by year and continent, of the life expectancy medianGapminder = gapminder.groupby(['continent','year']).lifeExp.median() # Now plot it sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"continent\",data= medianGapminder) What happened? The groupby command makes continent and year indices of the data (you can see this if you print out the data frame). Having created the data frame we need to reset the indices. # First create a new data frame which has the medians, by year and continent, of the life expectancy medianGapminder = gapminder.groupby(['continent','year']).lifeExp.median() # Now plot it sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"continent\",data= medianGapminder.reset_index()) Exercise Repeat this using the GDP per capita. Use a different statistical summary apart from the median. Regression The line plots just \"join the dots\". It is more intesting to try and fit the data to a curve. We also want to do the fit and distinguish between the different continents. We can do this using the lmplot command. sns.lmplot(x=\"year\",y=\"lifeExp\",hue=\"continent\", data= medianGapminder.reset_index()) This does a simple linear regression. We can do more sophisticated types of fit, for example Loess (or Lowess). sns.lmplot(x=\"year\",y=\"lifeExp\",hue=\"continent\", lowess=True, data= medianGapminder.reset_index()) Exercise Repeat the above using the GDP per capita. More regression Now that we know how to fit curves through data with a number of different variables we can go back to the case of where we plotted the life expectancy against the GDP per capita. Instead of just doing a scatter plot we can now do regression (curve fitting) as function of the continent as well so we can see how the life expectancy varies between GDP per capita and the continent. sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True,data=gapminder) The problem here is that points are too large - we cannot see the trend. When you have many points make the point size smaller, indeed way smaller (someone described it as 'dust size') to see the trend better. Since Seaborn is based on Matplotlib we need to use a slightly different notation in the arguments to what was used previously. # scatter_kws is passed onto the underlying matplotlib plotting routine. sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True,data=gapminder,scatter_kws={\"s\":5}) The GDP per capita varies over a wide range and it would be good in the first instance to plot the x-axis on a logarithmic scale. Again since Seaborn is based on Matplotlib we need to use a slightly different notation to what was used previously. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xscale=\"log\") We now get a much better spread of the data but it's still hard to see how the different continents are behaving. To do that we make use of facet plots . These are simply plots of different but related variables that are organised on the same screen for easy comparison. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\") We note that the axes of all of these plots are the same so we can do a valid comparison. Still these plots are quite squashed as thy try and fit to the width of the page. Instead we can wrap them around. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", col_wrap=2, lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\") Finally, we can adjust the colour of the individual plots. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", hue=\"continent\", col_wrap=2, lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\") Exercise Do the same type of plots for life expectancy against year. Distributions We can plot the distribution of a list of data (one column from a data frame) using a kernal density approach and/or a histogram. sns.distplot(gapminder.lifeExp) distplot only allows a single column from a data frame! You can also add the raw data into this plot as well (although this isn't very useful in this case as there's so much data). sns.distplot(gapminder.lifeExp,rug=True) Exercise Do the same type of plot with the GDP per capita data. Again we would like to break this down in separate continents. Again we will make use of facet plots. lmplot is designed to create facet plots but distplot isn't so we need to use a specific function called FacetGrid to do this. In the call below we also adjust the height and aspect (the height/width ratio) of the figures. # Create a facet of the gapminder data based on the continents ordered alphabetically (orderedContinents) g = sns.FacetGrid(gapminder, row=\"continent\", row_order=orderedContinents, height=2, aspect=4) # Plot on the facets the distribution of the life expctancy data, but don't plot the histogram. g.map(sns.distplot, \"lifeExp\", hist=False) Finally the facet plot can also be used with just one facet! # Create a facet plot with one facet but colour on continent. g = sns.FacetGrid(gapminder, hue=\"continent\",height=2, aspect=4) # Plot the distributions with no histogram g.map(sns.distplot, \"lifeExp\", hist=False) # Give the colour scheme in a legend g.add_legend() Exercise Do the same type of plot with the GDP per capita data (Longer) Select the data from the gapminder data set for a particular continent and now create facet plots for those countries.","title":"Visualisation using Seaborn"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#visualisation-using-seaborn","text":"The notes listed here are based on this DataCamp tutorial on Seaborn by Karlijn Willems and this CODATA-RDA module on visualisation by Sara El Jadid . During this module we'll be making use of Seaborn , which provides a high-level interface to draw statistical graphics.","title":"Visualisation using Seaborn"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#seaborn-vs-matplotlib","text":"Seaborn is complimentary to Matplotlib and it specifically targets statistical data visualization. But it goes even further than that: Seaborn extends Matplotlib and that\u2019s why it can address the two biggest frustrations of working with Matplotlib. Or, as Michael Waskom says in the \u201c introduction to Seaborn \u201d: \u201cIf matplotlib \u201ctries to make easy things easy and hard things possible\u201d, seaborn tries to make a well-defined set of hard things easy too.\u201d One of these hard things or frustrations had to do with the default Matplotlib parameters. Seaborn works with different parameters, which undoubtedly speaks to those users that don\u2019t use the default looks of the Matplotlib plots. During this module we'll also be making some use of Pandas to extract features of the data that we need.","title":"Seaborn vs Matplotlib"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#getting-started","text":"In the first instance please get yourself set up with a notebook on the Google colab site. Please go to https://colab.research.google.com/notebooks/welcome.ipynb and then click on File and New Python 3 notebook.","title":"Getting started"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#or","text":"log into the Kabre jupyter server and then click on New and then New Python 3. We'll start with importing a set of libraries that will be useful for us and the gapminder data set. import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set(style=\"darkgrid\") The last line is a choice about how things look - you may want to leave that out. Now we'll read in the data. We will again use the gapminder data set but with the columns labelled differently. Please do not use the version you have used previously. The version is stored on a github repository which has been shortened using bit.ly. url=\"http://bit.ly/2PbVBcR\" #url=\"https://raw.githubusercontent.com/CODATA-RDA-DataScienceSchools/Materials/master/docs/DataSanJose2019/slides/Visualisation/gapminder.csv\" gapminder=pd.read_csv(url) So we are using pandas to read in the data set. The gapminder data set is a Data Frame .","title":"OR"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exploring-the-gapminder-data-set","text":"The gapminder data set is a set of socioeconomic data about populations, GDP per capita and expected life span for a large number of countries over a number of years. We can have a look at it using the head command. gapminder.head() You should get something like this. Unnamed: 0 country continent year lifeExp pop gdpPercap 0 1 Afghanistan Asia 1952 28.801 8425333 779.445314 1 2 Afghanistan Asia 1957 30.332 9240934 820.853030 2 3 Afghanistan Asia 1962 31.997 10267083 853.100710 3 4 Afghanistan Asia 1967 34.020 11537966 836.197138 4 5 Afghanistan Asia 1972 36.088 13079460 739.981106 So it is a combination of categorical data (countries and continents) and quantitative data (year, lifeExp etc.). It's also nice (though unrealistic) that it doesn't have missing values or malformed data e.g. Ireland is written sometimes as \"Ireland\" and sometimes \"ireland\" and sometimes \"Republic of Ireland\" or even \"Eire\"!! Dealing with those kinds of issues is not what we're going to focus on here. We can do a statistical summary of the numerical data using the describe function gapminder.describe() Unnamed: 0 year lifeExp pop gdpPercap count 1704.000000 1704.00000 1704.000000 1.704000e+03 1704.000000 mean 852.500000 1979.50000 59.474439 2.960121e+07 7215.327081 std 492.046746 17.26533 12.917107 1.061579e+08 9857.454543 min 1.000000 1952.00000 23.599000 6.001100e+04 241.165877 25% 426.750000 1965.75000 48.198000 2.793664e+06 1202.060309 50% 852.500000 1979.50000 60.712500 7.023596e+06 3531.846989 75% 1278.250000 1993.25000 70.845500 1.958522e+07 9325.462346 max 1704.000000 2007.00000 82.603000 1.318683e+09 113523.132900 One can find the names of the continents by executing the following. list(set(gapminder.continent)) We note that gapminder.continent gives us the list with the column corresponding to the continent entry. The set command converts the list into a set (which only has unique entries) and then list turns that back into a list again. We can list these entries alphabetically from the command sorted(list(set(gapminder.continent)))","title":"Exploring the gapminder data set"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise","text":"What does the function sorted do? Do the same for the countries. The gsapminder data set also presents lots of questions such as Is there a relationship between gdpPercap (roughly a measure of the average wealth of each person in a country) and their average life span? Is the average life span changing over time? How does picture change over different countries or comntinents? Visualisation allows us to explore all of this!","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#getting-started-with-seaborn","text":"Let us start with doing box plots which count the number of entries that we have for each continent. sns.countplot(x=\"continent\", data=gapminder) You should get the folllowing. Note that generically seaborn generally looks like this. sns. (x= , y= , ... , data =< a data frame>) We use countplot here to just count entries and plot them as a box plot.","title":"Getting started with seaborn"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_1","text":"What happens if we do the following? sns.countplot(x=\"Continent\", data=gapminder) What does that tell us?","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#printing-out","text":"You can save a figure as a PNG or as a PDF then in the same cell as the command you run to plot use the savefig command. sns.countplot(x=\"continent\", data=gapminder) plt.savefig(\"Histogram.png\") plt.savefig(\"Histogram.pdf\")","title":"Printing out"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#looking-at-1-d-distributions","text":"We can use the command catplot to just look at the distribution of life expenctancies. sns.catplot(y=\"lifeExp\", data=gapminder) You should get something like this. The points are jittered i.e. randomly moved in the horizontal axis to make things clearer. We can switch that off if we wish. sns.catplot(y=\"lifeExp\", data=gapminder,jitter=False) This scatterplot is not very informative! We can create a box plot of the data sns.boxplot(y=\"lifeExp\", data=gapminder) This should give the following.","title":"Looking at 1-d distributions"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_2","text":"One can use another type of plot called a violin plot which tries to summarise the distribution better than a boxplot. The width of the violin plot represents how big the distribution is at that value. It is quite useful for picking out multi-modal (one with a distribution that has more than one peak). The command in seaborn is violinplot. Try and implement this for this data.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#diving-deeper-into-the-data","text":"Just looking at the life expectancy for all of the countries isn't very informative. The first thing we can do is ask how does this vary across continents. Seaborn does this easily by introducing an x-axis which is the continent. Again, let's try with just the points. sns.catplot(x=\"continent\", y=\"lifeExp\", data=gapminder)","title":"Diving deeper into the data"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_3","text":"Repeat this using box plots (and violin plots if you wish). Repeat the above steps using GDP per capita (gdpPercap) instead of life expectancy. You can also try the swarmplot function as another way to represent this data. Can we just draw a distribution or a histogram? What about dividing it as continents? Yes! But we'll get to that in a bit.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#ordering","text":"Plotting the box plots with the continents in alphabetical order is quite easy. orderedContinents = sorted(list(set(gapminder.continent))) orderedContinents orderedContinents is a list with the continents ordered. sns.boxplot(x=\"continent\", y=\"lifeExp\", order=orderedContinents, data=gapminder) On the other hand we may want to order the box plots in ascending order of the medians of the life expectancy. This is more involved but is a good exercise in manipulating the data. #Create an empty dictionary medianLifeExps = {} # Loop through all the continents for val in gapminder.continent: # Create a new key which is the median life expectancy of that continent # gapminder[gapminder.continent == val] pulls out the continent in that loop # the .lifeExp.median() part then computes the median # of the remaining life expectancy data key = gapminder[gapminder.continent==val].lifeExp.median() # create a new entry in the dictionary with the continent as the value and the key # as the median. medianLifeExps[key] = val # Create a sorted list of the medians (in ascending order) sortedKeys = sorted(medianLifeExps) # Finally return the list of continents in that order orderedMedianContinents = [] for m in orderedMedians: orderedMedianContinents.append(medianLifeExps[m]) sns.boxplot(x=\"continent\", y=\"lifeExp\", order=orderedMedianContinents, data=gapminder)","title":"Ordering"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_4","text":"Do the same plot for GDP per capita.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#line-and-scatter-plots","text":"Instead of having a categorical variable on the horizontal access we now do scatter and line plots. Let's start with the whole data set. sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data=gapminder) This is hard to grasp as a whole, so we'll just consider one country - China. We can select data corresponding to China by the following. gapminder[gapminder.country==\"China\"] sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"])","title":"Line and scatter plots"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_5","text":"Do the same for your country - if it isn't listed in gapminder then pick another. A line plot works in the same way. It's possible to overlay these in the same cell. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"])","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#expressing-more-variables-with-different-attributes","text":"It is hard to make out the points from the lines. We can change the colour (color) of the points accordingly. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",color=\"red\",data= gapminder[gapminder.country==\"China\"]) We can use the colour of the points to represent a different column - e.g. the year. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\", data= gapminder[gapminder.country==\"China\"]) The problem here is that only a certain number of years have been picked out. We need to tell seaborn how many years there are and how to set a palette of colours for this (the default palette has six colours). # Find the number of years (why only set?) n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) # We use a rainbow-like palette but there are others. sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"]) We can use the size of the point to represent an additional column - the population. n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",size=\"pop\", palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"]) The problem now is that there is too much detail in the legend - so we'll switch that off. n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",size=\"pop\", palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"], legend=False) How useful is adding these attributes?","title":"Expressing more variables with different attributes"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_6","text":"Pick another country and try this out. Costa Rica is interesting. Does anybody have an explanation?","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#summarising","text":"We will now examine how life expenctancy has varied over time. sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"country\",data= gapminder,legend=False) There are so many countries here I haven't even tried to construct a palette! Looking at this many countries are generally increasing but some are not following that trend. We could explore those outliers but here we will focus on trying to summarise what is going on (is a particular continent not going with the trend of increasing life span over time?) To do this we need to use another pandas command groupby which creates a new data frame for particular variables. Once we have created that new data frame we can then plot the data. # First create a new data frame which has the medians, by year and continent, of the life expectancy medianGapminder = gapminder.groupby(['continent','year']).lifeExp.median() # Now plot it sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"continent\",data= medianGapminder) What happened? The groupby command makes continent and year indices of the data (you can see this if you print out the data frame). Having created the data frame we need to reset the indices. # First create a new data frame which has the medians, by year and continent, of the life expectancy medianGapminder = gapminder.groupby(['continent','year']).lifeExp.median() # Now plot it sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"continent\",data= medianGapminder.reset_index())","title":"Summarising"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_7","text":"Repeat this using the GDP per capita. Use a different statistical summary apart from the median.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#regression","text":"The line plots just \"join the dots\". It is more intesting to try and fit the data to a curve. We also want to do the fit and distinguish between the different continents. We can do this using the lmplot command. sns.lmplot(x=\"year\",y=\"lifeExp\",hue=\"continent\", data= medianGapminder.reset_index()) This does a simple linear regression. We can do more sophisticated types of fit, for example Loess (or Lowess). sns.lmplot(x=\"year\",y=\"lifeExp\",hue=\"continent\", lowess=True, data= medianGapminder.reset_index())","title":"Regression"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_8","text":"Repeat the above using the GDP per capita.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#more-regression","text":"Now that we know how to fit curves through data with a number of different variables we can go back to the case of where we plotted the life expectancy against the GDP per capita. Instead of just doing a scatter plot we can now do regression (curve fitting) as function of the continent as well so we can see how the life expectancy varies between GDP per capita and the continent. sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True,data=gapminder) The problem here is that points are too large - we cannot see the trend. When you have many points make the point size smaller, indeed way smaller (someone described it as 'dust size') to see the trend better. Since Seaborn is based on Matplotlib we need to use a slightly different notation in the arguments to what was used previously. # scatter_kws is passed onto the underlying matplotlib plotting routine. sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True,data=gapminder,scatter_kws={\"s\":5}) The GDP per capita varies over a wide range and it would be good in the first instance to plot the x-axis on a logarithmic scale. Again since Seaborn is based on Matplotlib we need to use a slightly different notation to what was used previously. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xscale=\"log\") We now get a much better spread of the data but it's still hard to see how the different continents are behaving. To do that we make use of facet plots . These are simply plots of different but related variables that are organised on the same screen for easy comparison. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\") We note that the axes of all of these plots are the same so we can do a valid comparison. Still these plots are quite squashed as thy try and fit to the width of the page. Instead we can wrap them around. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", col_wrap=2, lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\") Finally, we can adjust the colour of the individual plots. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", hue=\"continent\", col_wrap=2, lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\")","title":"More regression"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_9","text":"Do the same type of plots for life expectancy against year.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#distributions","text":"We can plot the distribution of a list of data (one column from a data frame) using a kernal density approach and/or a histogram. sns.distplot(gapminder.lifeExp) distplot only allows a single column from a data frame! You can also add the raw data into this plot as well (although this isn't very useful in this case as there's so much data). sns.distplot(gapminder.lifeExp,rug=True)","title":"Distributions"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_10","text":"Do the same type of plot with the GDP per capita data. Again we would like to break this down in separate continents. Again we will make use of facet plots. lmplot is designed to create facet plots but distplot isn't so we need to use a specific function called FacetGrid to do this. In the call below we also adjust the height and aspect (the height/width ratio) of the figures. # Create a facet of the gapminder data based on the continents ordered alphabetically (orderedContinents) g = sns.FacetGrid(gapminder, row=\"continent\", row_order=orderedContinents, height=2, aspect=4) # Plot on the facets the distribution of the life expctancy data, but don't plot the histogram. g.map(sns.distplot, \"lifeExp\", hist=False) Finally the facet plot can also be used with just one facet! # Create a facet plot with one facet but colour on continent. g = sns.FacetGrid(gapminder, hue=\"continent\",height=2, aspect=4) # Plot the distributions with no histogram g.map(sns.distplot, \"lifeExp\", hist=False) # Give the colour scheme in a legend g.add_legend()","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_11","text":"Do the same type of plot with the GDP per capita data (Longer) Select the data from the gapminder data set for a particular continent and now create facet plots for those countries.","title":"Exercise"},{"location":"DataSaoPaulo2018/","text":"Sao Paulo School of Research Data Science December 3 \u2013 14, 2018 ICTP-SAIFR, Sao Paulo, Brazil Background and purpose The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, ICTP and SAIFR, was conducted at the ICTP-SAIFR, Sao Paulo to introduce participants to the skills of RDS. Materials for the 2018 School of Research Data Science in Sao Paulo Day 1 - Introduction, Open Science , UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R , Reports Knitr Day 4 - Data Visualization Day 5 - Research Data Management , Data Management Plans Day 6 - Open Science , Information Security Day 7 - Overview of Machine Learning Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Lecture 4 Day 10 - Computational Infrastructures Wrap-Up - Lecture 5 , Lecture 6 School Close Out","title":"Sao Paulo School of Research Data Science"},{"location":"DataSaoPaulo2018/#sao-paulo-school-of-research-data-science","text":"December 3 \u2013 14, 2018 ICTP-SAIFR, Sao Paulo, Brazil","title":"Sao Paulo School of Research Data Science"},{"location":"DataSaoPaulo2018/#background-and-purpose","text":"The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, ICTP and SAIFR, was conducted at the ICTP-SAIFR, Sao Paulo to introduce participants to the skills of RDS.","title":"Background and purpose"},{"location":"DataSaoPaulo2018/#materials-for-the-2018-school-of-research-data-science-in-sao-paulo","text":"Day 1 - Introduction, Open Science , UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R , Reports Knitr Day 4 - Data Visualization Day 5 - Research Data Management , Data Management Plans Day 6 - Open Science , Information Security Day 7 - Overview of Machine Learning Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Lecture 4 Day 10 - Computational Infrastructures Wrap-Up - Lecture 5 , Lecture 6 School Close Out","title":"Materials for the 2018 School of Research Data Science in Sao Paulo"},{"location":"DataSaoPaulo2018/word_clouds/Readme/","text":"Code and instructions to create the word clouds can be found in here","title":"Readme"},{"location":"DataTrieste2019/","text":"Trieste School of Research Data Science 5-16 August, 2019 ICTP, Trieste, Italy Background and purpose The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA and ICTP, was conducted at the ICTP, Trieste to introduce participants to the skills of RDS. Materials for the 2019 School of Research Data Science in Trieste Day 1 - Introduction Open Science UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R Day 4 - Data Visualization Day 5 - Research Data Management , Data Management Plans Day 6 - Open Science , Information Security Day 7 - Overview of Machine Learning Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Lecture 4 Day 10 - Computational Infrastructures Wrap-Up - Lecture 5 , Lecture 6 School Close Out","title":"Trieste School of Research Data Science"},{"location":"DataTrieste2019/#trieste-school-of-research-data-science","text":"5-16 August, 2019 ICTP, Trieste, Italy","title":"Trieste School of Research Data Science"},{"location":"DataTrieste2019/#background-and-purpose","text":"The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA and ICTP, was conducted at the ICTP, Trieste to introduce participants to the skills of RDS.","title":"Background and purpose"},{"location":"DataTrieste2019/#materials-for-the-2019-school-of-research-data-science-in-trieste","text":"Day 1 - Introduction Open Science UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R Day 4 - Data Visualization Day 5 - Research Data Management , Data Management Plans Day 6 - Open Science , Information Security Day 7 - Overview of Machine Learning Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Lecture 4 Day 10 - Computational Infrastructures Wrap-Up - Lecture 5 , Lecture 6 School Close Out","title":"Materials for the 2019 School of Research Data Science in Trieste"}]}