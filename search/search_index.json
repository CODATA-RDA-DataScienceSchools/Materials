{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CODATA/RDA Schools for Research Data Science Materials from DataKigali 2018 Materials from DataSaoPaulo 2018 Materials from DataTrieste 2019 Materials from DataSanJose 2019","title":"Home"},{"location":"#codatarda-schools-for-research-data-science","text":"Materials from DataKigali 2018 Materials from DataSaoPaulo 2018 Materials from DataTrieste 2019 Materials from DataSanJose 2019","title":"CODATA/RDA Schools for Research Data Science"},{"location":"DataKigali2018/","text":"Kigali Foundational School in Research Data Science 22 October \u2013 2 November, 2018 University of Rwanda, Kigali, Rwanda Background and purpose The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, ICTP and EAIFR, was conducted at the University of Rwanda, Kigali to introduce participants to the skills of RDS. Materials for the 2018 School of Research Data Science in Kigali Day 1 - Introduction, Open Science , UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R Day 4 - Research Data Management Day 5 - Research Data Management , Open Science Day 6 - Data Visualization Day 7 - Overview of Machine Learning - 20of 20Machine 20Learning 202018.pdf\">Fundamentals , 20Systems 202018.pdf\">Recommender Systems Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Day 10 - Computational Infrastructures Wrap-Up - Lecture 4 , School Close Out","title":"Kigali Foundational School in Research Data Science"},{"location":"DataKigali2018/#kigali-foundational-school-in-research-data-science","text":"22 October \u2013 2 November, 2018 University of Rwanda, Kigali, Rwanda","title":"Kigali Foundational School in Research Data Science"},{"location":"DataKigali2018/#background-and-purpose","text":"The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, ICTP and EAIFR, was conducted at the University of Rwanda, Kigali to introduce participants to the skills of RDS.","title":"Background and purpose"},{"location":"DataKigali2018/#materials-for-the-2018-school-of-research-data-science-in-kigali","text":"Day 1 - Introduction, Open Science , UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R Day 4 - Research Data Management Day 5 - Research Data Management , Open Science Day 6 - Data Visualization Day 7 - Overview of Machine Learning - 20of 20Machine 20Learning 202018.pdf\">Fundamentals , 20Systems 202018.pdf\">Recommender Systems Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Day 10 - Computational Infrastructures Wrap-Up - Lecture 4 , School Close Out","title":"Materials for the 2018 School of Research Data Science in Kigali"},{"location":"DataKigali2018/slides/readme/","text":"Slides folder","title":"Readme"},{"location":"DataKigali2018/slides/DataVizMaterials/readme/","text":"download the folder for data visualization materials","title":"Readme"},{"location":"DataSanJose2019/","text":"San Jos\u00e9 School of Research Data Science December 2 \u2013 13, 2018 CeNAT, San Jos\u00e9, Costa Rica Background and purpose The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, CONARE and CeNAT, was conducted at the CeNAT, San Jos\u00e9 to introduce participants to the skills of RDS. Materials for the 2019 School of Research Data Science in San Jos\u00e9 Day 1 - C3 A919Monday.pdf\">Introduction , Open Science , 202_The 20Road 20to 20Linux%20(a 20little 20history)%202019.12.02.pdf\">Intro Unix , UNIX Shell Day 2 - Version Control with Git , 204_ 20Introduction_to_Python_Session_1\">Introduction to Python Day 3 - Introduction to Python , 204 20_ 20Introduction_to_Python_ 20Final_practice.md\">Practice Python Day 4 - Author Carpentry , Intro to Research Data Management , DMP example 1 , DMP example 2 Day 5 - Open Science , Data Management Plans* Day 6 - 20using 20Seaborn.md\">Data Visualization Day 7 - Overview of Machine Learning* Day 8 - Artificial Neural Networks Day 9 - Security and Computational Infrastructures - All material Day 10 - School Close Out","title":"San Jos\u00e9 School of Research Data Science"},{"location":"DataSanJose2019/#san-jose-school-of-research-data-science","text":"December 2 \u2013 13, 2018 CeNAT, San Jos\u00e9, Costa Rica","title":"San Jos\u00e9 School of Research Data Science"},{"location":"DataSanJose2019/#background-and-purpose","text":"The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, CONARE and CeNAT, was conducted at the CeNAT, San Jos\u00e9 to introduce participants to the skills of RDS.","title":"Background and purpose"},{"location":"DataSanJose2019/#materials-for-the-2019-school-of-research-data-science-in-san-jose","text":"Day 1 - C3 A919Monday.pdf\">Introduction , Open Science , 202_The 20Road 20to 20Linux%20(a 20little 20history)%202019.12.02.pdf\">Intro Unix , UNIX Shell Day 2 - Version Control with Git , 204_ 20Introduction_to_Python_Session_1\">Introduction to Python Day 3 - Introduction to Python , 204 20_ 20Introduction_to_Python_ 20Final_practice.md\">Practice Python Day 4 - Author Carpentry , Intro to Research Data Management , DMP example 1 , DMP example 2 Day 5 - Open Science , Data Management Plans* Day 6 - 20using 20Seaborn.md\">Data Visualization Day 7 - Overview of Machine Learning* Day 8 - Artificial Neural Networks Day 9 - Security and Computational Infrastructures - All material Day 10 - School Close Out","title":"Materials for the 2019 School of Research Data Science in San Jos\u00e9"},{"location":"DataSanJose2019/CI/00-Pre-Introduction-Login/","text":"Install an SSH-Cleint Download and install a SSH Client. We recommend PuTTY, but any ssh client is acceptable. https://www.putty.org/ Direct link to executable After opening the executable you should see the following screen: Authentication for CI Exercises You will receive login credentials at the beginning of this session. To authenticate (prove you who you say you are and establish what you are authorized to do) the bridgekeeper (login nodes) requires three bits of information: From Monty Python and the Holy Grail: BRIDGEKEEPER: Hee hee heh. Stop! What... is your name?\\ ARTHUR: It is 'Arthur', King of the Britons.\\ BRIDGEKEEPER: What... is your quest?\\ ARTHUR: To seek the Holy Grail.\\ BRIDGEKEEPER: What... is the air-speed velocity of an unladen swallow?\\ ARTHUR: What do you mean? An African or European swallow?\\ BRIDGEKEEPER: Huh? I-- I don't know that. Auuuuuuuugh!\\ BEDEVERE: How do know so much about swallows?\\ ARTHUR: Well, you have to know these things when you're a king, you know. or... 1) Tell me who you are. 2) Tell me something only you know. 3) Show me someething only you have. Why do you need both a password and a key? What is the role of the password in the public-private key scheme? Where you will work You will be logging into training.osgconnect.net for the CyberInfrastructure exercises. To confirm you have the proper authentication and authorization to do the exercises tomorrow and Friday we will test logins today. Due to the local network firewall setup (another security mechanism) and key installation, we will go to Brazil first (thanks to Raphael for setting up temporary VM). First be sure you are on the wireless network Eventos CeNAT . Replace XX with your osguser ID and use the password you have been supplied with the following command. ssh -o PreferredAuthentications=password osguserXX@200.145.46.31 If you are using putty, you should fill the Host Name (or IP address) with the value 200.145.46.31 as seen below: After hitting the Open button you may see the following message: You should hit the Yes button. Login on our submission node using the following command along with the password you have been supplied. $ ssh training.osgconnect.net The authenticity of host 'training.osgconnect.net (128.135.158.220)' can't be established. ECDSA key fingerprint is SHA256:gielJSpIiZisxGna5ocHtiK+0zAqFTdcEkLBOgnDUsg. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added 'training.osgconnect.net,128.135.158.220' (ECDSA) to the list of known hosts. Enter passphrase for key '/home/osguser01/.ssh/id_rsa': You may get a message asking you to establish the authenticity of this connection. Answer \"yes\". When you login to the machine you will be in your \"home directory\". We recommend that you work in this directory as nobody else can modify the files here (what security concept we covered today does this recommendation satisfy?).","title":"00 Pre Introduction Login"},{"location":"DataSanJose2019/CI/00-Pre-Introduction-Login/#install-an-ssh-cleint","text":"Download and install a SSH Client. We recommend PuTTY, but any ssh client is acceptable. https://www.putty.org/ Direct link to executable After opening the executable you should see the following screen:","title":"Install an SSH-Cleint"},{"location":"DataSanJose2019/CI/00-Pre-Introduction-Login/#authentication-for-ci-exercises","text":"You will receive login credentials at the beginning of this session. To authenticate (prove you who you say you are and establish what you are authorized to do) the bridgekeeper (login nodes) requires three bits of information: From Monty Python and the Holy Grail: BRIDGEKEEPER: Hee hee heh. Stop! What... is your name?\\ ARTHUR: It is 'Arthur', King of the Britons.\\ BRIDGEKEEPER: What... is your quest?\\ ARTHUR: To seek the Holy Grail.\\ BRIDGEKEEPER: What... is the air-speed velocity of an unladen swallow?\\ ARTHUR: What do you mean? An African or European swallow?\\ BRIDGEKEEPER: Huh? I-- I don't know that. Auuuuuuuugh!\\ BEDEVERE: How do know so much about swallows?\\ ARTHUR: Well, you have to know these things when you're a king, you know. or... 1) Tell me who you are. 2) Tell me something only you know. 3) Show me someething only you have. Why do you need both a password and a key? What is the role of the password in the public-private key scheme?","title":"Authentication for CI Exercises"},{"location":"DataSanJose2019/CI/00-Pre-Introduction-Login/#where-you-will-work","text":"You will be logging into training.osgconnect.net for the CyberInfrastructure exercises. To confirm you have the proper authentication and authorization to do the exercises tomorrow and Friday we will test logins today. Due to the local network firewall setup (another security mechanism) and key installation, we will go to Brazil first (thanks to Raphael for setting up temporary VM). First be sure you are on the wireless network Eventos CeNAT . Replace XX with your osguser ID and use the password you have been supplied with the following command. ssh -o PreferredAuthentications=password osguserXX@200.145.46.31 If you are using putty, you should fill the Host Name (or IP address) with the value 200.145.46.31 as seen below: After hitting the Open button you may see the following message: You should hit the Yes button. Login on our submission node using the following command along with the password you have been supplied. $ ssh training.osgconnect.net The authenticity of host 'training.osgconnect.net (128.135.158.220)' can't be established. ECDSA key fingerprint is SHA256:gielJSpIiZisxGna5ocHtiK+0zAqFTdcEkLBOgnDUsg. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added 'training.osgconnect.net,128.135.158.220' (ECDSA) to the list of known hosts. Enter passphrase for key '/home/osguser01/.ssh/id_rsa': You may get a message asking you to establish the authenticity of this connection. Answer \"yes\". When you login to the machine you will be in your \"home directory\". We recommend that you work in this directory as nobody else can modify the files here (what security concept we covered today does this recommendation satisfy?).","title":"Where you will work"},{"location":"DataSanJose2019/CI/01-Introduction/","text":"High Throughput Computing and Condor Introduction Preliminaries You will receive login credentials at the beginning of this session. You might want to refer to the online Condor 8.6.13 manual . You may enjoy browsing the Condor web page . Which Condor? We will be using Condor 8.6.13, which is a recent production version of Condor. Condor has two coexisting types of releases at any given time: stable and development. Condor 8.2.X and 7.8.x are considered stable releases, and you can know they are stable because the second digits (a 2 or a 8 in these cases) are even numbers. In a given stable series, all versions have the same features (for example 7.8.0 and 7.8.1 have the same set of features) and differ only in bug fixes. Where you will work Today you will log into training.osgconnect.net for all of your exercises. Login on submission node using the directions from yesterdays security session. They can be found here . When you login to the machine you will be in your \"home directory\". We recommend that you work in this directory as nobody else can modify the files here. You can always return to your home directory by running the command $ cd ~ The Exercises Throughout the Condor exercises, you will be given a fair amount of guidance. In several spots, there are suggestions for extra exercises to do \"on your own\" or as \"challenges\". Since you aren't being graded, there is no extra credit for doing them, but we encourage you to try them out. If you prefer, you can come back to the extra credit after you've completed the basic exercises. If you simply cruise through the exercises, you'll probably have free time--we encourage you to delve in more deeply. For all of the exercises, we'll assume that you are logged into user-training.osgconnect.net. You should have received your name and password for user-training.osgconnect.net at the beginning of the Computation Infrastructures lecture.","title":"High Throughput Computing and Condor Introduction"},{"location":"DataSanJose2019/CI/01-Introduction/#high-throughput-computing-and-condor-introduction","text":"","title":"High Throughput Computing and Condor Introduction"},{"location":"DataSanJose2019/CI/01-Introduction/#preliminaries","text":"You will receive login credentials at the beginning of this session. You might want to refer to the online Condor 8.6.13 manual . You may enjoy browsing the Condor web page .","title":"Preliminaries"},{"location":"DataSanJose2019/CI/01-Introduction/#which-condor","text":"We will be using Condor 8.6.13, which is a recent production version of Condor. Condor has two coexisting types of releases at any given time: stable and development. Condor 8.2.X and 7.8.x are considered stable releases, and you can know they are stable because the second digits (a 2 or a 8 in these cases) are even numbers. In a given stable series, all versions have the same features (for example 7.8.0 and 7.8.1 have the same set of features) and differ only in bug fixes.","title":"Which Condor?"},{"location":"DataSanJose2019/CI/01-Introduction/#where-you-will-work","text":"Today you will log into training.osgconnect.net for all of your exercises. Login on submission node using the directions from yesterdays security session. They can be found here . When you login to the machine you will be in your \"home directory\". We recommend that you work in this directory as nobody else can modify the files here. You can always return to your home directory by running the command $ cd ~","title":"Where you will work"},{"location":"DataSanJose2019/CI/01-Introduction/#the-exercises","text":"Throughout the Condor exercises, you will be given a fair amount of guidance. In several spots, there are suggestions for extra exercises to do \"on your own\" or as \"challenges\". Since you aren't being graded, there is no extra credit for doing them, but we encourage you to try them out. If you prefer, you can come back to the extra credit after you've completed the basic exercises. If you simply cruise through the exercises, you'll probably have free time--we encourage you to delve in more deeply. For all of the exercises, we'll assume that you are logged into user-training.osgconnect.net. You should have received your name and password for user-training.osgconnect.net at the beginning of the Computation Infrastructures lecture.","title":"The Exercises"},{"location":"DataSanJose2019/CI/02-OurJobManager/","text":"Our Condor Installation Objective This exercise should help you understand the basics of how Condor is installed, what Condor processes (a.k.a. daemons) are running, and what they do. Login to the Condor submit computer Before you start, make sure you are logged into training.osgconnect.net $ hostname training.osgconnect.net You should have been given your username and password when you arrived this afternoon. If you don't know them, talk to Rob. Looking at our Condor installation How do you know what version of Condor you are using? Try condor_version : $ condor_version $CondorVersion: 8.6.13 Jan 16 2019 $ $CondorPlatform: X86_64-CentOS_7.6 $ Note that the \"CondorPlatform\" reports the type of computer we built it on, not the computer we're running on. It was built on CentOS_7.6, but you might notice that we're running on CentOS Linux release 7.3.1. Extra Tip: The OS version Do you know how to find the OS version? You can usually look in /etc/os-release to find out: $ cat /etc/os-release Or you can run: $ hostnamectl Where is Condor installed? # Show the location of the condor_q binary $ which condor_q /usr/bin/condor_q # Show which RPM installed Condor $ rpm -q condor condor-8.6.13-1.2.osg34.el7.x86_64 Condor has some configuration files that it needs to find. They are in the standard location, /etc/condor $ ls /etc/condor condor_config condor_ssh_to_job_sshd_config_template ganglia.d condor_config.local config.d Condor has some directories that it keeps records of jobs in. Remember that each submission computer keeps track of all jobs submitted to it. That's in the local directory: $ condor_config_val -v LOCAL_DIR LOCAL_DIR = /var # at: /etc/condor/condor_config, line 26 # raw: LOCAL_DIR = /var $ ls -CF /var/lib/condor execute/ spool/ spool.q1/ spool.q2/ spool.q3/ spool.q4/ spool.q5/ The spool directory is where Condor keeps the jobs you submit, while the execute directory is where Condor keeps running jobs. Since this is a submission-only computer, it should be empty. Check if Condor is running. Your output will differ slightly, but you should see condor_master with the other Condor daemons listed under it: $ ps auwx --forest | grep condor_ | grep -v grep condor 2299245 0.0 0.1 50972 7348 ? Ss Jul10 0:08 condor_master -pidfile /var/run/condor/condor_master.pid root 2299287 0.0 0.1 25924 5072 ? S Jul10 1:54 \\_ condor_procd -A /var/run/condor/procd_pipe -L /var/log/condor/ProcLog -R 1000000 -S 60 -C 499 condor 2299288 0.0 0.1 50596 7796 ? Ss Jul10 0:16 \\_ condor_shared_port -f condor 2299289 0.0 0.2 70020 9100 ? Ss Jul10 0:13 \\_ condor_collector -f condor 2299290 0.0 0.5 116132 23872 ? Ss Jul10 6:19 \\_ condor_schedd -f condor 2299291 0.0 0.1 51056 7956 ? Ss Jul10 0:59 \\_ condor_negotiator -f For this version of Condor there are four processes running: the condor_master, the condor_schedd, the condor_procd, and condor_schedd. In general, you might see many different Condor processes. Here's a list of the processes: condor_master : This program runs constantly and ensures that all other parts of Condor are running. If they hang or crash, it restarts them. condor_schedd : If this program is running, it allows jobs to be submitted from this computer--that is, your computer is a \"submit machine\". This will advertise jobs to the central manager so that it knows about them. It will contact a condor_startd on other execute machines for each job that needs to be started. condor_procd: This process helps Condor track process (from jobs) that it creates condor_collector: This program is part of the Condor central manager. It collects information about all computers in the pool as well as which users want to run jobs. It is what normally responds to the condor_status command. At the school, it is running on a different computer, and you can figure out which one: Other daemons include: condor_negotiator: This program is part of the Condor central manager. It decides what jobs should be run where. It is run on the same computer as the collector. condor_startd: If this program is running, it allows jobs to be started up on this computer--that is, your computer is an \"execute machine\". This advertises your computer to the central manager so that it knows about this computer. It will start up the jobs that run. condor_shadow: For each job that has been submitted from this computer, there is one condor_shadow running. It will watch over the job as it runs remotely. In some cases it will provide some assistance (see the standard universe later.) You may or may not see any condor_shadow processes running, depending on what is happening on the computer when you try it out. condor_shared_port: Used to assist Condor with networking by allowing multiple Condor processes to share a single network port. condor_q You can find out what jobs have been submitted on your computer with the condor_q command: $ condor_q -- Schedd: user-training.osgconnect.net : <128.135.158.195:9618?... @ 08/12/18 16:10:58 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE HOLD TOTAL JOB_IDS 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended The output that you see will be different depending on what jobs are running. Notice what we can see from this: ID : We can see each jobs cluster and process number. For the first job, the cluster is 60256 and the process is 0. OWNER : We can see who owns the job. SUBMITTED : We can see when the job was submitted RUN_TIME : We can see how long the job has been running. ST : We can see what the current state of the job is. I is idle, R is running. PRI : We can see the priority of the job. SIZE : We can see the memory consumption of the job. CMD : We can see the program that is being executed. Extra Tip What else can you find out with condor_q? Try any one of: man condor_q condor_q -help condor_q from the online manual Double bonus points How do you use the -constraint or -format options to condor_q ? When would you want them? When would you use the -l option? This might be an easier exercise to try once you submit some jobs. condor_status You can find out what computers are in your Condor pool. (A pool is similar to a cluster, but it doesn't have the connotation that all computers are dedicated full-time to computation: some may be desktop computers owned by users.) To look, use condor_status: $ condor_status -pool flock.opensciencegrid.org Name OpSys Arch State Activity LoadAv Mem ActvtyTime slot1@amundsen.grid.uchicago.edu LINUX X86_64 Owner Idle 0.000 32768 1+02:46:31 slot2@amundsen.grid.uchicago.edu LINUX X86_64 Owner Idle 0.000 32768 5+01:05:58 slot1@c2 LINUX X86_64 Unclaimed Idle 0.000 48289 3+10:04:49 slot1@dhcp-10-1-202-3 LINUX X86_64 Unclaimed Idle 0.000 3251 0+08:10:13 slot1_1@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 6144 0+01:09:46 slot1_2@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 6144 0+00:46:46 slot1_3@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 2048 0+00:53:08 slot1_4@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 1024 0+05:48:14 slot1_5@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.000 6144 0+00:16:48 slot1_6@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 2816 0+13:16:34 ... Let's look at exactly what you can see: Name : The name of the computer. Sometimes this gets chopped off, like above. OpSys : The operating system, though not at the granularity you may wish: It says \"Linux\" instead of which distribution and version of Linux. Arch : The architecture, such as INTEL or PPC. State : The state is often Claimed (when it is running a Condor job) or Unclaimed (when it is not running a Condor job). It can be in a few other states as well, such as Matched. Activity : This is usually something like Busy or Idle. Sometimes you may see a computer that is Claimed, but no job has yet begun on the computer. Then it is Claimed/Idle. Hopefully this doesn't last very long. LoadAv : The load average on the computer. Mem : The computers memory in megabytes. ActvtyTime : How long the computer has been doing what it's been doing. Extra credit What else can you find out with condor_status? Try any one of: man condor_status condor_status -help condor_status from the online manual Note in particular the options like -master and -schedd . When would these be useful? When would the -l option be useful?","title":"Our Condor Installation"},{"location":"DataSanJose2019/CI/02-OurJobManager/#our-condor-installation","text":"","title":"Our Condor Installation"},{"location":"DataSanJose2019/CI/02-OurJobManager/#objective","text":"This exercise should help you understand the basics of how Condor is installed, what Condor processes (a.k.a. daemons) are running, and what they do.","title":"Objective"},{"location":"DataSanJose2019/CI/02-OurJobManager/#login-to-the-condor-submit-computer","text":"Before you start, make sure you are logged into training.osgconnect.net $ hostname training.osgconnect.net You should have been given your username and password when you arrived this afternoon. If you don't know them, talk to Rob.","title":"Login to the Condor submit computer"},{"location":"DataSanJose2019/CI/02-OurJobManager/#looking-at-our-condor-installation","text":"How do you know what version of Condor you are using? Try condor_version : $ condor_version $CondorVersion: 8.6.13 Jan 16 2019 $ $CondorPlatform: X86_64-CentOS_7.6 $ Note that the \"CondorPlatform\" reports the type of computer we built it on, not the computer we're running on. It was built on CentOS_7.6, but you might notice that we're running on CentOS Linux release 7.3.1.","title":"Looking at our Condor installation"},{"location":"DataSanJose2019/CI/02-OurJobManager/#extra-tip-the-os-version","text":"Do you know how to find the OS version? You can usually look in /etc/os-release to find out: $ cat /etc/os-release Or you can run: $ hostnamectl Where is Condor installed? # Show the location of the condor_q binary $ which condor_q /usr/bin/condor_q # Show which RPM installed Condor $ rpm -q condor condor-8.6.13-1.2.osg34.el7.x86_64 Condor has some configuration files that it needs to find. They are in the standard location, /etc/condor $ ls /etc/condor condor_config condor_ssh_to_job_sshd_config_template ganglia.d condor_config.local config.d Condor has some directories that it keeps records of jobs in. Remember that each submission computer keeps track of all jobs submitted to it. That's in the local directory: $ condor_config_val -v LOCAL_DIR LOCAL_DIR = /var # at: /etc/condor/condor_config, line 26 # raw: LOCAL_DIR = /var $ ls -CF /var/lib/condor execute/ spool/ spool.q1/ spool.q2/ spool.q3/ spool.q4/ spool.q5/ The spool directory is where Condor keeps the jobs you submit, while the execute directory is where Condor keeps running jobs. Since this is a submission-only computer, it should be empty. Check if Condor is running. Your output will differ slightly, but you should see condor_master with the other Condor daemons listed under it: $ ps auwx --forest | grep condor_ | grep -v grep condor 2299245 0.0 0.1 50972 7348 ? Ss Jul10 0:08 condor_master -pidfile /var/run/condor/condor_master.pid root 2299287 0.0 0.1 25924 5072 ? S Jul10 1:54 \\_ condor_procd -A /var/run/condor/procd_pipe -L /var/log/condor/ProcLog -R 1000000 -S 60 -C 499 condor 2299288 0.0 0.1 50596 7796 ? Ss Jul10 0:16 \\_ condor_shared_port -f condor 2299289 0.0 0.2 70020 9100 ? Ss Jul10 0:13 \\_ condor_collector -f condor 2299290 0.0 0.5 116132 23872 ? Ss Jul10 6:19 \\_ condor_schedd -f condor 2299291 0.0 0.1 51056 7956 ? Ss Jul10 0:59 \\_ condor_negotiator -f For this version of Condor there are four processes running: the condor_master, the condor_schedd, the condor_procd, and condor_schedd. In general, you might see many different Condor processes. Here's a list of the processes: condor_master : This program runs constantly and ensures that all other parts of Condor are running. If they hang or crash, it restarts them. condor_schedd : If this program is running, it allows jobs to be submitted from this computer--that is, your computer is a \"submit machine\". This will advertise jobs to the central manager so that it knows about them. It will contact a condor_startd on other execute machines for each job that needs to be started. condor_procd: This process helps Condor track process (from jobs) that it creates condor_collector: This program is part of the Condor central manager. It collects information about all computers in the pool as well as which users want to run jobs. It is what normally responds to the condor_status command. At the school, it is running on a different computer, and you can figure out which one: Other daemons include: condor_negotiator: This program is part of the Condor central manager. It decides what jobs should be run where. It is run on the same computer as the collector. condor_startd: If this program is running, it allows jobs to be started up on this computer--that is, your computer is an \"execute machine\". This advertises your computer to the central manager so that it knows about this computer. It will start up the jobs that run. condor_shadow: For each job that has been submitted from this computer, there is one condor_shadow running. It will watch over the job as it runs remotely. In some cases it will provide some assistance (see the standard universe later.) You may or may not see any condor_shadow processes running, depending on what is happening on the computer when you try it out. condor_shared_port: Used to assist Condor with networking by allowing multiple Condor processes to share a single network port.","title":"Extra Tip: The OS version"},{"location":"DataSanJose2019/CI/02-OurJobManager/#condor_q","text":"You can find out what jobs have been submitted on your computer with the condor_q command: $ condor_q -- Schedd: user-training.osgconnect.net : <128.135.158.195:9618?... @ 08/12/18 16:10:58 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE HOLD TOTAL JOB_IDS 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended The output that you see will be different depending on what jobs are running. Notice what we can see from this: ID : We can see each jobs cluster and process number. For the first job, the cluster is 60256 and the process is 0. OWNER : We can see who owns the job. SUBMITTED : We can see when the job was submitted RUN_TIME : We can see how long the job has been running. ST : We can see what the current state of the job is. I is idle, R is running. PRI : We can see the priority of the job. SIZE : We can see the memory consumption of the job. CMD : We can see the program that is being executed.","title":"condor_q"},{"location":"DataSanJose2019/CI/02-OurJobManager/#extra-tip","text":"What else can you find out with condor_q? Try any one of: man condor_q condor_q -help condor_q from the online manual","title":"Extra Tip"},{"location":"DataSanJose2019/CI/02-OurJobManager/#double-bonus-points","text":"How do you use the -constraint or -format options to condor_q ? When would you want them? When would you use the -l option? This might be an easier exercise to try once you submit some jobs.","title":"Double bonus points"},{"location":"DataSanJose2019/CI/02-OurJobManager/#condor_status","text":"You can find out what computers are in your Condor pool. (A pool is similar to a cluster, but it doesn't have the connotation that all computers are dedicated full-time to computation: some may be desktop computers owned by users.) To look, use condor_status: $ condor_status -pool flock.opensciencegrid.org Name OpSys Arch State Activity LoadAv Mem ActvtyTime slot1@amundsen.grid.uchicago.edu LINUX X86_64 Owner Idle 0.000 32768 1+02:46:31 slot2@amundsen.grid.uchicago.edu LINUX X86_64 Owner Idle 0.000 32768 5+01:05:58 slot1@c2 LINUX X86_64 Unclaimed Idle 0.000 48289 3+10:04:49 slot1@dhcp-10-1-202-3 LINUX X86_64 Unclaimed Idle 0.000 3251 0+08:10:13 slot1_1@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 6144 0+01:09:46 slot1_2@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 6144 0+00:46:46 slot1_3@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 2048 0+00:53:08 slot1_4@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 1024 0+05:48:14 slot1_5@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.000 6144 0+00:16:48 slot1_6@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 2816 0+13:16:34 ... Let's look at exactly what you can see: Name : The name of the computer. Sometimes this gets chopped off, like above. OpSys : The operating system, though not at the granularity you may wish: It says \"Linux\" instead of which distribution and version of Linux. Arch : The architecture, such as INTEL or PPC. State : The state is often Claimed (when it is running a Condor job) or Unclaimed (when it is not running a Condor job). It can be in a few other states as well, such as Matched. Activity : This is usually something like Busy or Idle. Sometimes you may see a computer that is Claimed, but no job has yet begun on the computer. Then it is Claimed/Idle. Hopefully this doesn't last very long. LoadAv : The load average on the computer. Mem : The computers memory in megabytes. ActvtyTime : How long the computer has been doing what it's been doing.","title":"condor_status"},{"location":"DataSanJose2019/CI/02-OurJobManager/#extra-credit","text":"What else can you find out with condor_status? Try any one of: man condor_status condor_status -help condor_status from the online manual Note in particular the options like -master and -schedd . When would these be useful? When would the -l option be useful?","title":"Extra credit"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/","text":"Submitting your first Condor job Objective The objective of this exercise to have you run and understand your first Condor job, as well as run small sets of jobs in a parameter sweep. This is an important exercise because it is the basis for everything that follows. If there is anything you don't understand in this exercise, please ask before you continue on. Because this is an important foundation, please seriously consider doing the \u201cOn Your Own\u201d section. First you need a job Before you can submit a job to Condor, you need a job. We will quickly write a small program in C. If you aren't an expert C programmer, fear not. We will hold your hand throughout this process. Create a file called simple.c using your favorite editor. Put it anywhere you like in your home directory. In that file, put the following text. Copy and paste is a good choice: $ mkdir -p ~/condor-test $ cd ~/condor-test Use your preferred text editor to create this C program. (Shown below with nano.) $ nano simple.c Paste in the following C code. #include <stdio.h> int main(int argc, char **argv) { int sleep_time; int input; int failure; if (argc != 3) { printf(\"Usage: simple &lt;sleep-time&gt; &lt;integer&gt;\\n\"); failure = 1; } else { sleep_time = atoi(argv[1]); input = atoi(argv[2]); printf(\"Thinking really hard for %d seconds...\\n\", sleep_time); sleep(sleep_time); printf(\"We calculated: %d\\n\", input * 2); failure = 0; } return failure; } Now compile that program: $ gcc -o simple simple.c $ ls -lh simple -rwxrwxr-x 1 roy roy 595K Jun 20 11:12 simple Finally, run the program and tell it to sleep for four seconds and calculate 10 * 2: $ ./simple 4 10 Thinking really hard for 4 seconds... We calculated: 20 Great! You just had a job run locally on the machine you are logged into (user-training.osgconnect.net). The next step is to run this job on a remote computer - and this is a job you can tell Condor to run! Although it clearly isn't an interesting job, it models some of the aspects of a real scientific program: it takes a while to run and it does a calculation. Think back to the lecture. I said that our first step was to have a job to run. Now we'll work on running it in Condor, and eventually running lots of copies of it. Submitting your job Now that you have a job, you just have to tell Condor to run it. Put the following text into a file called submit : Universe = vanilla Executable = simple Arguments = 4 10 +ProjectName = \"ConnectTrain\" Log = simple.log Output = simple.out Error = simple.error requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ON_EXIT Queue Let's examine each of these lines: Universe: The vanilla universe means a plain old job. Later on, we'll encounter some special universes. Executable: The name of your program Arguments: These are the arguments you want. They will be the same arguments we typed above. Log: This is the name of a file where Condor will record information about your job's execution. While it's not required, it is a really good idea to have a log. If something goes wrong you can refer to this log to help figure out the problem. Output: Where Condor should put the standard output from your job. Error: Where Condor should put the standard error from your job. Our job isn't likely to have any, but we'll put it there to be safe. should_transfer_files: Tell Condor that it should transfer files, instead of relying on a shared filesystem. While your home directories (on the glite-tutor computers) are mounted on NFS, you do not have user accounts on the worker nodes, so your jobs cannot access files on NFS. In addition, NFS isn't available between the local UI computers and the remote worker nodes. Therefore we will have Condor transfer files to the remote computer. when_to_transfer_output: A technical detail about when files should be transported back to the computer from which you submitted your job. Don't worry about the details for now. If you're really curious, you can read all the details in the Condor manual . Next, tell Condor to run your job: $ condor_submit submit Submitting job(s). 1 job(s) submitted to cluster 16. Now, watch your job run (insert your username in the command below instead of USER . If you forgot your username use the whoami command. Note that most of your output will be different than the example, the important column to watch is the ST column - the job state): # Note the job state of 'I' means the job is idle - not yet running $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:08 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 2056.0 osguser99 7/19 03:40 0+00:00:00 I 0 0.0 simple 4 10 Total for query: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended Total for all users: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended # After some time your job will enter the 'R' state which means it is currently running $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:14 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 2056.0 osguser99 7/19 03:40 0+00:00:02 R 0 0.0 simple 4 10 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended # When your job disappears from the queue that means it completed. $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:21 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD Total for query: 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Total for all users: 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Tip : While you are waiting for your job to run and complete you can check out \"A few tips and tricks\" to learn how to user condor_q more effectively. When my job was done, it was no longer listed. Because I told Condor to log information about my job, I can see what happened: $ cat simple.log 000 (032.000.000) 08/18 15:18:13 Job submitted from host: <10.0.0.252:9645> ... 001 (032.000.000) 08/18 15:18:32 Job executing on host: <172.16.200.1:9250> ... 006 (032.000.000) 08/18 15:18:32 Image size of job updated: 7 0 - MemoryUsage of job (MB) 0 - ResidentSetSize of job (KB) ... 005 (032.000.000) 08/18 15:18:33 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 56 - Run Bytes Sent By Job 7059 - Run Bytes Received By Job 56 - Total Bytes Sent By Job 7059 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 15 7 17605109 Memory (MB) : 0 1 1900 That looks good: the job started up quickly, though you will often see slightly slower startups. Condor doesn't optimize for fast job startup, but for high throughput, The job ran for four seconds. Now take a look at the job's output: $ cat simple.out Thinking really hard for 4 seconds... We calculated: 20 Excellent! We ran our sophisticated scientific job on a Condor pool! We've only run one job though. Can we run more? Doing a parameter sweep If you only ever had to run a single job, you probably wouldn't need Condor. But we would like to have our program calculate a whole set of values for different inputs. How can we do that? Let's change our submit file to look like this: Universe = vanilla Executable = simple +ProjectName = \"ConnectTrain\" Arguments = 4 10 Log = simple.$(Process).log Output = simple.$(Process).out Error = simple.$(Process).error requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ON_EXIT Queue Arguments = 4 11 Queue Arguments = 4 12 Queue There are two important differences to notice here. First, the Log, Output and Error lines have the $(Process) macro in them. This means that the output and error files will be named according to the process number of the job. You'll see what this looks like in a moment. Second, we told Condor to run the same job an extra two times by adding extra Arguments and Queue statements. We are doing a parameter sweep on the values 10, 11, and 12. Let's see what happens: $ condor_submit submit Submitting job(s)... 3 job(s) submitted to cluster 18. $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 34.0 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 0 34 34.1 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 1 34 34.2 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 2 34 3 jobs; 0 completed, 0 removed, 4 idle, 0 running, 0 held, 0 suspended $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 34.0 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 0 34 34.1 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 1 34 34.2 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 2 34 3 jobs; 0 completed, 0 removed, 0 idle, 4 running, 0 held, 0 suspended $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended $ ls simple*out simple.0.out simple.1.out simple.2.out simple.out $ cat simple.0.out Thinking really hard for 4 seconds... We calculated: 20 $ cat simple.1.out Thinking really hard for 4 seconds... We calculated: 22 $ cat simple.2.out Thinking really hard for 4 seconds... We calculated: 24 Notice that we had three jobs with the same cluster number, but different process numbers. They have the same cluster number because they were all submitted from the same submit file. When the jobs ran, they created three different output files, each with the desired output. You are now ready to submit lots of jobs! Although this example was simple, Condor has many, many options so you can get a wide variety of behaviors. You can find many of these if you look at the documentation for condor_submit . On your own Now that you've gotten your feet wet, try a few things on your own. Just one log file There's no reason to have a separate log file for each job. Change your submit file so that it uses a single log file. Does it all still work? New outputs for each run You might have noticed that the output files were over-written when you re-ran the jobs. (That is, simple.1.out was just re-written.) That was okay for a simple exercise, but it might be very bad if you had wanted to keep around the results. Maybe you changed a parameter or rebuilt your program, and you want to compare the outputs. Just like you used $(Process) , you can also use $(Cluster) . This will be a number from your job ID. For example, it would be 34 from the above example. Change your submit file to use $(Cluster) and $(Process) . If you do two job submissions, will you have separate output files? Lots of jobs Instead of specifying the Arguments multiple times with multiple queue statements, try this: Arguments = $(Process) $(Cluster) queue 10 What does it mean? What happens? Does it work as you expect? (An aside: you might wish to be able to do math, something like $(Process)+1 . Unfortunately, you can't do that.) Challenges If you have time and feel comfortable with the technical background, try these extra challenges. You'll need to peruse the Condor manual (particularly the manual page for condor_submit ) to find answers. Feel free to ask Rob--he'd love to give you hints! Make another scientific program (probably just modify simple.c) that takes its input from a file. Now submit 3 copies of this program where each input file is in a separate directory. Use the initialdir option described in the manual . This will let you specify a directory for the input to the program. You can run specify the initialdir with $(Process) . You can specify extra files to copy with transfer_input_files . Now you're really learning the basics of running something like a real scientific job! Condor can send you email when a job finishes. How can you control this? You know that your job should never run for more than four hours. If it does, then the job should be killed because there is a problem. How can you tell Condor to do this for you?","title":"Submitting your first Condor job"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#submitting-your-first-condor-job","text":"","title":"Submitting your first Condor job"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#objective","text":"The objective of this exercise to have you run and understand your first Condor job, as well as run small sets of jobs in a parameter sweep. This is an important exercise because it is the basis for everything that follows. If there is anything you don't understand in this exercise, please ask before you continue on. Because this is an important foundation, please seriously consider doing the \u201cOn Your Own\u201d section.","title":"Objective"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#first-you-need-a-job","text":"Before you can submit a job to Condor, you need a job. We will quickly write a small program in C. If you aren't an expert C programmer, fear not. We will hold your hand throughout this process. Create a file called simple.c using your favorite editor. Put it anywhere you like in your home directory. In that file, put the following text. Copy and paste is a good choice: $ mkdir -p ~/condor-test $ cd ~/condor-test Use your preferred text editor to create this C program. (Shown below with nano.) $ nano simple.c Paste in the following C code. #include <stdio.h> int main(int argc, char **argv) { int sleep_time; int input; int failure; if (argc != 3) { printf(\"Usage: simple &lt;sleep-time&gt; &lt;integer&gt;\\n\"); failure = 1; } else { sleep_time = atoi(argv[1]); input = atoi(argv[2]); printf(\"Thinking really hard for %d seconds...\\n\", sleep_time); sleep(sleep_time); printf(\"We calculated: %d\\n\", input * 2); failure = 0; } return failure; } Now compile that program: $ gcc -o simple simple.c $ ls -lh simple -rwxrwxr-x 1 roy roy 595K Jun 20 11:12 simple Finally, run the program and tell it to sleep for four seconds and calculate 10 * 2: $ ./simple 4 10 Thinking really hard for 4 seconds... We calculated: 20 Great! You just had a job run locally on the machine you are logged into (user-training.osgconnect.net). The next step is to run this job on a remote computer - and this is a job you can tell Condor to run! Although it clearly isn't an interesting job, it models some of the aspects of a real scientific program: it takes a while to run and it does a calculation. Think back to the lecture. I said that our first step was to have a job to run. Now we'll work on running it in Condor, and eventually running lots of copies of it.","title":"First you need a job"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#submitting-your-job","text":"Now that you have a job, you just have to tell Condor to run it. Put the following text into a file called submit : Universe = vanilla Executable = simple Arguments = 4 10 +ProjectName = \"ConnectTrain\" Log = simple.log Output = simple.out Error = simple.error requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ON_EXIT Queue Let's examine each of these lines: Universe: The vanilla universe means a plain old job. Later on, we'll encounter some special universes. Executable: The name of your program Arguments: These are the arguments you want. They will be the same arguments we typed above. Log: This is the name of a file where Condor will record information about your job's execution. While it's not required, it is a really good idea to have a log. If something goes wrong you can refer to this log to help figure out the problem. Output: Where Condor should put the standard output from your job. Error: Where Condor should put the standard error from your job. Our job isn't likely to have any, but we'll put it there to be safe. should_transfer_files: Tell Condor that it should transfer files, instead of relying on a shared filesystem. While your home directories (on the glite-tutor computers) are mounted on NFS, you do not have user accounts on the worker nodes, so your jobs cannot access files on NFS. In addition, NFS isn't available between the local UI computers and the remote worker nodes. Therefore we will have Condor transfer files to the remote computer. when_to_transfer_output: A technical detail about when files should be transported back to the computer from which you submitted your job. Don't worry about the details for now. If you're really curious, you can read all the details in the Condor manual . Next, tell Condor to run your job: $ condor_submit submit Submitting job(s). 1 job(s) submitted to cluster 16. Now, watch your job run (insert your username in the command below instead of USER . If you forgot your username use the whoami command. Note that most of your output will be different than the example, the important column to watch is the ST column - the job state): # Note the job state of 'I' means the job is idle - not yet running $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:08 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 2056.0 osguser99 7/19 03:40 0+00:00:00 I 0 0.0 simple 4 10 Total for query: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended Total for all users: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended # After some time your job will enter the 'R' state which means it is currently running $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:14 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 2056.0 osguser99 7/19 03:40 0+00:00:02 R 0 0.0 simple 4 10 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended # When your job disappears from the queue that means it completed. $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:21 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD Total for query: 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Total for all users: 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Tip : While you are waiting for your job to run and complete you can check out \"A few tips and tricks\" to learn how to user condor_q more effectively. When my job was done, it was no longer listed. Because I told Condor to log information about my job, I can see what happened: $ cat simple.log 000 (032.000.000) 08/18 15:18:13 Job submitted from host: <10.0.0.252:9645> ... 001 (032.000.000) 08/18 15:18:32 Job executing on host: <172.16.200.1:9250> ... 006 (032.000.000) 08/18 15:18:32 Image size of job updated: 7 0 - MemoryUsage of job (MB) 0 - ResidentSetSize of job (KB) ... 005 (032.000.000) 08/18 15:18:33 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 56 - Run Bytes Sent By Job 7059 - Run Bytes Received By Job 56 - Total Bytes Sent By Job 7059 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 15 7 17605109 Memory (MB) : 0 1 1900 That looks good: the job started up quickly, though you will often see slightly slower startups. Condor doesn't optimize for fast job startup, but for high throughput, The job ran for four seconds. Now take a look at the job's output: $ cat simple.out Thinking really hard for 4 seconds... We calculated: 20 Excellent! We ran our sophisticated scientific job on a Condor pool! We've only run one job though. Can we run more?","title":"Submitting your job"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#doing-a-parameter-sweep","text":"If you only ever had to run a single job, you probably wouldn't need Condor. But we would like to have our program calculate a whole set of values for different inputs. How can we do that? Let's change our submit file to look like this: Universe = vanilla Executable = simple +ProjectName = \"ConnectTrain\" Arguments = 4 10 Log = simple.$(Process).log Output = simple.$(Process).out Error = simple.$(Process).error requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ON_EXIT Queue Arguments = 4 11 Queue Arguments = 4 12 Queue There are two important differences to notice here. First, the Log, Output and Error lines have the $(Process) macro in them. This means that the output and error files will be named according to the process number of the job. You'll see what this looks like in a moment. Second, we told Condor to run the same job an extra two times by adding extra Arguments and Queue statements. We are doing a parameter sweep on the values 10, 11, and 12. Let's see what happens: $ condor_submit submit Submitting job(s)... 3 job(s) submitted to cluster 18. $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 34.0 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 0 34 34.1 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 1 34 34.2 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 2 34 3 jobs; 0 completed, 0 removed, 4 idle, 0 running, 0 held, 0 suspended $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 34.0 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 0 34 34.1 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 1 34 34.2 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 2 34 3 jobs; 0 completed, 0 removed, 0 idle, 4 running, 0 held, 0 suspended $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended $ ls simple*out simple.0.out simple.1.out simple.2.out simple.out $ cat simple.0.out Thinking really hard for 4 seconds... We calculated: 20 $ cat simple.1.out Thinking really hard for 4 seconds... We calculated: 22 $ cat simple.2.out Thinking really hard for 4 seconds... We calculated: 24 Notice that we had three jobs with the same cluster number, but different process numbers. They have the same cluster number because they were all submitted from the same submit file. When the jobs ran, they created three different output files, each with the desired output. You are now ready to submit lots of jobs! Although this example was simple, Condor has many, many options so you can get a wide variety of behaviors. You can find many of these if you look at the documentation for condor_submit .","title":"Doing a parameter sweep"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#on-your-own","text":"Now that you've gotten your feet wet, try a few things on your own.","title":"On your own"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#just-one-log-file","text":"There's no reason to have a separate log file for each job. Change your submit file so that it uses a single log file. Does it all still work?","title":"Just one log file"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#new-outputs-for-each-run","text":"You might have noticed that the output files were over-written when you re-ran the jobs. (That is, simple.1.out was just re-written.) That was okay for a simple exercise, but it might be very bad if you had wanted to keep around the results. Maybe you changed a parameter or rebuilt your program, and you want to compare the outputs. Just like you used $(Process) , you can also use $(Cluster) . This will be a number from your job ID. For example, it would be 34 from the above example. Change your submit file to use $(Cluster) and $(Process) . If you do two job submissions, will you have separate output files?","title":"New outputs for each run"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#lots-of-jobs","text":"Instead of specifying the Arguments multiple times with multiple queue statements, try this: Arguments = $(Process) $(Cluster) queue 10 What does it mean? What happens? Does it work as you expect? (An aside: you might wish to be able to do math, something like $(Process)+1 . Unfortunately, you can't do that.)","title":"Lots of jobs"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#challenges","text":"If you have time and feel comfortable with the technical background, try these extra challenges. You'll need to peruse the Condor manual (particularly the manual page for condor_submit ) to find answers. Feel free to ask Rob--he'd love to give you hints! Make another scientific program (probably just modify simple.c) that takes its input from a file. Now submit 3 copies of this program where each input file is in a separate directory. Use the initialdir option described in the manual . This will let you specify a directory for the input to the program. You can run specify the initialdir with $(Process) . You can specify extra files to copy with transfer_input_files . Now you're really learning the basics of running something like a real scientific job! Condor can send you email when a job finishes. How can you control this? You know that your job should never run for more than four hours. If it does, then the job should be killed because there is a problem. How can you tell Condor to do this for you?","title":"Challenges"},{"location":"DataSanJose2019/CI/04-TipsandTricks/","text":"A few tips and tricks Objective This exercise will teach you a few nifty commands to help you use Condor more easily. Tips for condor_q condor_q can show you your job ClassAd. Recall back to the lecture and the discussion of ClassAds. For instance, you can look at the ClassAd for a single job: $ condor_q -l YOUR_JOB_CLUSTER_NUMBER MaxHosts = 1 User = \"kagross@frontal.cci.ucad.sn\" OnExitHold = false CoreSize = 0 MachineAttrCpus0 = 1 WantRemoteSyscalls = false MyType = \"Job\" Rank = 0.0 CumulativeSuspensionTime = 0 MinHosts = 1 PeriodicHold = false PeriodicRemove = false Err = \"simple.49.error\" ProcId = 49 EnteredCurrentStatus = 1408374244 UserLog = \"/home/kagross/condor-test/s ... output trimmed ... There are some interesting parts you can check out. How many CPUs is the job requesting. (This can be more than one, but for the exercises we will do today it will be 1) $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep RequestCpus RequestCpus = 1 Where is the user log for this job? This is helpful when you assist someone else in debugging and they're not sure. $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep UserLog UserLog = \"/home/kagross/condor-test/simple.47.log\" What are the job's requirements? Condor automatically fills some in for you to make sure your job runs on a reasonable computer in our cluster, but you can override any of these. I've broken the output into multiple lines to explain it to you. $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep Requirements Requirements =( TARGET.Arch == \"X86_64\" ) <em># Run on a 64-bit computer && ( TARGET.OpSys == \"LINUX\" ) <em># Make sure you run on Linux && ( TARGET.Disk >= RequestDisk ) <em># Make sure the default disk Condor is on has enough disk space. && ( TARGET.Memory >= RequestMemory ) <em># Make sure the computer has enough memory && ( TARGET.HasFileTransfer ) <em># Only run on a computer that can accept your files. What else can you find that's interesting in the ClassAd? Removing jobs If you submit a job that you realize has a problem, you can remove it with condor_rm . For example: $ condor_q -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 29.0 roy 6/21 15:23 0+00:00:00 I 0 0.7 simple 60 10 1 jobs; 0 completed, 0 removed, 2 idle, 0 running, 0 held, 0 suspended $ condor_rm YOUR_JOB_CLUSTER_NUMBER Job 29.0 marked for removal $ condor_q -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended A few tips: You can remove all of your jobs with the -all option. You can't remove other users jobs. There are fancy options to condor_rm . Historical information You can see information about jobs that completed and are no longer in the queue with the condor_history command. It's rare that you want to see all the jobs, so try looking at jobs for just you: $ condor_history USER For example: $ condor_history kagross 9.9 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 9 9 9.8 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 8 9 9.11 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 11 9 9.7 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 7 9 9.5 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 5 9 9.6 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 6 9 9.3 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 3 9 9.2 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 2 9 9.1 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 1 9 9.0 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 9.4 kagross 7/31 12:44 0+00:00:01 C 7/31 12:44 /home/kagross/simple 4 9 8.0 kagross 7/31 12:42 0+00:00:07 C 7/31 12:42 /home/kagross/simple 4 10 ...","title":"A few tips and tricks"},{"location":"DataSanJose2019/CI/04-TipsandTricks/#a-few-tips-and-tricks","text":"","title":"A few tips and tricks"},{"location":"DataSanJose2019/CI/04-TipsandTricks/#objective","text":"This exercise will teach you a few nifty commands to help you use Condor more easily.","title":"Objective"},{"location":"DataSanJose2019/CI/04-TipsandTricks/#tips-for-condor_q","text":"condor_q can show you your job ClassAd. Recall back to the lecture and the discussion of ClassAds. For instance, you can look at the ClassAd for a single job: $ condor_q -l YOUR_JOB_CLUSTER_NUMBER MaxHosts = 1 User = \"kagross@frontal.cci.ucad.sn\" OnExitHold = false CoreSize = 0 MachineAttrCpus0 = 1 WantRemoteSyscalls = false MyType = \"Job\" Rank = 0.0 CumulativeSuspensionTime = 0 MinHosts = 1 PeriodicHold = false PeriodicRemove = false Err = \"simple.49.error\" ProcId = 49 EnteredCurrentStatus = 1408374244 UserLog = \"/home/kagross/condor-test/s ... output trimmed ... There are some interesting parts you can check out. How many CPUs is the job requesting. (This can be more than one, but for the exercises we will do today it will be 1) $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep RequestCpus RequestCpus = 1 Where is the user log for this job? This is helpful when you assist someone else in debugging and they're not sure. $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep UserLog UserLog = \"/home/kagross/condor-test/simple.47.log\" What are the job's requirements? Condor automatically fills some in for you to make sure your job runs on a reasonable computer in our cluster, but you can override any of these. I've broken the output into multiple lines to explain it to you. $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep Requirements Requirements =( TARGET.Arch == \"X86_64\" ) <em># Run on a 64-bit computer && ( TARGET.OpSys == \"LINUX\" ) <em># Make sure you run on Linux && ( TARGET.Disk >= RequestDisk ) <em># Make sure the default disk Condor is on has enough disk space. && ( TARGET.Memory >= RequestMemory ) <em># Make sure the computer has enough memory && ( TARGET.HasFileTransfer ) <em># Only run on a computer that can accept your files. What else can you find that's interesting in the ClassAd?","title":"Tips for condor_q"},{"location":"DataSanJose2019/CI/04-TipsandTricks/#removing-jobs","text":"If you submit a job that you realize has a problem, you can remove it with condor_rm . For example: $ condor_q -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 29.0 roy 6/21 15:23 0+00:00:00 I 0 0.7 simple 60 10 1 jobs; 0 completed, 0 removed, 2 idle, 0 running, 0 held, 0 suspended $ condor_rm YOUR_JOB_CLUSTER_NUMBER Job 29.0 marked for removal $ condor_q -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended A few tips: You can remove all of your jobs with the -all option. You can't remove other users jobs. There are fancy options to condor_rm .","title":"Removing jobs"},{"location":"DataSanJose2019/CI/04-TipsandTricks/#historical-information","text":"You can see information about jobs that completed and are no longer in the queue with the condor_history command. It's rare that you want to see all the jobs, so try looking at jobs for just you: $ condor_history USER For example: $ condor_history kagross 9.9 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 9 9 9.8 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 8 9 9.11 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 11 9 9.7 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 7 9 9.5 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 5 9 9.6 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 6 9 9.3 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 3 9 9.2 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 2 9 9.1 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 1 9 9.0 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 9.4 kagross 7/31 12:44 0+00:00:01 C 7/31 12:44 /home/kagross/simple 4 9 8.0 kagross 7/31 12:42 0+00:00:07 C 7/31 12:42 /home/kagross/simple 4 10 ...","title":"Historical information"},{"location":"DataSanJose2019/CI/05-ScriptingJob/","text":"Using scripting languages Objective The objective of this exercise is to demonstrate that you can submit jobs to Condor in any language, including scripting languages. At this point, you might be asking yourself, \"This is all well and good, but I don't write programs in C. Can I use other languages?\" Absolutely. Let's assume you like to write program in Bourne shell. Make sure your program begins with #!/bin/sh , and you're good to go. Save this example code into a file called simple.sh using nano or your favorite editor. #!/bin/sh if [ $# -ne 2 ]; then echo \"Usage: simple.sh sleep-time integer\" exit 1 fi echo \"Thinking really hard for $1 seconds..\" sleep $1 answer=$(( $2 * 2 )) echo \"We calculated $answer.\" exit 0 This script will not be executable without changing the permissions. $ chmod 755 simple.sh Can you write a submit file to run this job? This should be easy--the script is your Executable , not /bin/sh . You may also want to change the name of your submit.log , submit.out , and submit.err in your submit file to be sure they are not written over when you run this submission. Challenge Rewrite this script in Perl or Python (if you're comfortable with one of those languages). Does it still work for you?","title":"Using scripting languages"},{"location":"DataSanJose2019/CI/05-ScriptingJob/#using-scripting-languages","text":"","title":"Using scripting languages"},{"location":"DataSanJose2019/CI/05-ScriptingJob/#objective","text":"The objective of this exercise is to demonstrate that you can submit jobs to Condor in any language, including scripting languages. At this point, you might be asking yourself, \"This is all well and good, but I don't write programs in C. Can I use other languages?\" Absolutely. Let's assume you like to write program in Bourne shell. Make sure your program begins with #!/bin/sh , and you're good to go. Save this example code into a file called simple.sh using nano or your favorite editor. #!/bin/sh if [ $# -ne 2 ]; then echo \"Usage: simple.sh sleep-time integer\" exit 1 fi echo \"Thinking really hard for $1 seconds..\" sleep $1 answer=$(( $2 * 2 )) echo \"We calculated $answer.\" exit 0 This script will not be executable without changing the permissions. $ chmod 755 simple.sh Can you write a submit file to run this job? This should be easy--the script is your Executable , not /bin/sh . You may also want to change the name of your submit.log , submit.out , and submit.err in your submit file to be sure they are not written over when you run this submission.","title":"Objective"},{"location":"DataSanJose2019/CI/05-ScriptingJob/#challenge","text":"Rewrite this script in Perl or Python (if you're comfortable with one of those languages). Does it still work for you?","title":"Challenge"},{"location":"DataSanJose2019/CI/06-RJob/","text":"Running a job with R Objective The objective of this exercise is to learn how to run a program that depends on a larger run-time environment that isn't already available on your cluster. The run-time environment refers to all the hardware and software resources available on the machine where your job is running. We will focus on the available software in this exercise. The Problem Sometimes you want to run a program that depends on a larger run-time environment. For example, perhaps you wrote your program in Perl, but there is no Perl installed on the cluster. (That's an unlikely example, intended just to give you feel for what I mean by \"run-time environment\".) This is a common problem distributed computing users encounter. For example, many people like to use Matlab (or its open-source cousin, Octave ) or R for doing calculations. These require a fair amount of run-time environment to run the programs you write. What do you do if they aren't installed? There are at least two possibilities: Ask your kindly system administrator to install it for you on all of the computers you might run on. Bring the environment (Such as Octave or R) along with your job. Before you read any further, please stop for a moment, and think about the tradeoffs between these two methodologies. They both have benefits and drawbacks. Why would you choose each of them? Why not? Here are some of my answers... Pros and cons of having your system administrator do it for you PRO - It's a lot easier for you. PRO - You have to transfer less data with each job. CON - You have to wait for the system administrator to install them. CON - If you want upgrades (or downgrades), you have to ask again and wait for them. Pros and cons of bringing it along CON - It's more complex for you. CON - You have to transfer the application and data with each job (or have a job that pre-stages it for you.) PRO - You are in complete control: when you need a tweak, an upgrade, or a downgrade, you can make it happen. Clearly, there is a choice here. I'd like to enable you to be able to bring along your run-time environment with you. In my experience, if you are capable of bringing it with you, you can take advantage of more computers: you don't have to wait for someone to build and install the environment for you. We'd like to demonstrate how to install one run-time environment called R . Don't worry if you have no experience with it: neither do I. It's a package for doing math and statistics and it lets you write programs in the R language. (Actually, it's the S language. Who chose names like this that are hard to Google?) We've built a minimal version of R that won't do graphical output, which makes it much less interesting, but it's good enough for our purposes. Setup OSG's implementation of CVMFS is called OASIS and we will be using this to gain access to R. $ source /cvmfs/oasis.opensciencegrid.org/osg/modules/lmod/5.6.2/init/bash Let's look at what software is available: $ module avail And then we load the R module: $ module load R You'll need an R program. After hours of combing the internet coding, I present to you my first R program. Save it in a file called demo.r using nano or your favorite editor: len <- 100 fibvals <- numeric(len) fibvals[1] <- 1 fibvals[2] <- 1 for (i in 3:len) { fibvals[i] <- fibvals[i-1]+fibvals[i-2] } print(\"First 100 Fibonacci numbers:\") for (i in 1:len) { print(fibvals[i], digits = 21) } print(\"Number of possible combinations of cards in a 52 card deck:\") comb <- factorial(52) print(comb, digits = 21) This program prints the first 100 Fibonacci numbers . These are the numbers that show up in the weirdest places, like pineapples and sunflowers. It's a sequence of numbers beginning with 0, 1, 1, 2, 3, 5, 8... where each successive number is the sum of the previous two numbers. It also prints 52 factorial, which is the number of possible combinations of a standard 52-card deck (not including the jokers, of course). R is a bit fussy about where it's been installed on disk, so I had to write a wrapper program so it will happily run wherever it lands in our cluster. I could make you work it out, but that seems unfair. Save this program in run-r.sh . If you're curious about exactly why it's needed, ask Rob. There are two important parts to it, and you should know what they are, at least conceptually because this is the magic you would need to do for any run-time environment you want to bring along. Load the R environment using module . In general you might have to do more work. Invoke R, using whatever magic is needed. In our case, I set up some environment variables and invoke the right executable. #!/bin/sh -x if [ $# -ne 1 ]; then echo \"Usage: run-r \" exit 1 fi # Step 1: Set up our environment, the R module. source /cvmfs/oasis.opensciencegrid.org/osg/modules/lmod/current/init/bash module load R module load libgfortran # Step 2, Invoke R with the proper environment R --slave --vanilla < $1 You could easily execute this on OSG Connect locally by making the shell script executable and executing it. $ chmod 755 run-r.sh $ ./run-r.sh demo.r On your own Write a Condor submit file that will use R to run the demo.r program. Make sure you get back the output. Make sure you transfer the program. The answer This should be easy for you now, but if it's not, here's the answer, just in case. universe = vanilla executable = run-r.sh +ProjectName = \"ConnectTrain\" arguments = demo.r transfer_input_files = demo.r log = R.log.$(Cluster).$(Process) error = R.err.$(Cluster).$(Process) output = R.out.$(Cluster).$(Process) requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue","title":"Running a job with R"},{"location":"DataSanJose2019/CI/06-RJob/#running-a-job-with-r","text":"","title":"Running a job with R"},{"location":"DataSanJose2019/CI/06-RJob/#objective","text":"The objective of this exercise is to learn how to run a program that depends on a larger run-time environment that isn't already available on your cluster. The run-time environment refers to all the hardware and software resources available on the machine where your job is running. We will focus on the available software in this exercise.","title":"Objective"},{"location":"DataSanJose2019/CI/06-RJob/#the-problem","text":"Sometimes you want to run a program that depends on a larger run-time environment. For example, perhaps you wrote your program in Perl, but there is no Perl installed on the cluster. (That's an unlikely example, intended just to give you feel for what I mean by \"run-time environment\".) This is a common problem distributed computing users encounter. For example, many people like to use Matlab (or its open-source cousin, Octave ) or R for doing calculations. These require a fair amount of run-time environment to run the programs you write. What do you do if they aren't installed? There are at least two possibilities: Ask your kindly system administrator to install it for you on all of the computers you might run on. Bring the environment (Such as Octave or R) along with your job. Before you read any further, please stop for a moment, and think about the tradeoffs between these two methodologies. They both have benefits and drawbacks. Why would you choose each of them? Why not? Here are some of my answers...","title":"The Problem"},{"location":"DataSanJose2019/CI/06-RJob/#pros-and-cons-of-having-your-system-administrator-do-it-for-you","text":"PRO - It's a lot easier for you. PRO - You have to transfer less data with each job. CON - You have to wait for the system administrator to install them. CON - If you want upgrades (or downgrades), you have to ask again and wait for them.","title":"Pros and cons of having your system administrator do it for you"},{"location":"DataSanJose2019/CI/06-RJob/#pros-and-cons-of-bringing-it-along","text":"CON - It's more complex for you. CON - You have to transfer the application and data with each job (or have a job that pre-stages it for you.) PRO - You are in complete control: when you need a tweak, an upgrade, or a downgrade, you can make it happen. Clearly, there is a choice here. I'd like to enable you to be able to bring along your run-time environment with you. In my experience, if you are capable of bringing it with you, you can take advantage of more computers: you don't have to wait for someone to build and install the environment for you. We'd like to demonstrate how to install one run-time environment called R . Don't worry if you have no experience with it: neither do I. It's a package for doing math and statistics and it lets you write programs in the R language. (Actually, it's the S language. Who chose names like this that are hard to Google?) We've built a minimal version of R that won't do graphical output, which makes it much less interesting, but it's good enough for our purposes.","title":"Pros and cons of bringing it along"},{"location":"DataSanJose2019/CI/06-RJob/#setup","text":"OSG's implementation of CVMFS is called OASIS and we will be using this to gain access to R. $ source /cvmfs/oasis.opensciencegrid.org/osg/modules/lmod/5.6.2/init/bash Let's look at what software is available: $ module avail And then we load the R module: $ module load R You'll need an R program. After hours of combing the internet coding, I present to you my first R program. Save it in a file called demo.r using nano or your favorite editor: len <- 100 fibvals <- numeric(len) fibvals[1] <- 1 fibvals[2] <- 1 for (i in 3:len) { fibvals[i] <- fibvals[i-1]+fibvals[i-2] } print(\"First 100 Fibonacci numbers:\") for (i in 1:len) { print(fibvals[i], digits = 21) } print(\"Number of possible combinations of cards in a 52 card deck:\") comb <- factorial(52) print(comb, digits = 21) This program prints the first 100 Fibonacci numbers . These are the numbers that show up in the weirdest places, like pineapples and sunflowers. It's a sequence of numbers beginning with 0, 1, 1, 2, 3, 5, 8... where each successive number is the sum of the previous two numbers. It also prints 52 factorial, which is the number of possible combinations of a standard 52-card deck (not including the jokers, of course). R is a bit fussy about where it's been installed on disk, so I had to write a wrapper program so it will happily run wherever it lands in our cluster. I could make you work it out, but that seems unfair. Save this program in run-r.sh . If you're curious about exactly why it's needed, ask Rob. There are two important parts to it, and you should know what they are, at least conceptually because this is the magic you would need to do for any run-time environment you want to bring along. Load the R environment using module . In general you might have to do more work. Invoke R, using whatever magic is needed. In our case, I set up some environment variables and invoke the right executable. #!/bin/sh -x if [ $# -ne 1 ]; then echo \"Usage: run-r \" exit 1 fi # Step 1: Set up our environment, the R module. source /cvmfs/oasis.opensciencegrid.org/osg/modules/lmod/current/init/bash module load R module load libgfortran # Step 2, Invoke R with the proper environment R --slave --vanilla < $1 You could easily execute this on OSG Connect locally by making the shell script executable and executing it. $ chmod 755 run-r.sh $ ./run-r.sh demo.r","title":"Setup"},{"location":"DataSanJose2019/CI/06-RJob/#on-your-own","text":"Write a Condor submit file that will use R to run the demo.r program. Make sure you get back the output. Make sure you transfer the program.","title":"On your own"},{"location":"DataSanJose2019/CI/06-RJob/#the-answer","text":"This should be easy for you now, but if it's not, here's the answer, just in case. universe = vanilla executable = run-r.sh +ProjectName = \"ConnectTrain\" arguments = demo.r transfer_input_files = demo.r log = R.log.$(Cluster).$(Process) error = R.err.$(Cluster).$(Process) output = R.out.$(Cluster).$(Process) requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue","title":"The answer"},{"location":"DataSanJose2019/CI/07-WorkingwithFiles/","text":"Working with data in files Objective The objective of this exercise is to teach you how to provide files as input to your job, and get output as files back from your job. Data Movement So far, we've done really simple examples where the entire input to the program is just on the command-line. What do you do if you have data files to deal with? Let's walk through a short example. First, let's make a program, call it analyze.sh that analyzes a text file that it is provided on the command-line. #!/bin/sh if [ $# -ne 1 ]; then echo \"Usage: analyze.sh <filename>\" exit 1 fi echo \"About to do a deep analysis of $1...\" echo \"First, we convert it to all upper case (see $1.upper)\" tr \"[:lower:]\" \"[:upper:]\" < $1 > $1.upper echo \"Next, we find the 10 most common words (see $1.10)\" cat $1 | tr \"[:upper:]\" \"[:lower:]\" | tr -cs \"[:alpha:]\" \"\\n\" | sort | uniq -c | sort --key=1,7 -n -r | head -10 > $1.10 sleep 5 You also need a file to analyze. Put the following text into a file called gettysburg . Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this. But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us -- that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion -- that we here highly resolve that these dead shall not have died in vain -- that this nation, under God, shall have a new birth of freedom -- and that government of the people, by the people, for the people, shall not perish from the earth. Our submit file looks nearly identical to what we had before, except for the one bolded line that specifies the data file to transfer. Put the following text into a file called submit.speech . Universe = vanilla Executable = analyze.sh Output = analyze.out Error = analyze.error Log = analyze.log Arguments = gettysburg +ProjectName = \"ConnectTrain\" requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") ShouldTransferFiles = Yes WhenToTransferOutput = ON_EXIT transfer_input_files = gettysburg queue Notice that you just had to specify the input files and not the output files. Condor will automatically transfer back any new files, so you don't have to worry about it. Nifty, huh? Now run the job. $ condor_submit submit.speech Submitting job(s). 1 job(s) submitted to cluster 37. $ ls -lh gettys* -rw-rw-r--. 1 kagross kagross 1.5K Aug 18 15:41 gettysburg -rw-r--r--. 1 kagross kagross 120 Aug 18 15:42 gettysburg.10 -rw-r--r--. 1 kagross kagross 1.5K Aug 18 15:42 gettysburg.upper You got your files! Check them out--do they look okay? On your own Create several text files, then submit jobs (preferably from a single submit file) to analyze each of them. If you're at a loss to create some text files, here are a few for you. Walkthrough of the Original Text Game \"Adventure\" The Story of Captain Midnight The Universal Geek Code Tao of Programming Instead of downloading these files and transferring them directly, can you change your transfer-input-files to use a URL and have Condor download them for you? Give this a try.","title":"Working with data in files"},{"location":"DataSanJose2019/CI/07-WorkingwithFiles/#working-with-data-in-files","text":"","title":"Working with data in files"},{"location":"DataSanJose2019/CI/07-WorkingwithFiles/#objective","text":"The objective of this exercise is to teach you how to provide files as input to your job, and get output as files back from your job.","title":"Objective"},{"location":"DataSanJose2019/CI/07-WorkingwithFiles/#data-movement","text":"So far, we've done really simple examples where the entire input to the program is just on the command-line. What do you do if you have data files to deal with? Let's walk through a short example. First, let's make a program, call it analyze.sh that analyzes a text file that it is provided on the command-line. #!/bin/sh if [ $# -ne 1 ]; then echo \"Usage: analyze.sh <filename>\" exit 1 fi echo \"About to do a deep analysis of $1...\" echo \"First, we convert it to all upper case (see $1.upper)\" tr \"[:lower:]\" \"[:upper:]\" < $1 > $1.upper echo \"Next, we find the 10 most common words (see $1.10)\" cat $1 | tr \"[:upper:]\" \"[:lower:]\" | tr -cs \"[:alpha:]\" \"\\n\" | sort | uniq -c | sort --key=1,7 -n -r | head -10 > $1.10 sleep 5 You also need a file to analyze. Put the following text into a file called gettysburg . Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this. But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us -- that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion -- that we here highly resolve that these dead shall not have died in vain -- that this nation, under God, shall have a new birth of freedom -- and that government of the people, by the people, for the people, shall not perish from the earth. Our submit file looks nearly identical to what we had before, except for the one bolded line that specifies the data file to transfer. Put the following text into a file called submit.speech . Universe = vanilla Executable = analyze.sh Output = analyze.out Error = analyze.error Log = analyze.log Arguments = gettysburg +ProjectName = \"ConnectTrain\" requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") ShouldTransferFiles = Yes WhenToTransferOutput = ON_EXIT transfer_input_files = gettysburg queue Notice that you just had to specify the input files and not the output files. Condor will automatically transfer back any new files, so you don't have to worry about it. Nifty, huh? Now run the job. $ condor_submit submit.speech Submitting job(s). 1 job(s) submitted to cluster 37. $ ls -lh gettys* -rw-rw-r--. 1 kagross kagross 1.5K Aug 18 15:41 gettysburg -rw-r--r--. 1 kagross kagross 120 Aug 18 15:42 gettysburg.10 -rw-r--r--. 1 kagross kagross 1.5K Aug 18 15:42 gettysburg.upper You got your files! Check them out--do they look okay?","title":"Data Movement"},{"location":"DataSanJose2019/CI/07-WorkingwithFiles/#on-your-own","text":"Create several text files, then submit jobs (preferably from a single submit file) to analyze each of them. If you're at a loss to create some text files, here are a few for you. Walkthrough of the Original Text Game \"Adventure\" The Story of Captain Midnight The Universal Geek Code Tao of Programming Instead of downloading these files and transferring them directly, can you change your transfer-input-files to use a URL and have Condor download them for you? Give this a try.","title":"On your own"},{"location":"DataSanJose2019/CI/08-Mandlebrot/","text":"A brief detour through the Mandlebrot set Before we dive into a more complicated DAG, let's get a more interesting job. I'm tired of this lazy, sleepy job that only does trivial mathematics. Let's make pretty pictures! We have a small program that draws pictures of the Mandlebrot set. You can read about the Mandlebrot set on Wikipedia , or you can simply appreciate the pretty pictures. It's a fractal. We have a simple program that can draw the Mandlebrot set. It's called goatbrot , A simple invocation of goatbrot You can generate the Mandlebrot set with two simple commands. Generate a PPM image of the Mandlebrot set: $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000000_000000.ppm -c 0,0 -w 3 -s 1000,1000 Add the Fast Fourier Transform and ImageMagick packages: $ module load fftw Convert it to a JPEG image and write into your home directory: $ convert tile_000000_000000.ppm ~/mandle.gif Open a new terminal window and move the file to local machine for viewing (substitute your username in place of USER ): $ scp USER@training.osgconnect.net:/home/USER/mandle.gif ./ Point Browser at the file URL: firefox ./mandle.gif The goatbroat program takes several parameters. Let's break them down: -i 1000 The number of iterations. Bigger numbers generate more accurate images but are slower to run. -o tile_000000_000000.ppm The output file to generate. -c 0,0 The center point of the image. Here it is the point (0,0). -w 3 The width of the image. Here is 3. -s 1000,1000 The size of the final image. Here we generate a picture that is 1000 pixels wide and 1000 pixels tall. Dividing goatbrot up The Mandlebrot set can take a while to create, particularly if you make the iterations large or the image size large. What if we broke the creation of the image into multiple invocations then stitched them together? Once we do that, we can run the each goatbroat in parallel in our cluster. Here's an example you can run by hand. Run goatbroat 4 times : $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000000_000000.ppm -c -0.75,0.75 -w 1.5 -s 500,500 $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000000_000001.ppm -c 0.75,0.75 -w 1.5 -s 500,500 $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000001_000000.ppm -c -0.75,-0.75 -w 1.5 -s 500,500 $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000001_000001.ppm -c 0.75,-0.75 -w 1.5 -s 500,500 Stitch them together : $ montage tile_000000_000000.ppm tile_000000_000001.ppm tile_000001_000000.ppm tile_000001_000001.ppm -mode Concatenate -tile 2x2 ~/mandle.gif This will produce the same image as above. We broke the image space into a 2 by 2 grid and ran goatbrot on each section of the grid. The montage program simply stitches the files together. Try it! Run the commands above, make sure you can create the Mandlebrot image. When you create the image, you might wonder how you can view it. The same way we did above, or more simply by moving the file to a web accessible location. cp mandle.gif ~/public point your browser at the stash web server: http://stash.osgconnect.net/~USER","title":"A brief detour through the Mandlebrot set"},{"location":"DataSanJose2019/CI/08-Mandlebrot/#a-brief-detour-through-the-mandlebrot-set","text":"Before we dive into a more complicated DAG, let's get a more interesting job. I'm tired of this lazy, sleepy job that only does trivial mathematics. Let's make pretty pictures! We have a small program that draws pictures of the Mandlebrot set. You can read about the Mandlebrot set on Wikipedia , or you can simply appreciate the pretty pictures. It's a fractal. We have a simple program that can draw the Mandlebrot set. It's called goatbrot ,","title":"A brief detour through the Mandlebrot set"},{"location":"DataSanJose2019/CI/08-Mandlebrot/#a-simple-invocation-of-goatbrot","text":"You can generate the Mandlebrot set with two simple commands. Generate a PPM image of the Mandlebrot set: $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000000_000000.ppm -c 0,0 -w 3 -s 1000,1000 Add the Fast Fourier Transform and ImageMagick packages: $ module load fftw Convert it to a JPEG image and write into your home directory: $ convert tile_000000_000000.ppm ~/mandle.gif Open a new terminal window and move the file to local machine for viewing (substitute your username in place of USER ): $ scp USER@training.osgconnect.net:/home/USER/mandle.gif ./ Point Browser at the file URL: firefox ./mandle.gif The goatbroat program takes several parameters. Let's break them down: -i 1000 The number of iterations. Bigger numbers generate more accurate images but are slower to run. -o tile_000000_000000.ppm The output file to generate. -c 0,0 The center point of the image. Here it is the point (0,0). -w 3 The width of the image. Here is 3. -s 1000,1000 The size of the final image. Here we generate a picture that is 1000 pixels wide and 1000 pixels tall.","title":"A simple invocation of goatbrot"},{"location":"DataSanJose2019/CI/08-Mandlebrot/#dividing-goatbrot-up","text":"The Mandlebrot set can take a while to create, particularly if you make the iterations large or the image size large. What if we broke the creation of the image into multiple invocations then stitched them together? Once we do that, we can run the each goatbroat in parallel in our cluster. Here's an example you can run by hand. Run goatbroat 4 times : $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000000_000000.ppm -c -0.75,0.75 -w 1.5 -s 500,500 $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000000_000001.ppm -c 0.75,0.75 -w 1.5 -s 500,500 $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000001_000000.ppm -c -0.75,-0.75 -w 1.5 -s 500,500 $ /stash/user/rquick/public/goatbrot-master/goatbrot -i 1000 -o tile_000001_000001.ppm -c 0.75,-0.75 -w 1.5 -s 500,500 Stitch them together : $ montage tile_000000_000000.ppm tile_000000_000001.ppm tile_000001_000000.ppm tile_000001_000001.ppm -mode Concatenate -tile 2x2 ~/mandle.gif This will produce the same image as above. We broke the image space into a 2 by 2 grid and ran goatbrot on each section of the grid. The montage program simply stitches the files together.","title":"Dividing goatbrot up"},{"location":"DataSanJose2019/CI/08-Mandlebrot/#try-it","text":"Run the commands above, make sure you can create the Mandlebrot image. When you create the image, you might wonder how you can view it. The same way we did above, or more simply by moving the file to a web accessible location. cp mandle.gif ~/public point your browser at the stash web server: http://stash.osgconnect.net/~USER","title":"Try it!"},{"location":"DataSanJose2019/CI/09-SimpleDAG/","text":"Coordinating set of jobs: A simple DAG Objective The objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job. What is DAGMan? Your tutorial leader will introduce you to DAGMan and DAGs. In short, DAGMan lets you submit complex sequences of jobs as long as they can be expressed as a directed acylic graph. For example, you may wish to run a large parameter sweep but before the sweep run you need to prepare your data. After the sweep runs, you need to collate the results. DAGMan has many abilities such as throttling jobs, recovery from failures, and more. More information about DAGMan can be found at in the Condor manual . Submitting a simple DAG We're going to go back to the \"simple\" example that we did first. (The one with the job that slept and multiplied a number by 2.) Make sure that you have a submit file has only one queue command in it, as when we first wrote it. And we will just run vanilla universe jobs for now, though we could equally well run standard universe jobs. Universe = vanilla Executable = simple Arguments = 4 10 +ProjectName = \"ConnectTrain\" Log = simple.log Output = simple.out Error = simple.error requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ON_EXIT Queue Make sure you've built the simple program. If you need to, go back to the instructions for your first job to do it again. We are going to get a bit more sophisticated in submitting our jobs now. Let's have three windows open. In one window you'll submit the job. In the second you will watch the queue. And in the third you will watch what DAGMan does. First we will create the most minimal DAG that can be created: a DAG with just one node. Put the text below into a file named simple.dag . job simple submit In your first window, submit the DAG: $ condor_submit_dag simple.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : simple.dag.condor.sub Log of DAGMan debugging messages : simple.dag.dagman.out Log of Condor library output : simple.dag.lib.out Log of Condor library error messages : simple.dag.lib.err Log of the life of condor_dagman itself : simple.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 61. ----------------------------------------------------------------------- In the second window, watch the queue (press Ctrl+C when finished watching to kill this process): $ watch -n 10 condor_q USER -nobatch -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:00:01 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:01:25 R 0 0.3 condor_dagman 62.0 roy 6/21 22:51 0+00:00:00 I 0 0.7 simple 4 10 2 jobs; 0 completed, 0 removed, 1 idle, 1 running, 0 held, 0 suspended -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:03:47 R 0 0.3 condor_dagman 62.0 roy 6/21 22:51 0+00:00:03 R 0 0.7 simple 4 10 2 jobs; 0 completed, 0 removed, 0 idle, 2 running, 0 held, 0 suspended -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Ctrl-C In the third window, watch what DAGMan does: $ tail -f --lines=500 simple.dag.dagman.out 6/21/12 22:51:13 Setting maximum accepts per cycle 8. 06/21/12 22:51:13 ****************************************************** 06/21/12 22:51:13 ** condor_scheduniv_exec.61.0 (CONDOR_DAGMAN) STARTING UP 06/21/12 22:51:13 ** /usr/bin/condor_dagman 06/21/12 22:51:13 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/21/12 22:51:13 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/21/12 22:51:13 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/21/12 22:51:13 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/21/12 22:51:13 ** PID = 5812 06/21/12 22:51:13 ** Log last touched 6/21 22:51:00 06/21/12 22:51:13 ****************************************************** 06/21/12 22:51:13 Using config source: /etc/condor/condor_config 06/21/12 22:51:13 Using local config sources: 06/21/12 22:51:13 /etc/condor/config.d/00-chtc-global.conf 06/21/12 22:51:13 /etc/condor/config.d/01-chtc-submit.conf 06/21/12 22:51:13 /etc/condor/config.d/02-chtc-flocking.conf 06/21/12 22:51:13 /etc/condor/config.d/03-chtc-jobrouter.conf 06/21/12 22:51:13 /etc/condor/config.d/04-chtc-blacklist.conf 06/21/12 22:51:13 /etc/condor/config.d/99-osg-ss-group.conf 06/21/12 22:51:13 /etc/condor/config.d/99-roy-extras.conf 06/21/12 22:51:13 /etc/condor/condor_config.local 06/21/12 22:51:13 DaemonCore: command socket at <128.104.100.55:60417> 06/21/12 22:51:13 DaemonCore: private command socket at <128.104.100.55:60417> 06/21/12 22:51:13 Setting maximum accepts per cycle 8. 06/21/12 22:51:13 DAGMAN_USE_STRICT setting: 0 06/21/12 22:51:13 DAGMAN_VERBOSITY setting: 3 06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880 06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_ENABLE setting: False 06/21/12 22:51:13 DAGMAN_SUBMIT_DELAY setting: 0 06/21/12 22:51:13 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6 06/21/12 22:51:13 DAGMAN_STARTUP_CYCLE_DETECT setting: False 06/21/12 22:51:13 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 5 06/21/12 22:51:13 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5 06/21/12 22:51:13 allow_events (DAGMAN_IGNORE_DUPLICATE_JOB_EXECUTION, DAGMAN_ALLOW_EVENTS) setting: 114 06/21/12 22:51:13 DAGMAN_RETRY_SUBMIT_FIRST setting: True 06/21/12 22:51:13 DAGMAN_RETRY_NODE_FIRST setting: False 06/21/12 22:51:13 DAGMAN_MAX_JOBS_IDLE setting: 0 06/21/12 22:51:13 DAGMAN_MAX_JOBS_SUBMITTED setting: 0 06/21/12 22:51:15 DAGMAN_MAX_PRE_SCRIPTS setting: 0 06/21/12 22:51:15 DAGMAN_MAX_POST_SCRIPTS setting: 0 06/21/12 22:51:15 DAGMAN_ALLOW_LOG_ERROR setting: False 06/21/12 22:51:15 DAGMAN_MUNGE_NODE_NAMES setting: True 06/21/12 22:51:15 DAGMAN_PROHIBIT_MULTI_JOBS setting: False 06/21/12 22:51:15 DAGMAN_SUBMIT_DEPTH_FIRST setting: False 06/21/12 22:51:15 DAGMAN_ALWAYS_RUN_POST setting: True 06/21/12 22:51:15 DAGMAN_ABORT_DUPLICATES setting: True 06/21/12 22:51:15 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: True 06/21/12 22:51:15 DAGMAN_PENDING_REPORT_INTERVAL setting: 600 06/21/12 22:51:15 DAGMAN_AUTO_RESCUE setting: True 06/21/12 22:51:15 DAGMAN_MAX_RESCUE_NUM setting: 100 06/21/12 22:51:15 DAGMAN_WRITE_PARTIAL_RESCUE setting: True 06/21/12 22:51:15 DAGMAN_DEFAULT_NODE_LOG setting: null 06/21/12 22:51:15 DAGMAN_GENERATE_SUBDAG_SUBMITS setting: True 06/21/12 22:51:15 ALL_DEBUG setting: 06/21/12 22:51:15 DAGMAN_DEBUG setting: 06/21/12 22:51:15 argv[0] == \"condor_scheduniv_exec.61.0\" 06/21/12 22:51:15 argv[1] == \"-Lockfile\" 06/21/12 22:51:15 argv[2] == \"simple.dag.lock\" 06/21/12 22:51:15 argv[3] == \"-AutoRescue\" 06/21/12 22:51:15 argv[4] == \"1\" 06/21/12 22:51:15 argv[5] == \"-DoRescueFrom\" 06/21/12 22:51:15 argv[6] == \"0\" 06/21/12 22:51:15 argv[7] == \"-Dag\" 06/21/12 22:51:15 argv[8] == \"simple.dag\" 06/21/12 22:51:15 argv[9] == \"-CsdVersion\" 06/21/12 22:51:15 argv[10] == \"$CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $\" 06/21/12 22:51:15 argv[11] == \"-Force\" 06/21/12 22:51:15 argv[12] == \"-Dagman\" 06/21/12 22:51:15 argv[13] == \"/usr/bin/condor_dagman\" 06/21/12 22:51:15 Default node log file is: </home/roy/condor/simple.dag.nodes.log> 06/21/12 22:51:15 DAG Lockfile will be written to simple.dag.lock 06/21/12 22:51:15 DAG Input file is simple.dag 06/21/12 22:51:15 Parsing 1 dagfiles 06/21/12 22:51:15 Parsing simple.dag ... 06/21/12 22:51:15 Dag contains 1 total jobs 06/21/12 22:51:15 Sleeping for 12 seconds to ensure ProcessId uniqueness 06/21/12 22:51:27 Bootstrapping... 06/21/12 22:51:27 Number of pre-completed nodes: 0 06/21/12 22:51:27 Registering condor_event_timer... 06/21/12 22:51:28 Sleeping for one second for log file consistency 06/21/12 22:51:29 MultiLogFiles: truncating log file /home/roy/condor/simple.log 06/21/12 22:51:29 Submitting Condor Node Simple job(s)... # Here's where the job is submitted 06/21/12 22:51:29 submitting: condor_submit -a dag_node_name' '=' 'Simple -a +DAGManJobId' '=' '61 -a DAGManJobId' '=' '61 -a submit_event_notes' '=' 'DAG' 'Node:' 'Simple -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"\" submit 06/21/12 22:51:30 From submit: Submitting job(s). 06/21/12 22:51:30 From submit: 1 job(s) submitted to cluster 62. 06/21/12 22:51:30 assigned Condor ID (62.0.0) 06/21/12 22:51:30 Just submitted 1 job this cycle... 06/21/12 22:51:30 Currently monitoring 1 Condor log file(s) 06/21/12 22:51:30 Event: ULOG_SUBMIT for Condor Node Simple (62.0.0) 06/21/12 22:51:30 Number of idle job procs: 1 06/21/12 22:51:30 Of 1 nodes total: 06/21/12 22:51:30 Done Pre Queued Post Ready Un-Ready Failed 06/21/12 22:51:30 === === === === === === === 06/21/12 22:51:30 0 0 1 0 0 0 0 06/21/12 22:51:30 0 job proc(s) currently held 06/21/12 22:55:05 Currently monitoring 1 Condor log file(s) # Here's where DAGMan noticed that the job is running 06/21/12 22:55:05 Event: ULOG_EXECUTE for Condor Node Simple (62.0.0) 06/21/12 22:55:05 Number of idle job procs: 0 06/21/12 22:55:10 Currently monitoring 1 Condor log file(s) 06/21/12 22:55:10 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0) 06/21/12 22:56:05 Currently monitoring 1 Condor log file(s) 06/21/12 22:56:05 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0) # Here's where DAGMan noticed that the job finished. 06/21/12 22:56:05 Event: ULOG_JOB_TERMINATED for Condor Node Simple (62.0.0) 06/21/12 22:56:05 Node Simple job proc (62.0.0) completed successfully. 06/21/12 22:56:05 Node Simple job completed 06/21/12 22:56:05 Number of idle job procs: 0 06/21/12 22:56:05 Of 1 nodes total: 06/21/12 22:56:05 Done Pre Queued Post Ready Un-Ready Failed 06/21/12 22:56:05 === === === === === === === 06/21/12 22:56:05 1 0 0 0 0 0 0 06/21/12 22:56:05 0 job proc(s) currently held # Here's where DAGMan noticed that all the work is done. 06/21/12 22:56:05 All jobs Completed! 06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/21/12 22:56:05 Note: 0 total job deferrals because of node category throttles 06/21/12 22:56:05 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/21/12 22:56:05 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/21/12 22:56:05 **** condor_scheduniv_exec.61.0 (condor_DAGMAN) pid 5812 EXITING WITH STATUS 0 Now verify your results: $ cat simple.log 000 (062.000.000) 06/21 22:51:30 Job submitted from host: <128.104.100.55:9618?sock=28867_10e4_2> DAG Node: Simple ... 001 (062.000.000) 06/21 22:55:00 Job executing on host: <128.104.58.36:46761> ... 006 (062.000.000) 06/21 22:55:09 Image size of job updated: 750 3 - MemoryUsage of job (MB) 2324 - ResidentSetSize of job (KB) ... 006 (062.000.000) 06/21 22:56:00 Image size of job updated: 780 3 - MemoryUsage of job (MB) 2324 - ResidentSetSize of job (KB) ... 005 (062.000.000) 06/21 22:56:00 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 57 - Run Bytes Sent By Job 608490 - Run Bytes Received By Job 57 - Total Bytes Sent By Job 608490 - Total Bytes Received By Job Partitionable Resources : Usage Request Cpus : 1 Disk (KB) : 750 750 Memory (MB) : 3 3 ... $ cat simple.out Thinking really hard for 4 seconds... We calculated: 20 Looking at DAGMan's various files, we see that DAGMan itself ran as a Condor job (specifically, a scheduler universe job). $ ls simple.dag.* simple.dag.condor.sub simple.dag.dagman.log simple.dag.dagman.out simple.dag.lib.err simple.dag.lib.out $ cat simple.dag.condor.sub # Filename: simple.dag.condor.sub # Generated by condor_submit_dag simple.dag universe = scheduler executable = /usr/bin/condor_dagman getenv = True output = simple.dag.lib.out error = simple.dag.lib.err log = simple.dag.dagman.log remove_kill_sig = SIGUSR1 +OtherJobRemoveRequirements = \"DAGManJobId == $(cluster)\" # Note: default on_exit_remove expression: # ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2)) # attempts to ensure that DAGMan is automatically # requeued by the schedd if it exits abnormally or # is killed (e.g., during a reboot). on_exit_remove = ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2)) copy_to_spool = False arguments = \"-f -l . -Lockfile simple.dag.lock -AutoRescue 1 -DoRescueFrom 0 -Dag simple.dag -CsdVersion $CondorVersion:' '7.7.6' 'Apr' '16' '2012' 'BuildID:' '34175' 'PRE-RELEASE-UWCS' '$ -Force -Dagman /usr/bin/condor_dagman\" environment = _CONDOR_DAGMAN_LOG=simple.dag.dagman.out;_CONDOR_MAX_DAGMAN_LOG=0 queue Clean up some of these files: $ rm simple.dag.* On your own Why does DAGman run as a Condor job? Look at the submit file for DAGMan: what does on_exit_remove do? Why is this here?","title":"Coordinating set of jobs: A simple DAG"},{"location":"DataSanJose2019/CI/09-SimpleDAG/#coordinating-set-of-jobs-a-simple-dag","text":"","title":"Coordinating set of jobs: A simple DAG"},{"location":"DataSanJose2019/CI/09-SimpleDAG/#objective","text":"The objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job.","title":"Objective"},{"location":"DataSanJose2019/CI/09-SimpleDAG/#what-is-dagman","text":"Your tutorial leader will introduce you to DAGMan and DAGs. In short, DAGMan lets you submit complex sequences of jobs as long as they can be expressed as a directed acylic graph. For example, you may wish to run a large parameter sweep but before the sweep run you need to prepare your data. After the sweep runs, you need to collate the results. DAGMan has many abilities such as throttling jobs, recovery from failures, and more. More information about DAGMan can be found at in the Condor manual .","title":"What is DAGMan?"},{"location":"DataSanJose2019/CI/09-SimpleDAG/#submitting-a-simple-dag","text":"We're going to go back to the \"simple\" example that we did first. (The one with the job that slept and multiplied a number by 2.) Make sure that you have a submit file has only one queue command in it, as when we first wrote it. And we will just run vanilla universe jobs for now, though we could equally well run standard universe jobs. Universe = vanilla Executable = simple Arguments = 4 10 +ProjectName = \"ConnectTrain\" Log = simple.log Output = simple.out Error = simple.error requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ON_EXIT Queue Make sure you've built the simple program. If you need to, go back to the instructions for your first job to do it again. We are going to get a bit more sophisticated in submitting our jobs now. Let's have three windows open. In one window you'll submit the job. In the second you will watch the queue. And in the third you will watch what DAGMan does. First we will create the most minimal DAG that can be created: a DAG with just one node. Put the text below into a file named simple.dag . job simple submit In your first window, submit the DAG: $ condor_submit_dag simple.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : simple.dag.condor.sub Log of DAGMan debugging messages : simple.dag.dagman.out Log of Condor library output : simple.dag.lib.out Log of Condor library error messages : simple.dag.lib.err Log of the life of condor_dagman itself : simple.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 61. ----------------------------------------------------------------------- In the second window, watch the queue (press Ctrl+C when finished watching to kill this process): $ watch -n 10 condor_q USER -nobatch -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:00:01 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:01:25 R 0 0.3 condor_dagman 62.0 roy 6/21 22:51 0+00:00:00 I 0 0.7 simple 4 10 2 jobs; 0 completed, 0 removed, 1 idle, 1 running, 0 held, 0 suspended -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:03:47 R 0 0.3 condor_dagman 62.0 roy 6/21 22:51 0+00:00:03 R 0 0.7 simple 4 10 2 jobs; 0 completed, 0 removed, 0 idle, 2 running, 0 held, 0 suspended -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Ctrl-C In the third window, watch what DAGMan does: $ tail -f --lines=500 simple.dag.dagman.out 6/21/12 22:51:13 Setting maximum accepts per cycle 8. 06/21/12 22:51:13 ****************************************************** 06/21/12 22:51:13 ** condor_scheduniv_exec.61.0 (CONDOR_DAGMAN) STARTING UP 06/21/12 22:51:13 ** /usr/bin/condor_dagman 06/21/12 22:51:13 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/21/12 22:51:13 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/21/12 22:51:13 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/21/12 22:51:13 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/21/12 22:51:13 ** PID = 5812 06/21/12 22:51:13 ** Log last touched 6/21 22:51:00 06/21/12 22:51:13 ****************************************************** 06/21/12 22:51:13 Using config source: /etc/condor/condor_config 06/21/12 22:51:13 Using local config sources: 06/21/12 22:51:13 /etc/condor/config.d/00-chtc-global.conf 06/21/12 22:51:13 /etc/condor/config.d/01-chtc-submit.conf 06/21/12 22:51:13 /etc/condor/config.d/02-chtc-flocking.conf 06/21/12 22:51:13 /etc/condor/config.d/03-chtc-jobrouter.conf 06/21/12 22:51:13 /etc/condor/config.d/04-chtc-blacklist.conf 06/21/12 22:51:13 /etc/condor/config.d/99-osg-ss-group.conf 06/21/12 22:51:13 /etc/condor/config.d/99-roy-extras.conf 06/21/12 22:51:13 /etc/condor/condor_config.local 06/21/12 22:51:13 DaemonCore: command socket at <128.104.100.55:60417> 06/21/12 22:51:13 DaemonCore: private command socket at <128.104.100.55:60417> 06/21/12 22:51:13 Setting maximum accepts per cycle 8. 06/21/12 22:51:13 DAGMAN_USE_STRICT setting: 0 06/21/12 22:51:13 DAGMAN_VERBOSITY setting: 3 06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880 06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_ENABLE setting: False 06/21/12 22:51:13 DAGMAN_SUBMIT_DELAY setting: 0 06/21/12 22:51:13 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6 06/21/12 22:51:13 DAGMAN_STARTUP_CYCLE_DETECT setting: False 06/21/12 22:51:13 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 5 06/21/12 22:51:13 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5 06/21/12 22:51:13 allow_events (DAGMAN_IGNORE_DUPLICATE_JOB_EXECUTION, DAGMAN_ALLOW_EVENTS) setting: 114 06/21/12 22:51:13 DAGMAN_RETRY_SUBMIT_FIRST setting: True 06/21/12 22:51:13 DAGMAN_RETRY_NODE_FIRST setting: False 06/21/12 22:51:13 DAGMAN_MAX_JOBS_IDLE setting: 0 06/21/12 22:51:13 DAGMAN_MAX_JOBS_SUBMITTED setting: 0 06/21/12 22:51:15 DAGMAN_MAX_PRE_SCRIPTS setting: 0 06/21/12 22:51:15 DAGMAN_MAX_POST_SCRIPTS setting: 0 06/21/12 22:51:15 DAGMAN_ALLOW_LOG_ERROR setting: False 06/21/12 22:51:15 DAGMAN_MUNGE_NODE_NAMES setting: True 06/21/12 22:51:15 DAGMAN_PROHIBIT_MULTI_JOBS setting: False 06/21/12 22:51:15 DAGMAN_SUBMIT_DEPTH_FIRST setting: False 06/21/12 22:51:15 DAGMAN_ALWAYS_RUN_POST setting: True 06/21/12 22:51:15 DAGMAN_ABORT_DUPLICATES setting: True 06/21/12 22:51:15 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: True 06/21/12 22:51:15 DAGMAN_PENDING_REPORT_INTERVAL setting: 600 06/21/12 22:51:15 DAGMAN_AUTO_RESCUE setting: True 06/21/12 22:51:15 DAGMAN_MAX_RESCUE_NUM setting: 100 06/21/12 22:51:15 DAGMAN_WRITE_PARTIAL_RESCUE setting: True 06/21/12 22:51:15 DAGMAN_DEFAULT_NODE_LOG setting: null 06/21/12 22:51:15 DAGMAN_GENERATE_SUBDAG_SUBMITS setting: True 06/21/12 22:51:15 ALL_DEBUG setting: 06/21/12 22:51:15 DAGMAN_DEBUG setting: 06/21/12 22:51:15 argv[0] == \"condor_scheduniv_exec.61.0\" 06/21/12 22:51:15 argv[1] == \"-Lockfile\" 06/21/12 22:51:15 argv[2] == \"simple.dag.lock\" 06/21/12 22:51:15 argv[3] == \"-AutoRescue\" 06/21/12 22:51:15 argv[4] == \"1\" 06/21/12 22:51:15 argv[5] == \"-DoRescueFrom\" 06/21/12 22:51:15 argv[6] == \"0\" 06/21/12 22:51:15 argv[7] == \"-Dag\" 06/21/12 22:51:15 argv[8] == \"simple.dag\" 06/21/12 22:51:15 argv[9] == \"-CsdVersion\" 06/21/12 22:51:15 argv[10] == \"$CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $\" 06/21/12 22:51:15 argv[11] == \"-Force\" 06/21/12 22:51:15 argv[12] == \"-Dagman\" 06/21/12 22:51:15 argv[13] == \"/usr/bin/condor_dagman\" 06/21/12 22:51:15 Default node log file is: </home/roy/condor/simple.dag.nodes.log> 06/21/12 22:51:15 DAG Lockfile will be written to simple.dag.lock 06/21/12 22:51:15 DAG Input file is simple.dag 06/21/12 22:51:15 Parsing 1 dagfiles 06/21/12 22:51:15 Parsing simple.dag ... 06/21/12 22:51:15 Dag contains 1 total jobs 06/21/12 22:51:15 Sleeping for 12 seconds to ensure ProcessId uniqueness 06/21/12 22:51:27 Bootstrapping... 06/21/12 22:51:27 Number of pre-completed nodes: 0 06/21/12 22:51:27 Registering condor_event_timer... 06/21/12 22:51:28 Sleeping for one second for log file consistency 06/21/12 22:51:29 MultiLogFiles: truncating log file /home/roy/condor/simple.log 06/21/12 22:51:29 Submitting Condor Node Simple job(s)... # Here's where the job is submitted 06/21/12 22:51:29 submitting: condor_submit -a dag_node_name' '=' 'Simple -a +DAGManJobId' '=' '61 -a DAGManJobId' '=' '61 -a submit_event_notes' '=' 'DAG' 'Node:' 'Simple -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"\" submit 06/21/12 22:51:30 From submit: Submitting job(s). 06/21/12 22:51:30 From submit: 1 job(s) submitted to cluster 62. 06/21/12 22:51:30 assigned Condor ID (62.0.0) 06/21/12 22:51:30 Just submitted 1 job this cycle... 06/21/12 22:51:30 Currently monitoring 1 Condor log file(s) 06/21/12 22:51:30 Event: ULOG_SUBMIT for Condor Node Simple (62.0.0) 06/21/12 22:51:30 Number of idle job procs: 1 06/21/12 22:51:30 Of 1 nodes total: 06/21/12 22:51:30 Done Pre Queued Post Ready Un-Ready Failed 06/21/12 22:51:30 === === === === === === === 06/21/12 22:51:30 0 0 1 0 0 0 0 06/21/12 22:51:30 0 job proc(s) currently held 06/21/12 22:55:05 Currently monitoring 1 Condor log file(s) # Here's where DAGMan noticed that the job is running 06/21/12 22:55:05 Event: ULOG_EXECUTE for Condor Node Simple (62.0.0) 06/21/12 22:55:05 Number of idle job procs: 0 06/21/12 22:55:10 Currently monitoring 1 Condor log file(s) 06/21/12 22:55:10 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0) 06/21/12 22:56:05 Currently monitoring 1 Condor log file(s) 06/21/12 22:56:05 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0) # Here's where DAGMan noticed that the job finished. 06/21/12 22:56:05 Event: ULOG_JOB_TERMINATED for Condor Node Simple (62.0.0) 06/21/12 22:56:05 Node Simple job proc (62.0.0) completed successfully. 06/21/12 22:56:05 Node Simple job completed 06/21/12 22:56:05 Number of idle job procs: 0 06/21/12 22:56:05 Of 1 nodes total: 06/21/12 22:56:05 Done Pre Queued Post Ready Un-Ready Failed 06/21/12 22:56:05 === === === === === === === 06/21/12 22:56:05 1 0 0 0 0 0 0 06/21/12 22:56:05 0 job proc(s) currently held # Here's where DAGMan noticed that all the work is done. 06/21/12 22:56:05 All jobs Completed! 06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/21/12 22:56:05 Note: 0 total job deferrals because of node category throttles 06/21/12 22:56:05 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/21/12 22:56:05 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/21/12 22:56:05 **** condor_scheduniv_exec.61.0 (condor_DAGMAN) pid 5812 EXITING WITH STATUS 0 Now verify your results: $ cat simple.log 000 (062.000.000) 06/21 22:51:30 Job submitted from host: <128.104.100.55:9618?sock=28867_10e4_2> DAG Node: Simple ... 001 (062.000.000) 06/21 22:55:00 Job executing on host: <128.104.58.36:46761> ... 006 (062.000.000) 06/21 22:55:09 Image size of job updated: 750 3 - MemoryUsage of job (MB) 2324 - ResidentSetSize of job (KB) ... 006 (062.000.000) 06/21 22:56:00 Image size of job updated: 780 3 - MemoryUsage of job (MB) 2324 - ResidentSetSize of job (KB) ... 005 (062.000.000) 06/21 22:56:00 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 57 - Run Bytes Sent By Job 608490 - Run Bytes Received By Job 57 - Total Bytes Sent By Job 608490 - Total Bytes Received By Job Partitionable Resources : Usage Request Cpus : 1 Disk (KB) : 750 750 Memory (MB) : 3 3 ... $ cat simple.out Thinking really hard for 4 seconds... We calculated: 20 Looking at DAGMan's various files, we see that DAGMan itself ran as a Condor job (specifically, a scheduler universe job). $ ls simple.dag.* simple.dag.condor.sub simple.dag.dagman.log simple.dag.dagman.out simple.dag.lib.err simple.dag.lib.out $ cat simple.dag.condor.sub # Filename: simple.dag.condor.sub # Generated by condor_submit_dag simple.dag universe = scheduler executable = /usr/bin/condor_dagman getenv = True output = simple.dag.lib.out error = simple.dag.lib.err log = simple.dag.dagman.log remove_kill_sig = SIGUSR1 +OtherJobRemoveRequirements = \"DAGManJobId == $(cluster)\" # Note: default on_exit_remove expression: # ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2)) # attempts to ensure that DAGMan is automatically # requeued by the schedd if it exits abnormally or # is killed (e.g., during a reboot). on_exit_remove = ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2)) copy_to_spool = False arguments = \"-f -l . -Lockfile simple.dag.lock -AutoRescue 1 -DoRescueFrom 0 -Dag simple.dag -CsdVersion $CondorVersion:' '7.7.6' 'Apr' '16' '2012' 'BuildID:' '34175' 'PRE-RELEASE-UWCS' '$ -Force -Dagman /usr/bin/condor_dagman\" environment = _CONDOR_DAGMAN_LOG=simple.dag.dagman.out;_CONDOR_MAX_DAGMAN_LOG=0 queue Clean up some of these files: $ rm simple.dag.*","title":"Submitting a simple DAG"},{"location":"DataSanJose2019/CI/09-SimpleDAG/#on-your-own","text":"Why does DAGman run as a Condor job? Look at the submit file for DAGMan: what does on_exit_remove do? Why is this here?","title":"On your own"},{"location":"DataSanJose2019/CI/10-ComplexDAG/","text":"A More Complex DAG Objective The objective of this exercise is to run a real set of jobs with DAGMan. Make your job submission files We'll run our goatbrot example. If you didn't read about it yet, please do so now . We are going to make a DAG with four simultaneous jobs ( goatbrot ) and one final node to stitch them together ( montage ). This means we have five jobs. We're going to run goatbrot with more iterations (100,000) so it will take longer to run. You can create your five jobs. The goatbrot jobs very similar to each other, but they have slightly different parameters (arguments) and output files. I have placed the goatbrot executable in my public directory: /stash/user/rquick/public/goatbrot-master/goatbrot goatbrot1.sub executable = /stash/user/rquick/public/goatbrot-master/goatbrot arguments = -i 100000 -c -0.75,0.75 -w 1.5 -s 500,500 -o tile_0_0.ppm log = goatbrot.log output = goatbrot.out.0.0 error = goatbrot.err.0.0 requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ONEXIT queue goatbrot2.sub executable = /stash/user/rquick/public/goatbrot-master/goatbrot arguments = -i 100000 -c 0.75,0.75 -w 1.5 -s 500,500 -o tile_0_1.ppm log = goatbrot.log output = goatbrot.out.0.1 error = goatbrot.err.0.1 requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ONEXIT queue goatbrot3.sub executable = /stash/user/rquick/public/goatbrot-master/goatbrot arguments = -i 100000 -c -0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_0.ppm log = goatbrot.log output = goatbrot.out.1.0 error = goatbrot.err.1.0 requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ONEXIT queue goatbrot4.sub executable = /stash/user/rquick/public/goatbrot-master/goatbrot arguments = -i 100000 -c 0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_1.ppm log = goatbrot.log output = goatbrot.out.1.1 error = goatbrot.err.1.1 requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ONEXIT queue montage.sub You should notice a few things about the montage submission file: The transfer_input_files statement refers to the files created by the other jobs. We do not transfer the montage program because it is on OASIS. universe = vanilla executable = wrapper_montage.sh arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.gif should_transfer_files = YES when_to_transfer_output = ONEXIT transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm transfer_executable = true output = montage.out error = montage.err log = montage.log requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue wrapper_montage.sh Because we are using OASIS, we will need to create a wrapper script to load the ImageMagick module so that we can use it to create the montage. Put the following lines into wrapper_montage.sh : source /cvmfs/oasis.opensciencegrid.org/osg/modules/lmod/current/init/bash module load imagemagick montage tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.gif Make your DAG In a file called goatbrot.dag , you have your DAG specification: JOB g1 goatbrot1.sub JOB g2 goatbrot2.sub JOB g3 goatbrot3.sub JOB g4 goatbrot4.sub JOB montage montage.sub PARENT g1 g2 g3 g4 CHILD montage Ask yourself: do you know how we ensure that all the goatbrot commands can run simultaneously and all of them will complete before we run the montage job? Running the DAG Submit your DAG: $ condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 71. ----------------------------------------------------------------------- Watch your DAG Watch with condor_q: $ watch condor_q USER -nobatch Here we see DAGMan running: -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:10 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended DAGMan has submitted the goatbrot jobs, but they haven't started running yet (note that the I status stands for Idle): -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:10 R 0 0.3 condor_dagman 69.0 kagross 8/19 11:38 0+00:00:00 I 0 0.0 goatbrot -i 100000 70.0 kagross 8/19 11:38 0+00:00:00 I 0 0.0 goatbrot -i 100000 71.0 kagross 8/19 11:38 0+00:00:00 I 0 0.0 goatbrot -i 100000 72.0 kagross 8/19 11:38 0+00:00:00 I 0 0.0 goatbrot -i 100000 6 jobs; 0 completed, 0 removed, 4 idle, 2 running, 0 held, 0 suspended They're running! (All four jobs are in state R - running) -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:15 R 0 0.3 condor_dagman 69.0 kagross 8/19 11:38 0+00:00:05 R 0 0.0 goatbrot -i 100000 70.0 kagross 8/19 11:38 0+00:00:05 R 0 0.0 goatbrot -i 100000 71.0 kagross 8/19 11:38 0+00:00:05 R 0 0.0 goatbrot -i 100000 72.0 kagross 8/19 11:38 0+00:00:05 R 0 0.0 goatbrot -i 100000 5 jobs; 0 completed, 0 removed, 0 idle, 5 running, 0 held, 0 suspended Two of the jobs have finished, while the others are still running (remember that completed jobs disappear from condor_q output): -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:20 R 0 0.3 condor_dagman 71.0 kagross 8/19 11:38 0+00:00:10 R 0 0.0 goatbrot -i 100000 72.0 kagross 8/19 11:38 0+00:00:10 R 0 0.0 goatbrot -i 100000 3 jobs; 0 completed, 0 removed, 0 idle, 3 running, 0 held, 0 suspended They finished, but DAGMan hasn't noticed yet. It only checks periodically: -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:30 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended DAGMan submitted and ran the montage job. It ran so fast I didn't capture it running. DAGMan will finish up soon -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:01:01 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Now it's all done: -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Examine your results. For some reason, goatbrot prints everything to stderr, not stdout. $ cat goatbrot.err.0.0 Complex image: Center: -0.75 + 0.75i Width: 1.5 Height: 1.5 Upper Left: -1.5 + 1.5i Lower Right: 0 + 0i Output image: Filename: tile_0_0.ppm Width, Height: 500, 500 Theme: beej Antialiased: no Mandelbrot: Max Iterations: 100000 Continuous: no Goatbrot: Multithreading: not supported in this build Completed: 100.0% Examine your log files ( goatbrot.log and montage.log ) and DAGMan output file ( goatbrot.dag.dagman.out ). Do they look as you expect? Can you see the progress of the DAG in the DAGMan output file? Does your final Mandlebrot image ( mandle.gif ) look correct? To view it we can use Stash. $ cp mandle.gif ~/stash/public/ And now you can go to http://stash.osgconnect.net/~USER . You will see mandle.gif listed. You can click on it to view it. Clean up your results. Be careful about deleting the goatbrot.dag. files, you do not want to delete the goatbrot.dag file, just goatbrot.dag. . $ rm goatbrot.dag.* $ rm goatbrot.out.* $ rm goatbrot.err.* On your own. Re-run your DAG. When jobs are running, try condor_q -dag . What does it do differently? Challenge, if you have time: Make a bigger DAG by making more tiles in the same area.","title":"A More Complex DAG"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#a-more-complex-dag","text":"","title":"A More Complex DAG"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#objective","text":"The objective of this exercise is to run a real set of jobs with DAGMan.","title":"Objective"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#make-your-job-submission-files","text":"We'll run our goatbrot example. If you didn't read about it yet, please do so now . We are going to make a DAG with four simultaneous jobs ( goatbrot ) and one final node to stitch them together ( montage ). This means we have five jobs. We're going to run goatbrot with more iterations (100,000) so it will take longer to run. You can create your five jobs. The goatbrot jobs very similar to each other, but they have slightly different parameters (arguments) and output files. I have placed the goatbrot executable in my public directory: /stash/user/rquick/public/goatbrot-master/goatbrot","title":"Make your job submission files"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#goatbrot1sub","text":"executable = /stash/user/rquick/public/goatbrot-master/goatbrot arguments = -i 100000 -c -0.75,0.75 -w 1.5 -s 500,500 -o tile_0_0.ppm log = goatbrot.log output = goatbrot.out.0.0 error = goatbrot.err.0.0 requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ONEXIT queue","title":"goatbrot1.sub"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#goatbrot2sub","text":"executable = /stash/user/rquick/public/goatbrot-master/goatbrot arguments = -i 100000 -c 0.75,0.75 -w 1.5 -s 500,500 -o tile_0_1.ppm log = goatbrot.log output = goatbrot.out.0.1 error = goatbrot.err.0.1 requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ONEXIT queue","title":"goatbrot2.sub"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#goatbrot3sub","text":"executable = /stash/user/rquick/public/goatbrot-master/goatbrot arguments = -i 100000 -c -0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_0.ppm log = goatbrot.log output = goatbrot.out.1.0 error = goatbrot.err.1.0 requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ONEXIT queue","title":"goatbrot3.sub"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#goatbrot4sub","text":"executable = /stash/user/rquick/public/goatbrot-master/goatbrot arguments = -i 100000 -c 0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_1.ppm log = goatbrot.log output = goatbrot.out.1.1 error = goatbrot.err.1.1 requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ONEXIT queue","title":"goatbrot4.sub"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#montagesub","text":"You should notice a few things about the montage submission file: The transfer_input_files statement refers to the files created by the other jobs. We do not transfer the montage program because it is on OASIS. universe = vanilla executable = wrapper_montage.sh arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.gif should_transfer_files = YES when_to_transfer_output = ONEXIT transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm transfer_executable = true output = montage.out error = montage.err log = montage.log requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue","title":"montage.sub"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#wrapper_montagesh","text":"Because we are using OASIS, we will need to create a wrapper script to load the ImageMagick module so that we can use it to create the montage. Put the following lines into wrapper_montage.sh : source /cvmfs/oasis.opensciencegrid.org/osg/modules/lmod/current/init/bash module load imagemagick montage tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.gif","title":"wrapper_montage.sh"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#make-your-dag","text":"In a file called goatbrot.dag , you have your DAG specification: JOB g1 goatbrot1.sub JOB g2 goatbrot2.sub JOB g3 goatbrot3.sub JOB g4 goatbrot4.sub JOB montage montage.sub PARENT g1 g2 g3 g4 CHILD montage Ask yourself: do you know how we ensure that all the goatbrot commands can run simultaneously and all of them will complete before we run the montage job?","title":"Make your DAG"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#running-the-dag","text":"Submit your DAG: $ condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 71. -----------------------------------------------------------------------","title":"Running the DAG"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#watch-your-dag","text":"Watch with condor_q: $ watch condor_q USER -nobatch Here we see DAGMan running: -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:10 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended DAGMan has submitted the goatbrot jobs, but they haven't started running yet (note that the I status stands for Idle): -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:10 R 0 0.3 condor_dagman 69.0 kagross 8/19 11:38 0+00:00:00 I 0 0.0 goatbrot -i 100000 70.0 kagross 8/19 11:38 0+00:00:00 I 0 0.0 goatbrot -i 100000 71.0 kagross 8/19 11:38 0+00:00:00 I 0 0.0 goatbrot -i 100000 72.0 kagross 8/19 11:38 0+00:00:00 I 0 0.0 goatbrot -i 100000 6 jobs; 0 completed, 0 removed, 4 idle, 2 running, 0 held, 0 suspended They're running! (All four jobs are in state R - running) -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:15 R 0 0.3 condor_dagman 69.0 kagross 8/19 11:38 0+00:00:05 R 0 0.0 goatbrot -i 100000 70.0 kagross 8/19 11:38 0+00:00:05 R 0 0.0 goatbrot -i 100000 71.0 kagross 8/19 11:38 0+00:00:05 R 0 0.0 goatbrot -i 100000 72.0 kagross 8/19 11:38 0+00:00:05 R 0 0.0 goatbrot -i 100000 5 jobs; 0 completed, 0 removed, 0 idle, 5 running, 0 held, 0 suspended Two of the jobs have finished, while the others are still running (remember that completed jobs disappear from condor_q output): -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:20 R 0 0.3 condor_dagman 71.0 kagross 8/19 11:38 0+00:00:10 R 0 0.0 goatbrot -i 100000 72.0 kagross 8/19 11:38 0+00:00:10 R 0 0.0 goatbrot -i 100000 3 jobs; 0 completed, 0 removed, 0 idle, 3 running, 0 held, 0 suspended They finished, but DAGMan hasn't noticed yet. It only checks periodically: -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:00:30 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended DAGMan submitted and ran the montage job. It ran so fast I didn't capture it running. DAGMan will finish up soon -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 68.0 kagross 8/19 11:38 0+00:01:01 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Now it's all done: -- Submitter: kagross@frontal.cci.ucad.sn : <172.16.200.1:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Examine your results. For some reason, goatbrot prints everything to stderr, not stdout. $ cat goatbrot.err.0.0 Complex image: Center: -0.75 + 0.75i Width: 1.5 Height: 1.5 Upper Left: -1.5 + 1.5i Lower Right: 0 + 0i Output image: Filename: tile_0_0.ppm Width, Height: 500, 500 Theme: beej Antialiased: no Mandelbrot: Max Iterations: 100000 Continuous: no Goatbrot: Multithreading: not supported in this build Completed: 100.0% Examine your log files ( goatbrot.log and montage.log ) and DAGMan output file ( goatbrot.dag.dagman.out ). Do they look as you expect? Can you see the progress of the DAG in the DAGMan output file? Does your final Mandlebrot image ( mandle.gif ) look correct? To view it we can use Stash. $ cp mandle.gif ~/stash/public/ And now you can go to http://stash.osgconnect.net/~USER . You will see mandle.gif listed. You can click on it to view it. Clean up your results. Be careful about deleting the goatbrot.dag. files, you do not want to delete the goatbrot.dag file, just goatbrot.dag. . $ rm goatbrot.dag.* $ rm goatbrot.out.* $ rm goatbrot.err.*","title":"Watch your DAG"},{"location":"DataSanJose2019/CI/10-ComplexDAG/#on-your-own","text":"Re-run your DAG. When jobs are running, try condor_q -dag . What does it do differently? Challenge, if you have time: Make a bigger DAG by making more tiles in the same area.","title":"On your own."},{"location":"DataSanJose2019/CI/11-HandlingFailure/","text":"Handling a DAG that fails Objective The objective of this exercise is to help you learn how DAGMan deals with job failures. DAGMan is built to help you recover from such failures. DAGMan can handle a situation where some of the nodes in a DAG fails. DAGMan will run as many nodes as possible, then create a \"rescue DAG\". A rescue DAG allows you to fix the problem and then resume your job where it left off. Recall that DAGMan decides that a jobs fails if its exit code is non-zero. Let's modify our montage job so that it fails. Work in the same directory where you did the last DAG. Edit montage.sub to add a -h to the arguments. It will look like this (the change is bolded): universe = vanilla executable = wrapper_montage.sh arguments = -h tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.jpg should_transfer_files = YES when_to_transfer_output = ONEXIT transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm transfer_executable = true output = montage.out error = montage.err log = goat.log requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue Submit the DAG again: $ condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 77. ----------------------------------------------------------------------- Use watch to watch the jobs until they finish. In a separate window, use tail --lines=500 -f goatbrot.dag.dagman.out to watch what DAGMan does. 06/22/12 17:57:41 Setting maximum accepts per cycle 8. 06/22/12 17:57:41 ****************************************************** 06/22/12 17:57:41 ** condor_scheduniv_exec.77.0 (CONDOR_DAGMAN) STARTING UP 06/22/12 17:57:41 ** /usr/bin/condor_dagman 06/22/12 17:57:41 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/22/12 17:57:41 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/22/12 17:57:41 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/22/12 17:57:41 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/22/12 17:57:41 ** PID = 26867 06/22/12 17:57:41 ** Log last touched time unavailable (No such file or directory) 06/22/12 17:57:41 ****************************************************** 06/22/12 17:57:41 Using config source: /etc/condor/condor_config 06/22/12 17:57:41 Using local config sources: 06/22/12 17:57:41 /etc/condor/config.d/00-chtc-global.conf 06/22/12 17:57:41 /etc/condor/config.d/01-chtc-submit.conf 06/22/12 17:57:41 /etc/condor/config.d/02-chtc-flocking.conf 06/22/12 17:57:41 /etc/condor/config.d/03-chtc-jobrouter.conf 06/22/12 17:57:41 /etc/condor/config.d/04-chtc-blacklist.conf 06/22/12 17:57:41 /etc/condor/config.d/99-osg-ss-group.conf 06/22/12 17:57:41 /etc/condor/config.d/99-roy-extras.conf 06/22/12 17:57:41 /etc/condor/condor_config.local ... output trimmed ... 06/22/12 18:08:42 Event: ULOG_EXECUTE for Condor Node montage (82.0.0) 06/22/12 18:08:42 Number of idle job procs: 0 06/22/12 18:08:42 Event: ULOG_IMAGE_SIZE for Condor Node montage (82.0.0) 06/22/12 18:08:42 Event: ULOG_JOB_TERMINATED for Condor Node montage (82.0.0) 06/22/12 18:08:42 Node montage job proc (82.0.0) failed with status 1. 06/22/12 18:08:42 Number of idle job procs: 0 06/22/12 18:08:42 Of 5 nodes total: 06/22/12 18:08:42 Done Pre Queued Post Ready Un-Ready Failed 06/22/12 18:08:42 === === === === === === === 06/22/12 18:08:42 4 0 0 0 0 0 1 06/22/12 18:08:42 0 job proc(s) currently held 06/22/12 18:08:42 Aborting DAG... 06/22/12 18:08:42 Writing Rescue DAG to goatbrot.dag.rescue001... 06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/22/12 18:08:42 Note: 0 total job deferrals because of node category throttles 06/22/12 18:08:42 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/22/12 18:08:42 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/22/12 18:08:42 **** condor_scheduniv_exec.77.0 (condor_DAGMAN) pid 26867 EXITING WITH STATUS 1 DAGMan notices that one of the jobs failed because it's exit code was non-zero. DAGMan ran as much of the DAG as possible and logged enough information to continue the run when the situation is resolved. Do you see the part where it wrote the resuce DAG? Look at the rescue DAG. It's called a partial DAG: it indicates what part of the DAG has already been completed. When you re-submit the original DAG, DAGMan will notice the rescue DAG and use it in combination with the original DAG. (The rescue DAG used to be the full DAG with nodes marked as done and you would ask DAGMan to run the new rescue DAG. For your simplicity DAGMan lets you resubmit the original DAG and it reads both files.) $ cat goatbrot.dag.rescue001 # Rescue DAG file, created after running # the goatbrot.dag DAG file # Created 6/22/2012 23:08:42 UTC # Rescue DAG version: 2.0.1 (partial) # # Total number of Nodes: 5 # Nodes premarked DONE: 4 # Nodes that failed: 1 # montage,<ENDLIST> DONE g1 DONE g2 DONE g3 DONE g4 From the comment near the top, we know that the montage node failed. Let's fix it by getting rid of the offending -h argument. Change montage.sub to look like: universe = vanilla executable = wrapper_montage.sh arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.jpg should_transfer_files = YES when_to_transfer_output = ONEXIT transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm transfer_executable = true output = montage.out error = montage.err log = goat.log requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue Now we can re-submit our original DAG and DAGMan will pick up where it left off. It will automatically notice the rescue DAG If you didn't fix the problem, DAGMan would generate another rescue DAG. $ condor_submit_dag goatbrot.dag Running rescue DAG 1 ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 83. ----------------------------------------------------------------------- $ tail -f goatbrot.dag.dagman.out 06/23/12 11:30:53 ****************************************************** 06/23/12 11:30:53 ** condor_scheduniv_exec.83.0 (CONDOR_DAGMAN) STARTING UP 06/23/12 11:30:53 ** /usr/bin/condor_dagman 06/23/12 11:30:53 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/23/12 11:30:53 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/23/12 11:30:53 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/23/12 11:30:53 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/23/12 11:30:53 ** PID = 28576 06/23/12 11:30:53 ** Log last touched 6/22 18:08:42 06/23/12 11:30:53 ****************************************************** 06/23/12 11:30:53 Using config source: /etc/condor/condor_config ... Here is where DAGMAN notices that there is a rescue DAG: 06/23/12 11:30:53 Parsing 1 dagfiles 06/23/12 11:30:53 Parsing goatbrot.dag ... 06/23/12 11:30:53 Found rescue DAG number 1; running goatbrot.dag.rescue001 in combination with normal DAG file 06/23/12 11:30:53 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 06/23/12 11:30:53 USING RESCUE DAG goatbrot.dag.rescue001 06/23/12 11:30:53 Dag contains 5 total jobs Shortly thereafter it sees that four jobs have already finished: 06/23/12 11:31:05 Bootstrapping... 06/23/12 11:31:05 Number of pre-completed nodes: 4 06/23/12 11:31:05 Registering condor_event_timer... 06/23/12 11:31:06 Sleeping for one second for log file consistency 06/23/12 11:31:07 MultiLogFiles: truncating log file /home/roy/condor/goatbrot/montage.log Here is where DAGMan resubmits the montage job and waits for it to complete: 06/23/12 11:31:07 Submitting Condor Node montage job(s)... 06/23/12 11:31:07 submitting: condor_submit -a dag_node_name' '=' 'montage -a +DAGManJobId' '=' '83 -a DAGManJobId' '=' '83 -a submit_event_notes' '=' 'DAG' 'Node:' 'montage -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"g1,g2,g3,g4\" montage.sub 06/23/12 11:31:07 From submit: Submitting job(s). 06/23/12 11:31:07 From submit: 1 job(s) submitted to cluster 84. 06/23/12 11:31:07 assigned Condor ID (84.0.0) 06/23/12 11:31:07 Just submitted 1 job this cycle... 06/23/12 11:31:07 Currently monitoring 1 Condor log file(s) 06/23/12 11:31:07 Event: ULOG_SUBMIT for Condor Node montage (84.0.0) 06/23/12 11:31:07 Number of idle job procs: 1 06/23/12 11:31:07 Of 5 nodes total: 06/23/12 11:31:07 Done Pre Queued Post Ready Un-Ready Failed 06/23/12 11:31:07 === === === === === === === 06/23/12 11:31:07 4 0 1 0 0 0 0 06/23/12 11:31:07 0 job proc(s) currently held 06/23/12 11:40:22 Currently monitoring 1 Condor log file(s) 06/23/12 11:40:22 Event: ULOG_EXECUTE for Condor Node montage (84.0.0) 06/23/12 11:40:22 Number of idle job procs: 0 06/23/12 11:40:22 Event: ULOG_IMAGE_SIZE for Condor Node montage (84.0.0) 06/23/12 11:40:22 Event: ULOG_JOB_TERMINATED for Condor Node montage (84.0.0) This is where the montage finished: 06/23/12 11:40:22 Node montage job proc (84.0.0) completed successfully. 06/23/12 11:40:22 Node montage job completed 06/23/12 11:40:22 Number of idle job procs: 0 06/23/12 11:40:22 Of 5 nodes total: 06/23/12 11:40:22 Done Pre Queued Post Ready Un-Ready Failed 06/23/12 11:40:22 === === === === === === === 06/23/12 11:40:22 5 0 0 0 0 0 0 06/23/12 11:40:22 0 job proc(s) currently held And here DAGMan decides that the work is all done: 06/23/12 11:40:22 All jobs Completed! 06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/23/12 11:40:22 Note: 0 total job deferrals because of node category throttles 06/23/12 11:40:22 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/23/12 11:40:22 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/23/12 11:40:22 **** condor_scheduniv_exec.83.0 (condor_DAGMAN) pid 28576 EXITING WITH STATUS 0</pre> Success! Now go ahead and clean up. Challenge If you have time, add an extra node to the DAG. Copy our original simple program, but make it exit with a 1 instead of a 0. DAGMan would consider this a failure, but you'll tell DAGMan that it's really a success. This is reasonable--many real world programs use a variety of return codes, and you might need to help DAGMan distinguish success from failure. Write a POST script that checks the return value. Check the Condor manual to see how to describe your post script.","title":"Handling a DAG that fails"},{"location":"DataSanJose2019/CI/11-HandlingFailure/#handling-a-dag-that-fails","text":"","title":"Handling a DAG that fails"},{"location":"DataSanJose2019/CI/11-HandlingFailure/#objective","text":"The objective of this exercise is to help you learn how DAGMan deals with job failures. DAGMan is built to help you recover from such failures. DAGMan can handle a situation where some of the nodes in a DAG fails. DAGMan will run as many nodes as possible, then create a \"rescue DAG\". A rescue DAG allows you to fix the problem and then resume your job where it left off. Recall that DAGMan decides that a jobs fails if its exit code is non-zero. Let's modify our montage job so that it fails. Work in the same directory where you did the last DAG. Edit montage.sub to add a -h to the arguments. It will look like this (the change is bolded): universe = vanilla executable = wrapper_montage.sh arguments = -h tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.jpg should_transfer_files = YES when_to_transfer_output = ONEXIT transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm transfer_executable = true output = montage.out error = montage.err log = goat.log requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue Submit the DAG again: $ condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 77. ----------------------------------------------------------------------- Use watch to watch the jobs until they finish. In a separate window, use tail --lines=500 -f goatbrot.dag.dagman.out to watch what DAGMan does. 06/22/12 17:57:41 Setting maximum accepts per cycle 8. 06/22/12 17:57:41 ****************************************************** 06/22/12 17:57:41 ** condor_scheduniv_exec.77.0 (CONDOR_DAGMAN) STARTING UP 06/22/12 17:57:41 ** /usr/bin/condor_dagman 06/22/12 17:57:41 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/22/12 17:57:41 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/22/12 17:57:41 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/22/12 17:57:41 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/22/12 17:57:41 ** PID = 26867 06/22/12 17:57:41 ** Log last touched time unavailable (No such file or directory) 06/22/12 17:57:41 ****************************************************** 06/22/12 17:57:41 Using config source: /etc/condor/condor_config 06/22/12 17:57:41 Using local config sources: 06/22/12 17:57:41 /etc/condor/config.d/00-chtc-global.conf 06/22/12 17:57:41 /etc/condor/config.d/01-chtc-submit.conf 06/22/12 17:57:41 /etc/condor/config.d/02-chtc-flocking.conf 06/22/12 17:57:41 /etc/condor/config.d/03-chtc-jobrouter.conf 06/22/12 17:57:41 /etc/condor/config.d/04-chtc-blacklist.conf 06/22/12 17:57:41 /etc/condor/config.d/99-osg-ss-group.conf 06/22/12 17:57:41 /etc/condor/config.d/99-roy-extras.conf 06/22/12 17:57:41 /etc/condor/condor_config.local ... output trimmed ... 06/22/12 18:08:42 Event: ULOG_EXECUTE for Condor Node montage (82.0.0) 06/22/12 18:08:42 Number of idle job procs: 0 06/22/12 18:08:42 Event: ULOG_IMAGE_SIZE for Condor Node montage (82.0.0) 06/22/12 18:08:42 Event: ULOG_JOB_TERMINATED for Condor Node montage (82.0.0) 06/22/12 18:08:42 Node montage job proc (82.0.0) failed with status 1. 06/22/12 18:08:42 Number of idle job procs: 0 06/22/12 18:08:42 Of 5 nodes total: 06/22/12 18:08:42 Done Pre Queued Post Ready Un-Ready Failed 06/22/12 18:08:42 === === === === === === === 06/22/12 18:08:42 4 0 0 0 0 0 1 06/22/12 18:08:42 0 job proc(s) currently held 06/22/12 18:08:42 Aborting DAG... 06/22/12 18:08:42 Writing Rescue DAG to goatbrot.dag.rescue001... 06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/22/12 18:08:42 Note: 0 total job deferrals because of node category throttles 06/22/12 18:08:42 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/22/12 18:08:42 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/22/12 18:08:42 **** condor_scheduniv_exec.77.0 (condor_DAGMAN) pid 26867 EXITING WITH STATUS 1 DAGMan notices that one of the jobs failed because it's exit code was non-zero. DAGMan ran as much of the DAG as possible and logged enough information to continue the run when the situation is resolved. Do you see the part where it wrote the resuce DAG? Look at the rescue DAG. It's called a partial DAG: it indicates what part of the DAG has already been completed. When you re-submit the original DAG, DAGMan will notice the rescue DAG and use it in combination with the original DAG. (The rescue DAG used to be the full DAG with nodes marked as done and you would ask DAGMan to run the new rescue DAG. For your simplicity DAGMan lets you resubmit the original DAG and it reads both files.) $ cat goatbrot.dag.rescue001 # Rescue DAG file, created after running # the goatbrot.dag DAG file # Created 6/22/2012 23:08:42 UTC # Rescue DAG version: 2.0.1 (partial) # # Total number of Nodes: 5 # Nodes premarked DONE: 4 # Nodes that failed: 1 # montage,<ENDLIST> DONE g1 DONE g2 DONE g3 DONE g4 From the comment near the top, we know that the montage node failed. Let's fix it by getting rid of the offending -h argument. Change montage.sub to look like: universe = vanilla executable = wrapper_montage.sh arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.jpg should_transfer_files = YES when_to_transfer_output = ONEXIT transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm transfer_executable = true output = montage.out error = montage.err log = goat.log requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue Now we can re-submit our original DAG and DAGMan will pick up where it left off. It will automatically notice the rescue DAG If you didn't fix the problem, DAGMan would generate another rescue DAG. $ condor_submit_dag goatbrot.dag Running rescue DAG 1 ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 83. ----------------------------------------------------------------------- $ tail -f goatbrot.dag.dagman.out 06/23/12 11:30:53 ****************************************************** 06/23/12 11:30:53 ** condor_scheduniv_exec.83.0 (CONDOR_DAGMAN) STARTING UP 06/23/12 11:30:53 ** /usr/bin/condor_dagman 06/23/12 11:30:53 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/23/12 11:30:53 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/23/12 11:30:53 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/23/12 11:30:53 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/23/12 11:30:53 ** PID = 28576 06/23/12 11:30:53 ** Log last touched 6/22 18:08:42 06/23/12 11:30:53 ****************************************************** 06/23/12 11:30:53 Using config source: /etc/condor/condor_config ... Here is where DAGMAN notices that there is a rescue DAG: 06/23/12 11:30:53 Parsing 1 dagfiles 06/23/12 11:30:53 Parsing goatbrot.dag ... 06/23/12 11:30:53 Found rescue DAG number 1; running goatbrot.dag.rescue001 in combination with normal DAG file 06/23/12 11:30:53 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 06/23/12 11:30:53 USING RESCUE DAG goatbrot.dag.rescue001 06/23/12 11:30:53 Dag contains 5 total jobs Shortly thereafter it sees that four jobs have already finished: 06/23/12 11:31:05 Bootstrapping... 06/23/12 11:31:05 Number of pre-completed nodes: 4 06/23/12 11:31:05 Registering condor_event_timer... 06/23/12 11:31:06 Sleeping for one second for log file consistency 06/23/12 11:31:07 MultiLogFiles: truncating log file /home/roy/condor/goatbrot/montage.log Here is where DAGMan resubmits the montage job and waits for it to complete: 06/23/12 11:31:07 Submitting Condor Node montage job(s)... 06/23/12 11:31:07 submitting: condor_submit -a dag_node_name' '=' 'montage -a +DAGManJobId' '=' '83 -a DAGManJobId' '=' '83 -a submit_event_notes' '=' 'DAG' 'Node:' 'montage -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"g1,g2,g3,g4\" montage.sub 06/23/12 11:31:07 From submit: Submitting job(s). 06/23/12 11:31:07 From submit: 1 job(s) submitted to cluster 84. 06/23/12 11:31:07 assigned Condor ID (84.0.0) 06/23/12 11:31:07 Just submitted 1 job this cycle... 06/23/12 11:31:07 Currently monitoring 1 Condor log file(s) 06/23/12 11:31:07 Event: ULOG_SUBMIT for Condor Node montage (84.0.0) 06/23/12 11:31:07 Number of idle job procs: 1 06/23/12 11:31:07 Of 5 nodes total: 06/23/12 11:31:07 Done Pre Queued Post Ready Un-Ready Failed 06/23/12 11:31:07 === === === === === === === 06/23/12 11:31:07 4 0 1 0 0 0 0 06/23/12 11:31:07 0 job proc(s) currently held 06/23/12 11:40:22 Currently monitoring 1 Condor log file(s) 06/23/12 11:40:22 Event: ULOG_EXECUTE for Condor Node montage (84.0.0) 06/23/12 11:40:22 Number of idle job procs: 0 06/23/12 11:40:22 Event: ULOG_IMAGE_SIZE for Condor Node montage (84.0.0) 06/23/12 11:40:22 Event: ULOG_JOB_TERMINATED for Condor Node montage (84.0.0) This is where the montage finished: 06/23/12 11:40:22 Node montage job proc (84.0.0) completed successfully. 06/23/12 11:40:22 Node montage job completed 06/23/12 11:40:22 Number of idle job procs: 0 06/23/12 11:40:22 Of 5 nodes total: 06/23/12 11:40:22 Done Pre Queued Post Ready Un-Ready Failed 06/23/12 11:40:22 === === === === === === === 06/23/12 11:40:22 5 0 0 0 0 0 0 06/23/12 11:40:22 0 job proc(s) currently held And here DAGMan decides that the work is all done: 06/23/12 11:40:22 All jobs Completed! 06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/23/12 11:40:22 Note: 0 total job deferrals because of node category throttles 06/23/12 11:40:22 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/23/12 11:40:22 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/23/12 11:40:22 **** condor_scheduniv_exec.83.0 (condor_DAGMAN) pid 28576 EXITING WITH STATUS 0</pre> Success! Now go ahead and clean up.","title":"Objective"},{"location":"DataSanJose2019/CI/11-HandlingFailure/#challenge","text":"If you have time, add an extra node to the DAG. Copy our original simple program, but make it exit with a 1 instead of a 0. DAGMan would consider this a failure, but you'll tell DAGMan that it's really a success. This is reasonable--many real world programs use a variety of return codes, and you might need to help DAGMan distinguish success from failure. Write a POST script that checks the return value. Check the Condor manual to see how to describe your post script.","title":"Challenge"},{"location":"DataSanJose2019/CI/12-VariableSubstitution/","text":"Simpler DAGs with variable substitutions Objective The objective of this exercise is to help you write simpler DAGs by using variable substitutions in your submit files. If you look at the DAG we made, you might find it a bit tedious because each goatbrot job has a separate Condor submit file. They're nearly identical except for a couple of parameters. Can we make it simpler? Yes, we can! Declare your variables First you need to declare your variables in your submit file. Make one submit file for all of your goatbrot jobs. Here's what it looks like. Call it goatbrot.sub : executable = /tmp/goatbrot-master/goatbrot arguments = -i 100000 -c $(CENTERX),$(CENTERY) -w 1.5 -s 500,500 -o tile_$(TILEY)_$(TILEX).ppm log = goatbrot.log output = goatbrot.out.$(TILEY).$(TILEX) error = goatbrot.err.$(TILEY).$(TILEX) should_transfer_files = YES when_to_transfer_output = ONEXIT requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue Then you need to change your DAG to use VARS for variable substitution. Here's what one of the jobs would look like: JOB g1 goatbrot.sub VARS g1 CENTERX=\"-0.75\" VARS g1 CENTERY=\"0.75\" VARS g1 TILEX=\"0\" VARS g1 TILEY=\"0\" Edit your DAG similarly for all of your goatbrot jobs. If you need help, check the Condor manual for for a description of how to use VARS . What happens?","title":"Simpler DAGs with variable substitutions"},{"location":"DataSanJose2019/CI/12-VariableSubstitution/#simpler-dags-with-variable-substitutions","text":"","title":"Simpler DAGs with variable substitutions"},{"location":"DataSanJose2019/CI/12-VariableSubstitution/#objective","text":"The objective of this exercise is to help you write simpler DAGs by using variable substitutions in your submit files. If you look at the DAG we made, you might find it a bit tedious because each goatbrot job has a separate Condor submit file. They're nearly identical except for a couple of parameters. Can we make it simpler? Yes, we can!","title":"Objective"},{"location":"DataSanJose2019/CI/12-VariableSubstitution/#declare-your-variables","text":"First you need to declare your variables in your submit file. Make one submit file for all of your goatbrot jobs. Here's what it looks like. Call it goatbrot.sub : executable = /tmp/goatbrot-master/goatbrot arguments = -i 100000 -c $(CENTERX),$(CENTERY) -w 1.5 -s 500,500 -o tile_$(TILEY)_$(TILEX).ppm log = goatbrot.log output = goatbrot.out.$(TILEY).$(TILEX) error = goatbrot.err.$(TILEY).$(TILEX) should_transfer_files = YES when_to_transfer_output = ONEXIT requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") queue Then you need to change your DAG to use VARS for variable substitution. Here's what one of the jobs would look like: JOB g1 goatbrot.sub VARS g1 CENTERX=\"-0.75\" VARS g1 CENTERY=\"0.75\" VARS g1 TILEX=\"0\" VARS g1 TILEY=\"0\" Edit your DAG similarly for all of your goatbrot jobs. If you need help, check the Condor manual for for a description of how to use VARS . What happens?","title":"Declare your variables"},{"location":"DataSanJose2019/CI/13-DisciplineTutorials/","text":"Follow your Interest Exercises During this portion of the exercise you'll be able to find a tutorial that fits your interest and play with submission on OSG Connect. You can see the available tutorials by logging on to login.osgconnect.net and running the following command: $ tutorial list Some suggestions to get you started: The details of the tutorials will be in the README.md files with each tutorial. Bioinformatics Tutorials Molecule Docking $ tutorial AutoDockVina Genetic Sequence Analysis $ tutorial blast Statistical Tutorials Use R to calculate Pi $ tutorial R Matlab $ tutorial matlab-HelloWorld Molecular Dynamics Tutorials NAMD Simulations $ tutorial namd GROMACS $ tutorial gromacs High Energy Physics Tutorials Calculate ntuples with root $ tutorial root Programming Tutorials Python Virtual Environment $ tutorial python-virtualenv SWIFT Parallel Programming $ tutorial swift Advanced HTC Concepts Pegasus Workflows $ tutorial pegasus Scaling on the Open Science Grid Pegasus Workflows $ tutorial scaling Feel free to explore the OSG Connect Tutorials on your own.","title":"Follow your Interest Exercises"},{"location":"DataSanJose2019/CI/13-DisciplineTutorials/#follow-your-interest-exercises","text":"During this portion of the exercise you'll be able to find a tutorial that fits your interest and play with submission on OSG Connect. You can see the available tutorials by logging on to login.osgconnect.net and running the following command: $ tutorial list Some suggestions to get you started: The details of the tutorials will be in the README.md files with each tutorial.","title":"Follow your Interest Exercises"},{"location":"DataSanJose2019/CI/13-DisciplineTutorials/#bioinformatics-tutorials","text":"Molecule Docking $ tutorial AutoDockVina Genetic Sequence Analysis $ tutorial blast","title":"Bioinformatics Tutorials"},{"location":"DataSanJose2019/CI/13-DisciplineTutorials/#statistical-tutorials","text":"Use R to calculate Pi $ tutorial R Matlab $ tutorial matlab-HelloWorld","title":"Statistical Tutorials"},{"location":"DataSanJose2019/CI/13-DisciplineTutorials/#molecular-dynamics-tutorials","text":"NAMD Simulations $ tutorial namd GROMACS $ tutorial gromacs","title":"Molecular Dynamics Tutorials"},{"location":"DataSanJose2019/CI/13-DisciplineTutorials/#high-energy-physics-tutorials","text":"Calculate ntuples with root $ tutorial root","title":"High Energy Physics Tutorials"},{"location":"DataSanJose2019/CI/13-DisciplineTutorials/#programming-tutorials","text":"Python Virtual Environment $ tutorial python-virtualenv SWIFT Parallel Programming $ tutorial swift","title":"Programming Tutorials"},{"location":"DataSanJose2019/CI/13-DisciplineTutorials/#advanced-htc-concepts","text":"Pegasus Workflows $ tutorial pegasus Scaling on the Open Science Grid Pegasus Workflows $ tutorial scaling Feel free to explore the OSG Connect Tutorials on your own.","title":"Advanced HTC Concepts"},{"location":"DataSanJose2019/CI/14-Containers/","text":"Singularity Containers in OSG Objective Singularity is a container system to allow users full control over their enviroment. You can create your own container image which your job will execute within, or choose from a set of pre-defined images. For more information about Singularity, please see: Singularity Home Page The following talk describes Singularity for scientific computing: Singularity Talk Derek Weitzel wrote a blog post about Singularity on OSG, which provides a good introduction on how to create images and run them, but does not cover all the functionality described further down: Singularity on the OSG Default Image The default setup is to auto load an image on sites which support Singularity. Every job which lands on such a site, will have a container started just for that job, and then run within that container. Most users will not even know that their jobs are run within a container, but it will provide them with a consistent environment across OSG sites. The current default container is based on EL6 and contains a basic set of tools expected from OSG compute nodes. The image is loaded from /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el6 and the definition file is available in GitHub https://github.com/opensciencegrid/osgvo-el6 . If you want to steer a job to run on a default Singularity instance, use HAS_SINGULARITY == True in the job requirements. For example: universe = vanilla executable = job.sh requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") && HAS_SINGULARITY == TRUE should_transfer_files = IF_NEEDED when_to_transfer_output = ON_EXIT output = out error = err log = log queue To instruct the system to load a different image, use the +SingularityImage attribute in your job submit file. For example, to run your job under EL7: universe = vanilla executable = job.sh requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") && HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7\" +SingularityBindCVMFS = True should_transfer_files = IF_NEEDED when_to_transfer_output = ON_EXIT output = out error = err log = log queue The user support team maintains a set of images. These contain a basic set of tools and libraries. The images are are: Image Location Defintion Description EL 6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el6:latest GitHub A basic Enterprise Linux (CentOS) 6 based image. This is currently our default image EL 7 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest GitHub A basic Enterprise Linux (CentOS) 7 based image. Ubuntu Xenial /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-xenial:latest GitHub A good image if you prefer Ubuntu over EL flavors TensorFlow /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:latest GitHub Base on the TensorFlow base image, with a few OSG package added TensorFlow GPU /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:latest GitHub Used for running TensorFlow jobs on OSG GPU resources Exloring Images on the Submit Host Images can be explored interactively on the submit hosts by starting it in \"shell\" mode. The recommended command line, similar to how containers are started for jobs, is: singularity shell \\ --home $PWD:/srv \\ --pwd /srv \\ --bind /cvmfs \\ --scratch /var/tmp \\ --scratch /tmp \\ --contain --ipc --pid \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-xenial:latest Custom Images OSG Connect provides tooling for users to create, publish and load custom images. This is useful if your job requires some very specific software setup. Creating a Custom Image If you want to use an image you have created yourself, the image should be defined as a Docker image and published in the Docker Hub . The reason we use Docker as a source image repository is that it allows us to easily import the images into our own distribution system (see below). To get started, create a Docker user, sign in to the hub, and create a new repository. You will end up with an identifier of the namespace/repository_name format. Create an image locally using a Dockerfile and the docker build . We suggest you base the image on one of the provided OSG images. For example, if you want to base the image on our Ubuntu Xenial image, first download the Dockerfile from the GitHub repository . Edit the Dockerfile to fit your requirements. Then build the image with tag matching your Docker Hub repository: docker build -t namespace/repository_name . Once you have a successful build, push it to the hub: docker push namespace/repository_name Then register the image as described in the next section. If you prefer, you can base you image on images not already published by OSG, but if you do this, we recommend that you as one of the steps create the /cvmfs directory. This will enable the container to access tools and data published on /cvmfs. In your Dockerfile , add: # required directories RUN mkdir -p /cvmfs See one of the provided image defintions for a full example. If you do not want /cvmfs mounted in the container, please add +SingularityBindCVMFS = False to your job submit file. Distributing Custom Images Via CVMFS In order to be able to efficiently distribute the container images to a large of distributed compute hosts, OSG has choosen to host the images under CVMFS . Any image publically available in Docker can be included for automatic syncing into the CVMFS repository. The result is an unpacked image under /cvmfs/singularity.opensciencegrid.org/ To get your images included, please either create a git pull request against docker_images.txt in the cvmfs-singularity-sync repository , or contact user-support@opensciencegrid.org and we can help you. Once your image has been registered, new versions pushed to Docker Hub will automatically be detected and CVMFS will be updated accordingly. Source Paged sourced from https://support.opensciencegrid.org/support/solutions/articles/12000024676-singularity-containers.","title":"Singularity Containers in OSG"},{"location":"DataSanJose2019/CI/14-Containers/#singularity-containers-in-osg","text":"","title":"Singularity Containers in OSG"},{"location":"DataSanJose2019/CI/14-Containers/#objective","text":"Singularity is a container system to allow users full control over their enviroment. You can create your own container image which your job will execute within, or choose from a set of pre-defined images. For more information about Singularity, please see: Singularity Home Page The following talk describes Singularity for scientific computing: Singularity Talk Derek Weitzel wrote a blog post about Singularity on OSG, which provides a good introduction on how to create images and run them, but does not cover all the functionality described further down: Singularity on the OSG","title":"Objective"},{"location":"DataSanJose2019/CI/14-Containers/#default-image","text":"The default setup is to auto load an image on sites which support Singularity. Every job which lands on such a site, will have a container started just for that job, and then run within that container. Most users will not even know that their jobs are run within a container, but it will provide them with a consistent environment across OSG sites. The current default container is based on EL6 and contains a basic set of tools expected from OSG compute nodes. The image is loaded from /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el6 and the definition file is available in GitHub https://github.com/opensciencegrid/osgvo-el6 . If you want to steer a job to run on a default Singularity instance, use HAS_SINGULARITY == True in the job requirements. For example: universe = vanilla executable = job.sh requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") && HAS_SINGULARITY == TRUE should_transfer_files = IF_NEEDED when_to_transfer_output = ON_EXIT output = out error = err log = log queue To instruct the system to load a different image, use the +SingularityImage attribute in your job submit file. For example, to run your job under EL7: universe = vanilla executable = job.sh requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") && HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7\" +SingularityBindCVMFS = True should_transfer_files = IF_NEEDED when_to_transfer_output = ON_EXIT output = out error = err log = log queue The user support team maintains a set of images. These contain a basic set of tools and libraries. The images are are: Image Location Defintion Description EL 6 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el6:latest GitHub A basic Enterprise Linux (CentOS) 6 based image. This is currently our default image EL 7 /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-el7:latest GitHub A basic Enterprise Linux (CentOS) 7 based image. Ubuntu Xenial /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-xenial:latest GitHub A good image if you prefer Ubuntu over EL flavors TensorFlow /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow:latest GitHub Base on the TensorFlow base image, with a few OSG package added TensorFlow GPU /cvmfs/singularity.opensciencegrid.org/opensciencegrid/tensorflow-gpu:latest GitHub Used for running TensorFlow jobs on OSG GPU resources","title":"Default Image"},{"location":"DataSanJose2019/CI/14-Containers/#exloring-images-on-the-submit-host","text":"Images can be explored interactively on the submit hosts by starting it in \"shell\" mode. The recommended command line, similar to how containers are started for jobs, is: singularity shell \\ --home $PWD:/srv \\ --pwd /srv \\ --bind /cvmfs \\ --scratch /var/tmp \\ --scratch /tmp \\ --contain --ipc --pid \\ /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo-ubuntu-xenial:latest","title":"Exloring Images on the Submit Host"},{"location":"DataSanJose2019/CI/14-Containers/#custom-images","text":"OSG Connect provides tooling for users to create, publish and load custom images. This is useful if your job requires some very specific software setup.","title":"Custom Images"},{"location":"DataSanJose2019/CI/14-Containers/#creating-a-custom-image","text":"If you want to use an image you have created yourself, the image should be defined as a Docker image and published in the Docker Hub . The reason we use Docker as a source image repository is that it allows us to easily import the images into our own distribution system (see below). To get started, create a Docker user, sign in to the hub, and create a new repository. You will end up with an identifier of the namespace/repository_name format. Create an image locally using a Dockerfile and the docker build . We suggest you base the image on one of the provided OSG images. For example, if you want to base the image on our Ubuntu Xenial image, first download the Dockerfile from the GitHub repository . Edit the Dockerfile to fit your requirements. Then build the image with tag matching your Docker Hub repository: docker build -t namespace/repository_name . Once you have a successful build, push it to the hub: docker push namespace/repository_name Then register the image as described in the next section. If you prefer, you can base you image on images not already published by OSG, but if you do this, we recommend that you as one of the steps create the /cvmfs directory. This will enable the container to access tools and data published on /cvmfs. In your Dockerfile , add: # required directories RUN mkdir -p /cvmfs See one of the provided image defintions for a full example. If you do not want /cvmfs mounted in the container, please add +SingularityBindCVMFS = False to your job submit file.","title":"Creating a Custom Image"},{"location":"DataSanJose2019/CI/14-Containers/#distributing-custom-images-via-cvmfs","text":"In order to be able to efficiently distribute the container images to a large of distributed compute hosts, OSG has choosen to host the images under CVMFS . Any image publically available in Docker can be included for automatic syncing into the CVMFS repository. The result is an unpacked image under /cvmfs/singularity.opensciencegrid.org/ To get your images included, please either create a git pull request against docker_images.txt in the cvmfs-singularity-sync repository , or contact user-support@opensciencegrid.org and we can help you. Once your image has been registered, new versions pushed to Docker Hub will automatically be detected and CVMFS will be updated accordingly.","title":"Distributing Custom Images Via CVMFS"},{"location":"DataSanJose2019/CI/14-Containers/#source","text":"Paged sourced from https://support.opensciencegrid.org/support/solutions/articles/12000024676-singularity-containers.","title":"Source"},{"location":"DataSanJose2019/CI/Materials/","text":"Data San Jos\u00e9 School Materials We will be using OSG Connect for this set of sessions. Please visit http://www.osgconnect.net for more information. Wednesday Afternoon - Security and Setup - Session 0 Security Lecture - Intro to Security Authentication with Keys Thursday Morning - Computational Infrastructures - Session 1 Welcome and Introduction - The Landscape of Academic Research Computing Lecture 1 - Slides Introduction Our Job Manager (HTCondor) Your First Managed Job A few tips and tricks Thursday Morning - Computational Infrastructures - Session 2 Lecture 2 - Slides A scripting job An R Job Working with file I/O Thursday Afternoon - Computational Infrastructures - Session 3 Lecture 3 - Slides A brief detour through the Mandlebrot set Coordinating sets of jobs: A simple DAG A more complex DAG Challenge Exercises on DAGs Handling jobs that fail Variable Substitution Thursday Aftenoon - Computational Infrastructures - Session 4 Lecture 4 - Slides Discipline specific tutorials Containers with Singularity Bonus Lecture - Digital Object Architectures DOA and RPID Friday Morning - Computational Infrastructures - Session 5 Introduction to Cloud Computing Friday Morning - Computational Infrastructures - Session 6 Close Out - What to do next? Contact information Rob Quick - rquick@iu.edu http://www.osgconnect.net/ DOSAR: Distributed Organization for Scientific and Academic Research. If you want long\u2010term OSG access, you can go to http://www.osgconnect.net and sign up. Mention you attended Data Trieste 2019 and want to be added to the DOSAR Project.","title":"Data San Jos\u00e9 School Materials"},{"location":"DataSanJose2019/CI/Materials/#data-san-jose-school-materials","text":"We will be using OSG Connect for this set of sessions. Please visit http://www.osgconnect.net for more information.","title":"Data San Jos\u00e9 School Materials"},{"location":"DataSanJose2019/CI/Materials/#wednesday-afternoon-security-and-setup-session-0","text":"Security Lecture - Intro to Security Authentication with Keys","title":"Wednesday Afternoon - Security and Setup - Session 0"},{"location":"DataSanJose2019/CI/Materials/#thursday-morning-computational-infrastructures-session-1","text":"Welcome and Introduction - The Landscape of Academic Research Computing Lecture 1 - Slides Introduction Our Job Manager (HTCondor) Your First Managed Job A few tips and tricks","title":"Thursday Morning - Computational Infrastructures - Session 1"},{"location":"DataSanJose2019/CI/Materials/#thursday-morning-computational-infrastructures-session-2","text":"Lecture 2 - Slides A scripting job An R Job Working with file I/O","title":"Thursday Morning - Computational Infrastructures - Session 2"},{"location":"DataSanJose2019/CI/Materials/#thursday-afternoon-computational-infrastructures-session-3","text":"Lecture 3 - Slides A brief detour through the Mandlebrot set Coordinating sets of jobs: A simple DAG A more complex DAG","title":"Thursday Afternoon - Computational Infrastructures - Session 3"},{"location":"DataSanJose2019/CI/Materials/#challenge-exercises-on-dags","text":"Handling jobs that fail Variable Substitution","title":"Challenge Exercises on DAGs"},{"location":"DataSanJose2019/CI/Materials/#thursday-aftenoon-computational-infrastructures-session-4","text":"Lecture 4 - Slides Discipline specific tutorials Containers with Singularity","title":"Thursday Aftenoon - Computational Infrastructures - Session 4"},{"location":"DataSanJose2019/CI/Materials/#bonus-lecture-digital-object-architectures","text":"DOA and RPID","title":"Bonus Lecture - Digital Object Architectures"},{"location":"DataSanJose2019/CI/Materials/#friday-morning-computational-infrastructures-session-5","text":"Introduction to Cloud Computing","title":"Friday Morning - Computational Infrastructures - Session 5"},{"location":"DataSanJose2019/CI/Materials/#friday-morning-computational-infrastructures-session-6","text":"Close Out - What to do next?","title":"Friday Morning - Computational Infrastructures - Session 6"},{"location":"DataSanJose2019/CI/Materials/#contact-information","text":"Rob Quick - rquick@iu.edu http://www.osgconnect.net/ DOSAR: Distributed Organization for Scientific and Academic Research. If you want long\u2010term OSG access, you can go to http://www.osgconnect.net and sign up. Mention you attended Data Trieste 2019 and want to be added to the DOSAR Project.","title":"Contact information"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/","text":"Session 1 \u00b6 {#Session-1} Plotting and programming with Python 3 \u00b6 {#Plotting-and-programming-with-Python-3} Open a Jupyter Notebook \u00b6 {#Open-a-Jupyter-Notebook} (10 minutes) http://swcarpentry.github.io/python-novice-gapminder/01-run-quit/index.html Once you have your Notebook open let`s start with the session. Heo Variables and Assignment \u00b6 {#Variables-and-Assignment} (15 min) Exercises (10min) Variables are names for determined values. Note: To assing the value to a variable use = like this: variable = value The variable is created when a value is assingned. For example let's type in our names and ages: In [2]: age = 21 name = 'Mariana' ejemplo= 5+5 print(age, name, ejemplo) 21 Mariana 10 In [4]: #1variable = 1 variable1 = 1 Variable names \u00b6 {#Variable-names} Only letters, digits and underscore _ (for long names) Do not start with a digit It is possible to start with an underscore like __real_age , but this has a special meaning. (We won't do that yet) Display values with: \u00b6 {#Display-values-with:} print() This built-in function prints values as text Examples: In [5]: print('My name is', name, ', I am ', age) My name is Mariana , I am 21 Please remember that variables must be created before they are used {#Please-remember-that-variables-must-be-created-before-they-are-used!} Otherwise Python reports an error In [8]: cat_name = \"Salem\" print(cat_name) Salem Be aware that it is the order of execution of cells that is important in a Jupyter notebook, not the order in which they appear . Python will remember all the code that was run previously, including any variables you have defined, irrespective of the order in the notebook. Therefore if you define variables lower down the notebook and then (re)run cells further up, those defined further down will still be present. Variables can be reused \u00b6 {#Variables-can-be-reused} In [9]: age = age + 1 print('Next year I will be', age) Next year I will be 22 Also, you can index to get a single character from a string. 'AB' != 'BA' -> ordering matters, because we can treat the string as a list of characters. Python uses zero-based indexing. Use square brackets to get a character in a specific position. atom_name = 'helium' print(atom_name[0]) In [11]: atom_name = 'helium' print(atom_name) print(atom_name[0]) helium h Slicing to get a substring {#Slicing-to-get-a-substring!} Quick concepts Substring:A part of a string. (1 to n number of characters) Element: An item on a list. In a string, the elements are the characters. Slice: part of a string (or any list-like thing) How to slice strings: [start:stop] , where start is replaced with the index of the first element we want and stop is replaced with the index of the element just after the last element we want . Taking a slice does not change the contents of the original string. Instead, the slice is a copy of part of the original string. In [13]: atom_name = 'sodium' print(atom_name[0:3]) # Use len to find the lenght of a string print(len('helium')) sod 6 Final considerations: \u00b6 {#Final-considerations:} Python is case-sensitive upper- and lower-case letters are different. Name != name are different variables. Use meaningful variable names flabadab = 42 ewr_422_yY = 'Ahmed' print(ewr_422_yY, 'is', flabadab, 'years old') In [15]: hola = 1 HOLA= 2 print(hola, HOLA) 1 2 Excercises \u00b6 {#Excercises} What is the final value of position in the program below? (Try to predict the value without running the program, then check your prediction.) initial = 'left' position = initial initial = 'right' In [18]: initial = 'left' position = initial initial = 'right' print(initial, position) right left If you assign a = 123 , what happens if you try to get the second digit of a via a[1] ? In [19]: a = 123 a[1] ---------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-19-d77b277d0f9d> in <module>() 1 a = 123 ----> 2 a[1] TypeError: 'int' object is not subscriptable What does the following program print? atom_name = 'carbon' print('atom_name[1:3] is:', atom_name[1:3]) In [20]: atom_name = 'carbon' print('atom_name[1:3] is:', atom_name[1:3]) atom_name[1:3] is: ar 1.What does thing[low:high] do? 2.What does thing[low:] (without a value after the colon) do? 3.What does thing[:high] (without a value before the colon) do? 4.What does thing[:] (just a colon) do? 5.What does thing[number:some-negative-number] do? 6.What happens when you choose a high value which is out of range? (i.e., try atom_name[0:15]) KEY POINTS - Variable and Assignment \u00b6 {#KEY-POINTS---Variable-and-Assignment} Use variables to store values. Use print to display values. Variables persist between cells. Variables must be created before they are used. Variables can be used in calculations. Use an index to get a single character from a string. Use a slice to get a substring. Use the built-in function len to find the length of a string. Python is case-sensitive. Use meaningful variable names. Data Types and Type Conversion \u00b6 {#Data-Types-and-Type-Conversion} (10 min) Exercises (10min) Every value has a type. \u00b6 {#Every-value-has-a-type.} Existing types are: Integer (int) : positive or negative whole numbers Floating point numbers (float) : real numbers Character string (str) : text Written in either single or double quotes (but they have to match) Use the built-in function type to find out what type a value or variable has print(type(variable)) In [22]: print(type(52)) fitness = 'average' print(type(fitness)) <class 'float'> <class 'str'> Operations or methods for a given type \u00b6 {#Operations-or-methods-for-a-given-type} Types control what operations (or methodds) can be performed on a given value A value\u2019s type determines what the program can do to it. In [23]: #This works print(5 - 3) 2 In [24]: #This works? print('hello' - 'h') ---------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-24-c60e4b1b64d0> in <module>() 1 #This works? ----> 2 print('hello' - 'h') TypeError: unsupported operand type(s) for -: 'str' and 'str' Operators that can be used on strings + * \"Adding\" characters strings concatenates them. In [26]: full_name = 'Ahmed'+ 'Walsh' print(full_name) AhmedWalsh Multiplying a character string by an integer N creates a new string that consists of that character string repeated N times. In [27]: separator = '=' * 10 print(separator) ========== Try function len on strings and numbers In [28]: #Text print(len(full_name)) 10 In [29]: #Number print(len(256)) ---------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-29-00ccdb9afd37> in <module>() 1 #Number ----> 2 print(len(256)) TypeError: object of type 'int' has no len() Excercises \u00b6 {#Excercises} What type of value is 3.4? How can you find out? What type of value is 3.25 + 4? What type of value (integer, floating point number, or character string) would you use to represent each of the following? Try to come up with more than one good answer for each problem. For example, in # 1, when would counting days with a floating point variable make more sense than using an integer? Number of days since the start of the year. Time elapsed from the start of the year until now in days. Serial number of a piece of lab equipment. A lab specimen\u2019s age Current population of a city. Average population of a city over time. Which of the following will return the floating point number 2.0 ? Note: there may be more than one right answer. first = 1.0 second = \"1\" third = \"1.1\" 1.first + float(second) 2.float(second) + float(third) 3.first + int(third) 4.first + int(float(third)) 5.int(first) + int(float(third)) 2.0 * second Key Points - Data Types and Type Conversion \u00b6 {#Key-Points---Data-Types-and-Type-Conversion} Every value has a type. Use the built-in function type to find the type of a value. Types control what operations can be done on values. Strings can be added and multiplied. Strings have a length (but numbers don\u2019t). Must convert numbers to strings or vice versa when operating on them. Can mix integers and floats freely in operations. Variables only change value when something is assigned to them. STRETCHING TIME {#STRETCHING-TIME!} Numpy \u00b6 {#Numpy} NumPy is the fundamental package for scientific computing with Python. It contains among other things: a powerful N-dimensional array object sophisticated (broadcasting) functions tools for integrating C/C++ and Fortran code useful linear algebra, Fourier transform, and random number capabilities Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases. We can create random numers easily In [34]: import numpy as np In [30]: from numpy import random r = random.randint(1, 35) print(r) 26 A very useful data structure, very similar to lists with a few exceptions such as: The number of elements in the array cannot be changed. (This means we cant .append() or remove) All element in the arrays must be the type of variables Once the array is created we can't change the data type. Advantages over lists: Arrays can be n-dimensional (vector, matrix, tensor) Supports arithmetic algebraic Arrays work faster than lists In [32]: mylist = [1,2,3,4 ] mylist Out[32]: [1, 2, 3, 4] In [35]: np.array(mylist) Out[35]: array([1, 2, 3, 4]) In [36]: mymatrix = [[1,2,3], [4,5,6], [7,8,9]] mymatrix Out[36]: [[1, 2, 3], [4, 5, 6], [7, 8, 9]] In [37]: m = np.array(mymatrix) m Out[37]: array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) In [38]: type(m) Out[38]: numpy.ndarray There are many ways to create an array, let's try some: Let's asume a is any numpy array. Function Description a.shape Returns a tuple with the numer of elements per dimension a.ndim Number of dimension a.size Number of elements in an array a.dtype Data type of the elements in the array a.T Transposes the array a.flat Collapses the array in 1 dimension a.copy() Returns a copy of the array a.fill() Fills the array with a determined value a.reshape() Returns an array with the same data but in the shape we indicate a.resize() Changes the shape of the array, but this does not creates a copy of the original array a.sort() Reorders the array In [43]: # Try some! print(m.dtype) print(m) int64 [[1 2 3] [4 5 6] [7 8 9]] Slicing, but in arrays \u00b6 {#Slicing,-but-in-arrays} It's more flexible. Slicings follows this structure: beginning:end:step In [52]: a = np.linspace(0, 10, 11) #blank spaces mean \"everthing\" # all print(a[1:11]) [ 1. 2. 3. 4. 5. 6. 7. 8. 9. 10.] In [0]: # 3 to 8 print(a[3:8]) # 1 to 9 with steps of size 2 (odd) print(a[1:11:2]) [3. 4. 5. 6. 7.] [1. 3. 5. 7. 9.] In [54]: # conditionals print(a[a > 4]) [ 5. 6. 7. 8. 9. 10.] Exercises \u00b6 {#Exercises} Try it yourself Create a non squared matrix, and print the dimensions (.shape) Use .reshape to change the shape of the array Try . size is equal to np.prod(a.shape) Built-in Functions and Help \u00b6 {#Built-in-Functions-and-Help} (10 min) Exercises (10 min) Arguments in a function A function may take zero or more arguments An argument is a value passed into a function For example some of the functions we used so far have arguments len takes exactly one argument len(_x_) int, str and float create a new value from an existing one print takes zero or more arguments With no arguments prints a blank line Must use the parethesis to use the function print('before') print() print('after') Commonly-used built-in functions include max , min and round \u00b6 {#Commonly-used-built-in-functions-include-max,-min-and-round} max() min() round() You can also combine some functions print(max(1, 2, 3)) print(min('a', 'A', '0')) But these functions may only work for certain (combination of) arguments. For example: max and min must be given at least one argument. \u201cLargest of the empty set\u201d is a meaningless question. And they must be given things that can meaningfully be compared. In [59]: print(max(1,4,6,9,100000)) print(min('a', 'A', 0)) 100000 ---------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-59-4a37fdd5cb05> in <module>() 1 print(max(1,4,6,9,100000)) ----> 2 print(min('a', 'A', 0)) TypeError: '<' not supported between instances of 'int' and 'str' Functions may have default values for some arguments \u00b6 {#Functions-may-have-default-values-for-some-arguments} round will round off a floating-point number. By default, rounds to zero decimal places. In [61]: round(3.712) # We can specify the number of decimal places we want. Let's try: round(3.712, 2 ) Out[61]: 3.71 You can use the built-in function help to get help for a function. \u00b6 {#You-can-use-the-built-in-function-help-to-get-help-for-a-function.} Every built-in has online documentation. In [62]: help(round) Help on built-in function round in module builtins: round(...) round(number[, ndigits]) -> number Round a number to a given precision in decimal digits (default 0 digits). This returns an int when called with one argument, otherwise the same type as the number. ndigits may be negative. In [63]: help(max) Help on built-in function max in module builtins: max(...) max(iterable, *[, default=obj, key=func]) -> value max(arg1, arg2, *args, *[, key=func]) -> value With a single iterable argument, return its biggest item. The default keyword-only argument specifies an object to return if the provided iterable is empty. With two or more arguments, return the largest argument. Python reports a syntax error when it can\u2019t understand the source of a program. \u00b6 {#Python-reports-a-syntax-error-when-it-can\u2019t-understand-the-source-of-a-program.} # Forgot to close the quote marks around the string. name = 'Feng Try it! In [66]: name = 'Feng' print(name) Feng The message indicates a problem on first line of the input (\u201cline 1\u201d). In this case the \u201cipython-input\u201d section of the file name tells us that we are working with input into IPython, the Python interpreter used by the Jupyter Notebook. The -6- part of the filename indicates that the error occurred in cell 6 of our Notebook. Next is the problematic line of code, indicating the problem with a \\^ pointer. Python reports a runtime error when something goes wrong while a program is executing. \u00b6 {#Python-reports-a-runtime-error-when-something-goes-wrong-while-a-program-is-executing.} In [68]: age = 53 remaining = 100 - age # mis-spelled 'age' print(remaining) 47 Fix syntax errors by reading the source and runtime errors by tracing execution. Getting help in Jupyter \u00b6 {#Getting-help-in-Jupyter} There are 2 ways to get help in a Jupyter Notebook Place the cursor anywhere in the function invocation (i.e., the function name or its parameters), hold down shift , and press tab . Or type a function name with a question mark after it. Every function returns something Every function call produces some result. If the function doesn\u2019t have a useful result to return, it usually returns the special value None . In [69]: max? In [70]: result = print('example') print('result of print is', result) example result of print is None Excercises \u00b6 {#Excercises} What happens when... \u00b6 {#What-happens-when...} Explain in simple terms the order of operations in the following program: when does the addition happen, when does the subtraction happen, when is each function called, etc. What is the final value of radiance? In [71]: radiance = 1.0 radiance = max(2.1, 2.0 + min(radiance, 1.1 * radiance - 0.5)) # The value is ... 2.6 print(radiance) 2.6 Spot the Difference {#Spot-the-Difference!} Predict what each of the print statements in the program below will print. Does max(len(rich), poor) run or produce an error message? If it runs, does its result make any sense? easy_string = \"abc\" print(max(easy_string)) rich = \"gold\" poor = \"tin\" print(max(rich, poor)) print(max(len(rich), len(poor))) Why not? \u00b6 {#Why-not?} Why don\u2019t max and min return None when they are given no arguments? KEY POINTS \u00b6 {#KEY-POINTS} Use comments to add documentation to programs. A function may take zero or more arguments. Commonly-used built-in functions include max, min, and round. Functions may only work for certain (combinations of) arguments. Functions may have default values for some arguments. Use the built-in function help to get help for a function. The Jupyter Notebook has two ways to get help. Every function returns something. Python reports a syntax error when it can\u2019t understand the source of a program. Python reports a runtime error when something goes wrong while a program is executing. Fix syntax errors by reading the source code, and runtime errors by tracing the program\u2019s execution. Libraries \u00b6 {#Libraries} 10 min Most of the power of a programming language is in its libraries. Library = collection of files (modules) that functions for use by other programs. The Python standard library is an extensive suite of modules that comes with Python itself. Other libraries available in PyPI (the Python Package Index). A program must import a library module before using it. Use import to load a library module into a program\u2019s memory. Then refer to things from the module as module_name.thing_name. Python uses . to mean \u201cpart of\u201d. In [72]: import math print('pi is', math.pi) print('cos(pi) is', math.cos(math.pi)) pi is 3.141592653589793 cos(pi) is -1.0 We can also use help() to learn about the content of a library module, just like we do with functions! In [73]: help(math) Help on module math: NAME math MODULE REFERENCE https://docs.python.org/3.6/library/math The following documentation is automatically generated from the Python source files. It may be incomplete, incorrect or include features that are considered implementation detail and may vary between Python implementations. When in doubt, consult the module reference at the location listed above. DESCRIPTION This module is always available. It provides access to the mathematical functions defined by the C standard. FUNCTIONS acos(...) acos(x) Return the arc cosine (measured in radians) of x. acosh(...) acosh(x) Return the inverse hyperbolic cosine of x. asin(...) asin(x) Return the arc sine (measured in radians) of x. asinh(...) asinh(x) Return the inverse hyperbolic sine of x. atan(...) atan(x) Return the arc tangent (measured in radians) of x. atan2(...) atan2(y, x) Return the arc tangent (measured in radians) of y/x. Unlike atan(y/x), the signs of both x and y are considered. atanh(...) atanh(x) Return the inverse hyperbolic tangent of x. ceil(...) ceil(x) Return the ceiling of x as an Integral. This is the smallest integer >= x. copysign(...) copysign(x, y) Return a float with the magnitude (absolute value) of x but the sign of y. On platforms that support signed zeros, copysign(1.0, -0.0) returns -1.0. cos(...) cos(x) Return the cosine of x (measured in radians). cosh(...) cosh(x) Return the hyperbolic cosine of x. degrees(...) degrees(x) Convert angle x from radians to degrees. erf(...) erf(x) Error function at x. erfc(...) erfc(x) Complementary error function at x. exp(...) exp(x) Return e raised to the power of x. expm1(...) expm1(x) Return exp(x)-1. This function avoids the loss of precision involved in the direct evaluation of exp(x)-1 for small x. fabs(...) fabs(x) Return the absolute value of the float x. factorial(...) factorial(x) -> Integral Find x!. Raise a ValueError if x is negative or non-integral. floor(...) floor(x) Return the floor of x as an Integral. This is the largest integer <= x. fmod(...) fmod(x, y) Return fmod(x, y), according to platform C. x % y may differ. frexp(...) frexp(x) Return the mantissa and exponent of x, as pair (m, e). m is a float and e is an int, such that x = m * 2.**e. If x is 0, m and e are both 0. Else 0.5 <= abs(m) < 1.0. fsum(...) fsum(iterable) Return an accurate floating point sum of values in the iterable. Assumes IEEE-754 floating point arithmetic. gamma(...) gamma(x) Gamma function at x. gcd(...) gcd(x, y) -> int greatest common divisor of x and y hypot(...) hypot(x, y) Return the Euclidean distance, sqrt(x*x + y*y). isclose(...) isclose(a, b, *, rel_tol=1e-09, abs_tol=0.0) -> bool Determine whether two floating point numbers are close in value. rel_tol maximum difference for being considered \"close\", relative to the magnitude of the input values abs_tol maximum difference for being considered \"close\", regardless of the magnitude of the input values Return True if a is close in value to b, and False otherwise. For the values to be considered close, the difference between them must be smaller than at least one of the tolerances. -inf, inf and NaN behave similarly to the IEEE 754 Standard. That is, NaN is not close to anything, even itself. inf and -inf are only close to themselves. isfinite(...) isfinite(x) -> bool Return True if x is neither an infinity nor a NaN, and False otherwise. isinf(...) isinf(x) -> bool Return True if x is a positive or negative infinity, and False otherwise. isnan(...) isnan(x) -> bool Return True if x is a NaN (not a number), and False otherwise. ldexp(...) ldexp(x, i) Return x * (2**i). lgamma(...) lgamma(x) Natural logarithm of absolute value of Gamma function at x. log(...) log(x[, base]) Return the logarithm of x to the given base. If the base not specified, returns the natural logarithm (base e) of x. log10(...) log10(x) Return the base 10 logarithm of x. log1p(...) log1p(x) Return the natural logarithm of 1+x (base e). The result is computed in a way which is accurate for x near zero. log2(...) log2(x) Return the base 2 logarithm of x. modf(...) modf(x) Return the fractional and integer parts of x. Both results carry the sign of x and are floats. pow(...) pow(x, y) Return x**y (x to the power of y). radians(...) radians(x) Convert angle x from degrees to radians. sin(...) sin(x) Return the sine of x (measured in radians). sinh(...) sinh(x) Return the hyperbolic sine of x. sqrt(...) sqrt(x) Return the square root of x. tan(...) tan(x) Return the tangent of x (measured in radians). tanh(...) tanh(x) Return the hyperbolic tangent of x. trunc(...) trunc(x:Real) -> Integral Truncates x to the nearest Integral toward 0. Uses the __trunc__ magic method. DATA e = 2.718281828459045 inf = inf nan = nan pi = 3.141592653589793 tau = 6.283185307179586 FILE /usr/local/lib/python3.6/lib-dynload/math.cpython-36m-x86_64-linux-gnu.so Import specific items from a library module to shorten programs. Use from ... import ... to load only specific items from a library module. Then refer to them directly without library name as prefix. In [74]: from math import cos, pi print('cos(pi) is', cos(pi)) cos(pi) is -1.0 Create an alias for a library module when importing it to shorten programs. Use import ... as ... to give a library a short alias while importing it. Then refer to items in the library using that shortened name. Exercises \u00b6 {#Exercises} Exploring the Math Module \u00b6 {#Exploring-the-Math-Module} What function from the math module can you use to calculate a square root without using sqrt? Since the library contains this function, why does sqrt exist? Locating the Right Module \u00b6 {#Locating-the-Right-Module} You want to select a random character from a string: bases = 'ACTTGCTTGAC' Which standard library module could help you? Which function would you select from that module? Are there alternatives? Try to write a program that uses the function. In [78]: from random import randrange bases = 'ACTTGCTTGAC' print(bases[randrange(len(bases))]) A Jigsaw Puzzle (Parson\u2019s Problem) \u00b6 {#Jigsaw-Puzzle-(Parson\u2019s-Problem)} Rearrange the following statements so that a random DNA base is printed and its index in the string. Not all statements may be needed. Feel free to use/add intermediate variables. In [ ]: bases=\"ACTTGCTTGAC\" import math import random ___ = random.randrange(n_bases) ___ = len(bases) print(\"random base \", bases[___], \"base index\", ___) Importing with aliases \u00b6 {#Importing-with-aliases} Fill in the blanks so that the program below prints 90.0. Rewrite the program so that it uses import without as. Which form do you find easier to read? In [80]: import math as m angle = m.degrees(m.pi / 2) print(angle) 90.0 Importing Specific Items \u00b6 {#Importing-Specific-Items} Fill in the blanks so that the program below prints 90.0. Do you find this version easier to read than preceding ones? Why wouldn\u2019t programmers always use this form of import? In [ ]: ____ math import ____, ____ angle = degrees(pi / 2) print(angle) Reading Error Messages \u00b6 {#Reading-Error-Messages} Read the code below and try to identify what the errors are without running it. Run the code, and read the error message. What type of error is it? In [79]: from math import log log(0) ---------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-79-d72e1d780bab> in <module>() 1 from math import log ----> 2 log(0) ValueError: math domain error Keypoints \u00b6 {#Keypoints} Most of the power of a programming language is in its libraries. A program must import a library module in order to use it. Use help to learn about the contents of a library module. Import specific items from a library to shorten programs. Create an alias for a library when importing it to shorten programs. Data Frames \u00b6 {#Data-Frames} Reading tabular data into DataFrame \u00b6 {#Reading-tabular-data-into-DataFrame} (15 min) Exercises (10 min) Pandas \u00b6 {#Pandas} A widely know library for statistics, particularly on tabular data. Borrows many features from R\u00b4s dataframes. Dataframe: A 2-dimensional table whose columns have names and potentially have different data types. In [81]: #Let's import pandas import pandas as pd data = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_oceania.csv') print(data) country gdpPercap_1952 gdpPercap_1957 gdpPercap_1962 \\ 0 Australia 10039.59564 10949.64959 12217.22686 1 New Zealand 10556.57566 12247.39532 13175.67800 gdpPercap_1967 gdpPercap_1972 gdpPercap_1977 gdpPercap_1982 \\ 0 14526.12465 16788.62948 18334.19751 19477.00928 1 14463.91893 16046.03728 16233.71770 17632.41040 gdpPercap_1987 gdpPercap_1992 gdpPercap_1997 gdpPercap_2002 \\ 0 21888.88903 23424.76683 26997.93657 30687.75473 1 19007.19129 18363.32494 21050.41377 23189.80135 gdpPercap_2007 0 34435.36744 1 25185.00911 File not found \u00b6 {#File-not-found} Our lessons store their data files in a data sub-directory, which is why the path to the file is data/gapminder_gdp_oceania.csv. If you forget to include data/, or if you include it but your copy of the file is somewhere else, you will get a runtime error that ends with a line like this: ERROR OSError: File b'gapminder_gdp_oceania.csv' does not exist Use index_col to specify that a column\u2019s values should be used as row headings. \u00b6 {#Use-index_col-to-specify-that-a-column\u2019s-values-should-be-used-as-row-headings.} Row headings are numbers (0 and 1 in this case). Really want to index by country. Pass the name of the column to read_csv as its index_col parameter to do this. In [84]: data = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_oceania.csv', index_col='country') data Out[84]: gdpPercap_1952 gdpPercap_1957 gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 gdpPercap_1977 gdpPercap_1982 gdpPercap_1987 gdpPercap_1992 gdpPercap_1997 gdpPercap_2002 gdpPercap_2007 country Australia 10039.59564 10949.64959 12217.22686 14526.12465 16788.62948 18334.19751 19477.00928 21888.88903 23424.76683 26997.93657 30687.75473 34435.36744 New Zealand 10556.57566 12247.39532 13175.67800 14463.91893 16046.03728 16233.71770 17632.41040 19007.19129 18363.32494 21050.41377 23189.80135 25185.00911 Use DataFrame.info to explore a little the dataframe In [85]: data.info() <class 'pandas.core.frame.DataFrame'> Index: 2 entries, Australia to New Zealand Data columns (total 12 columns): gdpPercap_1952 2 non-null float64 gdpPercap_1957 2 non-null float64 gdpPercap_1962 2 non-null float64 gdpPercap_1967 2 non-null float64 gdpPercap_1972 2 non-null float64 gdpPercap_1977 2 non-null float64 gdpPercap_1982 2 non-null float64 gdpPercap_1987 2 non-null float64 gdpPercap_1992 2 non-null float64 gdpPercap_1997 2 non-null float64 gdpPercap_2002 2 non-null float64 gdpPercap_2007 2 non-null float64 dtypes: float64(12) memory usage: 208.0+ bytes What we know? data is a DataFrame It has two rows named 'Australia' and 'New Zealand' There are Twelve columns, each of which has two actual 64-bit floating point values. No null values, aka no missing observations. Uses 208.0 bytes of memory The DataFrame.columns variable stores information about the dataframe's columns \u00b6 {#The-DataFrame.columns-variable-stores-information-about-the-dataframe's-columns} Note that this is data, not a method. Like math.pi. So do not use () to try to call it. Called a member variable, or just member. data is the dataframe, not a method, don't use () to try to call it. In [86]: print(data.columns) Index(['gdpPercap_1952', 'gdpPercap_1957', 'gdpPercap_1962', 'gdpPercap_1967', 'gdpPercap_1972', 'gdpPercap_1977', 'gdpPercap_1982', 'gdpPercap_1987', 'gdpPercap_1992', 'gdpPercap_1997', 'gdpPercap_2002', 'gdpPercap_2007'], dtype='object') Use DataFrame.T to transpose a dataframe Sometimes want to treat columns as rows and vice versa. Transpose (written .T) doesn\u2019t copy the data, just changes the program\u2019s view of it. Like columns, it is a member variable. In [87]: print(data.T) data.T country Australia New Zealand gdpPercap_1952 10039.59564 10556.57566 gdpPercap_1957 10949.64959 12247.39532 gdpPercap_1962 12217.22686 13175.67800 gdpPercap_1967 14526.12465 14463.91893 gdpPercap_1972 16788.62948 16046.03728 gdpPercap_1977 18334.19751 16233.71770 gdpPercap_1982 19477.00928 17632.41040 gdpPercap_1987 21888.88903 19007.19129 gdpPercap_1992 23424.76683 18363.32494 gdpPercap_1997 26997.93657 21050.41377 gdpPercap_2002 30687.75473 23189.80135 gdpPercap_2007 34435.36744 25185.00911 Out[87]: country Australia New Zealand gdpPercap_1952 10039.59564 10556.57566 gdpPercap_1957 10949.64959 12247.39532 gdpPercap_1962 12217.22686 13175.67800 gdpPercap_1967 14526.12465 14463.91893 gdpPercap_1972 16788.62948 16046.03728 gdpPercap_1977 18334.19751 16233.71770 gdpPercap_1982 19477.00928 17632.41040 gdpPercap_1987 21888.88903 19007.19129 gdpPercap_1992 23424.76683 18363.32494 gdpPercap_1997 26997.93657 21050.41377 gdpPercap_2002 30687.75473 23189.80135 gdpPercap_2007 34435.36744 25185.00911 Use DataFrame.describe to get summary statistics about data \u00b6 {#Use-DataFrame.describe-to-get-summary-statistics-about-data} DataFrame.describe() gets the summary statistics of only the columns that have numerical data. All other columns are ignored, unless you use the argument include='all' In [89]: data.describe() Out[89]: gdpPercap_1952 gdpPercap_1957 gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 gdpPercap_1977 gdpPercap_1982 gdpPercap_1987 gdpPercap_1992 gdpPercap_1997 gdpPercap_2002 gdpPercap_2007 count 2.000000 2.000000 2.000000 2.000000 2.00000 2.000000 2.000000 2.000000 2.000000 2.000000 2.000000 2.000000 mean 10298.085650 11598.522455 12696.452430 14495.021790 16417.33338 17283.957605 18554.709840 20448.040160 20894.045885 24024.175170 26938.778040 29810.188275 std 365.560078 917.644806 677.727301 43.986086 525.09198 1485.263517 1304.328377 2037.668013 3578.979883 4205.533703 5301.853680 6540.991104 min 10039.595640 10949.649590 12217.226860 14463.918930 16046.03728 16233.717700 17632.410400 19007.191290 18363.324940 21050.413770 23189.801350 25185.009110 25% 10168.840645 11274.086022 12456.839645 14479.470360 16231.68533 16758.837652 18093.560120 19727.615725 19628.685413 22537.294470 25064.289695 27497.598692 50% 10298.085650 11598.522455 12696.452430 14495.021790 16417.33338 17283.957605 18554.709840 20448.040160 20894.045885 24024.175170 26938.778040 29810.188275 75% 10427.330655 11922.958888 12936.065215 14510.573220 16602.98143 17809.077557 19015.859560 21168.464595 22159.406358 25511.055870 28813.266385 32122.777857 max 10556.575660 12247.395320 13175.678000 14526.124650 16788.62948 18334.197510 19477.009280 21888.889030 23424.766830 26997.936570 30687.754730 34435.367440 Not particularly useful with just two records, but very helpful when there are thousands \u00b6 {#Not-particularly-useful-with-just-two-records,-but-very-helpful-when-there-are-thousands} Exercises \u00b6 {#Exercises} Read the data in gapminder_gdp_americas.csv (should be in the same directory as gapminder_gdp_oceania.csv ) Tip \u00b6 {#Tip} check the parameters to define the index. Inspect the data. Use the function help(americas.head) and help(americas.tail) the answer: What method callwill display the first three rows of this data? What method call will display the last three columns of this data? Reading files in other directories \u00b6 {#Reading-files-in-other-directories} The data for your current project is stored in a file called microbes.csv, which is located in a folder called field_data. You are doing analysis in a notebook called analysis.ipynb in a sibling folder called thesis: your_home_directory +-- field_data/ | +-- microbes.csv +-- thesis/ +-- analysis.ipynb What value(s) should you pass to read_csv to read microbes.csv in analysis.ipynb? Writing Data \u00b6 {#Writing-Data} As well as the read_csv function for reading data from a file, Pandas provides a to_csv function to write dataframes to files. Applying what you\u2019ve learned about reading from files, write one of your dataframes to a file called processed.csv. You can use help to get information on how to use to_csv. In [96]: data2 = data.copy() #pd.to_csv #data2.to_csv? data2.to_csv('/home/mcubero/dataSanJose19/data/processed.csv') Key Points \u00b6 {#Key-Points} Use the Pandas library to get basic statistics out of tabular data. Use index_col to specify that a column\u2019s values should be used as row headings. Use DataFrame.info to find out more about a dataframe. The DataFrame.columns variable stores information about the dataframe\u2019s columns. Use DataFrame.T to transpose a dataframe. Use DataFrame.describe to get summary statistics about data. STRETCHING TIME {#STRETCHING-TIME!} Recap \u00b6 {#Recap} Feedback & questions \u00b6 {#Feedback-&-questions}","title":"Unit 4  Introduction to Python Session 1"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#session-1-session-1","text":"","title":"Session 1\u00b6 {#Session-1}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#plotting-and-programming-with-python-3-plotting-and-programming-with-python-3","text":"","title":"Plotting and programming with Python 3\u00b6 {#Plotting-and-programming-with-Python-3}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#open-a-jupyter-notebook-open-a-jupyter-notebook","text":"(10 minutes) http://swcarpentry.github.io/python-novice-gapminder/01-run-quit/index.html Once you have your Notebook open let`s start with the session. Heo","title":"Open a Jupyter Notebook\u00b6 {#Open-a-Jupyter-Notebook}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#variables-and-assignment-variables-and-assignment","text":"(15 min) Exercises (10min) Variables are names for determined values. Note: To assing the value to a variable use = like this: variable = value The variable is created when a value is assingned. For example let's type in our names and ages: In [2]: age = 21 name = 'Mariana' ejemplo= 5+5 print(age, name, ejemplo) 21 Mariana 10 In [4]: #1variable = 1 variable1 = 1","title":"Variables and Assignment\u00b6 {#Variables-and-Assignment}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#variable-names-variable-names","text":"Only letters, digits and underscore _ (for long names) Do not start with a digit It is possible to start with an underscore like __real_age , but this has a special meaning. (We won't do that yet)","title":"Variable names\u00b6 {#Variable-names}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#display-values-with-display-values-with","text":"print() This built-in function prints values as text Examples: In [5]: print('My name is', name, ', I am ', age) My name is Mariana , I am 21","title":"Display values with:\u00b6 {#Display-values-with:}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#please-remember-that-variables-must-be-created-before-they-are-used-please-remember-that-variables-must-be-created-before-they-are-used","text":"Otherwise Python reports an error In [8]: cat_name = \"Salem\" print(cat_name) Salem Be aware that it is the order of execution of cells that is important in a Jupyter notebook, not the order in which they appear . Python will remember all the code that was run previously, including any variables you have defined, irrespective of the order in the notebook. Therefore if you define variables lower down the notebook and then (re)run cells further up, those defined further down will still be present.","title":"Please remember that variables must be created before they are used {#Please-remember-that-variables-must-be-created-before-they-are-used!}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#variables-can-be-reused-variables-can-be-reused","text":"In [9]: age = age + 1 print('Next year I will be', age) Next year I will be 22 Also, you can index to get a single character from a string. 'AB' != 'BA' -> ordering matters, because we can treat the string as a list of characters. Python uses zero-based indexing. Use square brackets to get a character in a specific position. atom_name = 'helium' print(atom_name[0]) In [11]: atom_name = 'helium' print(atom_name) print(atom_name[0]) helium h","title":"Variables can be reused\u00b6 {#Variables-can-be-reused}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#slicing-to-get-a-substring-slicing-to-get-a-substring","text":"Quick concepts Substring:A part of a string. (1 to n number of characters) Element: An item on a list. In a string, the elements are the characters. Slice: part of a string (or any list-like thing) How to slice strings: [start:stop] , where start is replaced with the index of the first element we want and stop is replaced with the index of the element just after the last element we want . Taking a slice does not change the contents of the original string. Instead, the slice is a copy of part of the original string. In [13]: atom_name = 'sodium' print(atom_name[0:3]) # Use len to find the lenght of a string print(len('helium')) sod 6","title":"Slicing to get a substring {#Slicing-to-get-a-substring!}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#final-considerations-final-considerations","text":"Python is case-sensitive upper- and lower-case letters are different. Name != name are different variables. Use meaningful variable names flabadab = 42 ewr_422_yY = 'Ahmed' print(ewr_422_yY, 'is', flabadab, 'years old') In [15]: hola = 1 HOLA= 2 print(hola, HOLA) 1 2","title":"Final considerations:\u00b6 {#Final-considerations:}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#excercises-excercises","text":"What is the final value of position in the program below? (Try to predict the value without running the program, then check your prediction.) initial = 'left' position = initial initial = 'right' In [18]: initial = 'left' position = initial initial = 'right' print(initial, position) right left If you assign a = 123 , what happens if you try to get the second digit of a via a[1] ? In [19]: a = 123 a[1] ---------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-19-d77b277d0f9d> in <module>() 1 a = 123 ----> 2 a[1] TypeError: 'int' object is not subscriptable What does the following program print? atom_name = 'carbon' print('atom_name[1:3] is:', atom_name[1:3]) In [20]: atom_name = 'carbon' print('atom_name[1:3] is:', atom_name[1:3]) atom_name[1:3] is: ar 1.What does thing[low:high] do? 2.What does thing[low:] (without a value after the colon) do? 3.What does thing[:high] (without a value before the colon) do? 4.What does thing[:] (just a colon) do? 5.What does thing[number:some-negative-number] do? 6.What happens when you choose a high value which is out of range? (i.e., try atom_name[0:15])","title":"Excercises\u00b6 {#Excercises}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#key-points-variable-and-assignment-key-points-variable-and-assignment","text":"Use variables to store values. Use print to display values. Variables persist between cells. Variables must be created before they are used. Variables can be used in calculations. Use an index to get a single character from a string. Use a slice to get a substring. Use the built-in function len to find the length of a string. Python is case-sensitive. Use meaningful variable names.","title":"KEY POINTS - Variable and Assignment\u00b6 {#KEY-POINTS---Variable-and-Assignment}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#data-types-and-type-conversion-data-types-and-type-conversion","text":"(10 min) Exercises (10min)","title":"Data Types and Type Conversion\u00b6 {#Data-Types-and-Type-Conversion}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#every-value-has-a-type-every-value-has-a-type","text":"Existing types are: Integer (int) : positive or negative whole numbers Floating point numbers (float) : real numbers Character string (str) : text Written in either single or double quotes (but they have to match) Use the built-in function type to find out what type a value or variable has print(type(variable)) In [22]: print(type(52)) fitness = 'average' print(type(fitness)) <class 'float'> <class 'str'>","title":"Every value has a type.\u00b6 {#Every-value-has-a-type.}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#operations-or-methods-for-a-given-type-operations-or-methods-for-a-given-type","text":"Types control what operations (or methodds) can be performed on a given value A value\u2019s type determines what the program can do to it. In [23]: #This works print(5 - 3) 2 In [24]: #This works? print('hello' - 'h') ---------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-24-c60e4b1b64d0> in <module>() 1 #This works? ----> 2 print('hello' - 'h') TypeError: unsupported operand type(s) for -: 'str' and 'str' Operators that can be used on strings + * \"Adding\" characters strings concatenates them. In [26]: full_name = 'Ahmed'+ 'Walsh' print(full_name) AhmedWalsh Multiplying a character string by an integer N creates a new string that consists of that character string repeated N times. In [27]: separator = '=' * 10 print(separator) ========== Try function len on strings and numbers In [28]: #Text print(len(full_name)) 10 In [29]: #Number print(len(256)) ---------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-29-00ccdb9afd37> in <module>() 1 #Number ----> 2 print(len(256)) TypeError: object of type 'int' has no len()","title":"Operations or methods for a given type\u00b6 {#Operations-or-methods-for-a-given-type}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#excercises-excercises_1","text":"What type of value is 3.4? How can you find out? What type of value is 3.25 + 4? What type of value (integer, floating point number, or character string) would you use to represent each of the following? Try to come up with more than one good answer for each problem. For example, in # 1, when would counting days with a floating point variable make more sense than using an integer? Number of days since the start of the year. Time elapsed from the start of the year until now in days. Serial number of a piece of lab equipment. A lab specimen\u2019s age Current population of a city. Average population of a city over time. Which of the following will return the floating point number 2.0 ? Note: there may be more than one right answer. first = 1.0 second = \"1\" third = \"1.1\" 1.first + float(second) 2.float(second) + float(third) 3.first + int(third) 4.first + int(float(third)) 5.int(first) + int(float(third)) 2.0 * second","title":"Excercises\u00b6 {#Excercises}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#key-points-data-types-and-type-conversion-key-points-data-types-and-type-conversion","text":"Every value has a type. Use the built-in function type to find the type of a value. Types control what operations can be done on values. Strings can be added and multiplied. Strings have a length (but numbers don\u2019t). Must convert numbers to strings or vice versa when operating on them. Can mix integers and floats freely in operations. Variables only change value when something is assigned to them.","title":"Key Points - Data Types and Type Conversion\u00b6 {#Key-Points---Data-Types-and-Type-Conversion}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#stretching-time-stretching-time","text":"","title":"STRETCHING TIME {#STRETCHING-TIME!}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#numpy-numpy","text":"NumPy is the fundamental package for scientific computing with Python. It contains among other things: a powerful N-dimensional array object sophisticated (broadcasting) functions tools for integrating C/C++ and Fortran code useful linear algebra, Fourier transform, and random number capabilities Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases. We can create random numers easily In [34]: import numpy as np In [30]: from numpy import random r = random.randint(1, 35) print(r) 26 A very useful data structure, very similar to lists with a few exceptions such as: The number of elements in the array cannot be changed. (This means we cant .append() or remove) All element in the arrays must be the type of variables Once the array is created we can't change the data type. Advantages over lists: Arrays can be n-dimensional (vector, matrix, tensor) Supports arithmetic algebraic Arrays work faster than lists In [32]: mylist = [1,2,3,4 ] mylist Out[32]: [1, 2, 3, 4] In [35]: np.array(mylist) Out[35]: array([1, 2, 3, 4]) In [36]: mymatrix = [[1,2,3], [4,5,6], [7,8,9]] mymatrix Out[36]: [[1, 2, 3], [4, 5, 6], [7, 8, 9]] In [37]: m = np.array(mymatrix) m Out[37]: array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) In [38]: type(m) Out[38]: numpy.ndarray There are many ways to create an array, let's try some: Let's asume a is any numpy array. Function Description a.shape Returns a tuple with the numer of elements per dimension a.ndim Number of dimension a.size Number of elements in an array a.dtype Data type of the elements in the array a.T Transposes the array a.flat Collapses the array in 1 dimension a.copy() Returns a copy of the array a.fill() Fills the array with a determined value a.reshape() Returns an array with the same data but in the shape we indicate a.resize() Changes the shape of the array, but this does not creates a copy of the original array a.sort() Reorders the array In [43]: # Try some! print(m.dtype) print(m) int64 [[1 2 3] [4 5 6] [7 8 9]]","title":"Numpy\u00b6 {#Numpy}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#slicing-but-in-arrays-slicing-but-in-arrays","text":"It's more flexible. Slicings follows this structure: beginning:end:step In [52]: a = np.linspace(0, 10, 11) #blank spaces mean \"everthing\" # all print(a[1:11]) [ 1. 2. 3. 4. 5. 6. 7. 8. 9. 10.] In [0]: # 3 to 8 print(a[3:8]) # 1 to 9 with steps of size 2 (odd) print(a[1:11:2]) [3. 4. 5. 6. 7.] [1. 3. 5. 7. 9.] In [54]: # conditionals print(a[a > 4]) [ 5. 6. 7. 8. 9. 10.]","title":"Slicing, but in arrays\u00b6 {#Slicing,-but-in-arrays}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#exercises-exercises","text":"Try it yourself Create a non squared matrix, and print the dimensions (.shape) Use .reshape to change the shape of the array Try . size is equal to np.prod(a.shape)","title":"Exercises\u00b6 {#Exercises}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#built-in-functions-and-help-built-in-functions-and-help","text":"(10 min) Exercises (10 min) Arguments in a function A function may take zero or more arguments An argument is a value passed into a function For example some of the functions we used so far have arguments len takes exactly one argument len(_x_) int, str and float create a new value from an existing one print takes zero or more arguments With no arguments prints a blank line Must use the parethesis to use the function print('before') print() print('after')","title":"Built-in Functions and Help\u00b6 {#Built-in-Functions-and-Help}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#commonly-used-built-in-functions-include-max-min-and-round-commonly-used-built-in-functions-include-max-min-and-round","text":"max() min() round() You can also combine some functions print(max(1, 2, 3)) print(min('a', 'A', '0')) But these functions may only work for certain (combination of) arguments. For example: max and min must be given at least one argument. \u201cLargest of the empty set\u201d is a meaningless question. And they must be given things that can meaningfully be compared. In [59]: print(max(1,4,6,9,100000)) print(min('a', 'A', 0)) 100000 ---------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-59-4a37fdd5cb05> in <module>() 1 print(max(1,4,6,9,100000)) ----> 2 print(min('a', 'A', 0)) TypeError: '<' not supported between instances of 'int' and 'str'","title":"Commonly-used built-in functions include max, min and round\u00b6 {#Commonly-used-built-in-functions-include-max,-min-and-round}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#functions-may-have-default-values-for-some-arguments-functions-may-have-default-values-for-some-arguments","text":"round will round off a floating-point number. By default, rounds to zero decimal places. In [61]: round(3.712) # We can specify the number of decimal places we want. Let's try: round(3.712, 2 ) Out[61]: 3.71","title":"Functions may have default values for some arguments\u00b6 {#Functions-may-have-default-values-for-some-arguments}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#you-can-use-the-built-in-function-help-to-get-help-for-a-function-you-can-use-the-built-in-function-help-to-get-help-for-a-function","text":"Every built-in has online documentation. In [62]: help(round) Help on built-in function round in module builtins: round(...) round(number[, ndigits]) -> number Round a number to a given precision in decimal digits (default 0 digits). This returns an int when called with one argument, otherwise the same type as the number. ndigits may be negative. In [63]: help(max) Help on built-in function max in module builtins: max(...) max(iterable, *[, default=obj, key=func]) -> value max(arg1, arg2, *args, *[, key=func]) -> value With a single iterable argument, return its biggest item. The default keyword-only argument specifies an object to return if the provided iterable is empty. With two or more arguments, return the largest argument.","title":"You can use the built-in function help to get help for a function.\u00b6 {#You-can-use-the-built-in-function-help-to-get-help-for-a-function.}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#python-reports-a-syntax-error-when-it-cant-understand-the-source-of-a-program-python-reports-a-syntax-error-when-it-cant-understand-the-source-of-a-program","text":"# Forgot to close the quote marks around the string. name = 'Feng Try it! In [66]: name = 'Feng' print(name) Feng The message indicates a problem on first line of the input (\u201cline 1\u201d). In this case the \u201cipython-input\u201d section of the file name tells us that we are working with input into IPython, the Python interpreter used by the Jupyter Notebook. The -6- part of the filename indicates that the error occurred in cell 6 of our Notebook. Next is the problematic line of code, indicating the problem with a \\^ pointer.","title":"Python reports a syntax error when it can\u2019t understand the source of a program.\u00b6 {#Python-reports-a-syntax-error-when-it-can\u2019t-understand-the-source-of-a-program.}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#python-reports-a-runtime-error-when-something-goes-wrong-while-a-program-is-executing-python-reports-a-runtime-error-when-something-goes-wrong-while-a-program-is-executing","text":"In [68]: age = 53 remaining = 100 - age # mis-spelled 'age' print(remaining) 47 Fix syntax errors by reading the source and runtime errors by tracing execution.","title":"Python reports a runtime error when something goes wrong while a program is executing.\u00b6 {#Python-reports-a-runtime-error-when-something-goes-wrong-while-a-program-is-executing.}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#getting-help-in-jupyter-getting-help-in-jupyter","text":"There are 2 ways to get help in a Jupyter Notebook Place the cursor anywhere in the function invocation (i.e., the function name or its parameters), hold down shift , and press tab . Or type a function name with a question mark after it. Every function returns something Every function call produces some result. If the function doesn\u2019t have a useful result to return, it usually returns the special value None . In [69]: max? In [70]: result = print('example') print('result of print is', result) example result of print is None","title":"Getting help in Jupyter\u00b6 {#Getting-help-in-Jupyter}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#excercises-excercises_2","text":"","title":"Excercises\u00b6 {#Excercises}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#what-happens-when-what-happens-when","text":"Explain in simple terms the order of operations in the following program: when does the addition happen, when does the subtraction happen, when is each function called, etc. What is the final value of radiance? In [71]: radiance = 1.0 radiance = max(2.1, 2.0 + min(radiance, 1.1 * radiance - 0.5)) # The value is ... 2.6 print(radiance) 2.6","title":"What happens when...\u00b6 {#What-happens-when...}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#spot-the-difference-spot-the-difference","text":"Predict what each of the print statements in the program below will print. Does max(len(rich), poor) run or produce an error message? If it runs, does its result make any sense? easy_string = \"abc\" print(max(easy_string)) rich = \"gold\" poor = \"tin\" print(max(rich, poor)) print(max(len(rich), len(poor)))","title":"Spot the Difference {#Spot-the-Difference!}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#why-not-why-not","text":"Why don\u2019t max and min return None when they are given no arguments?","title":"Why not?\u00b6 {#Why-not?}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#key-points-key-points","text":"Use comments to add documentation to programs. A function may take zero or more arguments. Commonly-used built-in functions include max, min, and round. Functions may only work for certain (combinations of) arguments. Functions may have default values for some arguments. Use the built-in function help to get help for a function. The Jupyter Notebook has two ways to get help. Every function returns something. Python reports a syntax error when it can\u2019t understand the source of a program. Python reports a runtime error when something goes wrong while a program is executing. Fix syntax errors by reading the source code, and runtime errors by tracing the program\u2019s execution.","title":"KEY POINTS\u00b6 {#KEY-POINTS}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#libraries-libraries","text":"10 min Most of the power of a programming language is in its libraries. Library = collection of files (modules) that functions for use by other programs. The Python standard library is an extensive suite of modules that comes with Python itself. Other libraries available in PyPI (the Python Package Index). A program must import a library module before using it. Use import to load a library module into a program\u2019s memory. Then refer to things from the module as module_name.thing_name. Python uses . to mean \u201cpart of\u201d. In [72]: import math print('pi is', math.pi) print('cos(pi) is', math.cos(math.pi)) pi is 3.141592653589793 cos(pi) is -1.0 We can also use help() to learn about the content of a library module, just like we do with functions! In [73]: help(math) Help on module math: NAME math MODULE REFERENCE https://docs.python.org/3.6/library/math The following documentation is automatically generated from the Python source files. It may be incomplete, incorrect or include features that are considered implementation detail and may vary between Python implementations. When in doubt, consult the module reference at the location listed above. DESCRIPTION This module is always available. It provides access to the mathematical functions defined by the C standard. FUNCTIONS acos(...) acos(x) Return the arc cosine (measured in radians) of x. acosh(...) acosh(x) Return the inverse hyperbolic cosine of x. asin(...) asin(x) Return the arc sine (measured in radians) of x. asinh(...) asinh(x) Return the inverse hyperbolic sine of x. atan(...) atan(x) Return the arc tangent (measured in radians) of x. atan2(...) atan2(y, x) Return the arc tangent (measured in radians) of y/x. Unlike atan(y/x), the signs of both x and y are considered. atanh(...) atanh(x) Return the inverse hyperbolic tangent of x. ceil(...) ceil(x) Return the ceiling of x as an Integral. This is the smallest integer >= x. copysign(...) copysign(x, y) Return a float with the magnitude (absolute value) of x but the sign of y. On platforms that support signed zeros, copysign(1.0, -0.0) returns -1.0. cos(...) cos(x) Return the cosine of x (measured in radians). cosh(...) cosh(x) Return the hyperbolic cosine of x. degrees(...) degrees(x) Convert angle x from radians to degrees. erf(...) erf(x) Error function at x. erfc(...) erfc(x) Complementary error function at x. exp(...) exp(x) Return e raised to the power of x. expm1(...) expm1(x) Return exp(x)-1. This function avoids the loss of precision involved in the direct evaluation of exp(x)-1 for small x. fabs(...) fabs(x) Return the absolute value of the float x. factorial(...) factorial(x) -> Integral Find x!. Raise a ValueError if x is negative or non-integral. floor(...) floor(x) Return the floor of x as an Integral. This is the largest integer <= x. fmod(...) fmod(x, y) Return fmod(x, y), according to platform C. x % y may differ. frexp(...) frexp(x) Return the mantissa and exponent of x, as pair (m, e). m is a float and e is an int, such that x = m * 2.**e. If x is 0, m and e are both 0. Else 0.5 <= abs(m) < 1.0. fsum(...) fsum(iterable) Return an accurate floating point sum of values in the iterable. Assumes IEEE-754 floating point arithmetic. gamma(...) gamma(x) Gamma function at x. gcd(...) gcd(x, y) -> int greatest common divisor of x and y hypot(...) hypot(x, y) Return the Euclidean distance, sqrt(x*x + y*y). isclose(...) isclose(a, b, *, rel_tol=1e-09, abs_tol=0.0) -> bool Determine whether two floating point numbers are close in value. rel_tol maximum difference for being considered \"close\", relative to the magnitude of the input values abs_tol maximum difference for being considered \"close\", regardless of the magnitude of the input values Return True if a is close in value to b, and False otherwise. For the values to be considered close, the difference between them must be smaller than at least one of the tolerances. -inf, inf and NaN behave similarly to the IEEE 754 Standard. That is, NaN is not close to anything, even itself. inf and -inf are only close to themselves. isfinite(...) isfinite(x) -> bool Return True if x is neither an infinity nor a NaN, and False otherwise. isinf(...) isinf(x) -> bool Return True if x is a positive or negative infinity, and False otherwise. isnan(...) isnan(x) -> bool Return True if x is a NaN (not a number), and False otherwise. ldexp(...) ldexp(x, i) Return x * (2**i). lgamma(...) lgamma(x) Natural logarithm of absolute value of Gamma function at x. log(...) log(x[, base]) Return the logarithm of x to the given base. If the base not specified, returns the natural logarithm (base e) of x. log10(...) log10(x) Return the base 10 logarithm of x. log1p(...) log1p(x) Return the natural logarithm of 1+x (base e). The result is computed in a way which is accurate for x near zero. log2(...) log2(x) Return the base 2 logarithm of x. modf(...) modf(x) Return the fractional and integer parts of x. Both results carry the sign of x and are floats. pow(...) pow(x, y) Return x**y (x to the power of y). radians(...) radians(x) Convert angle x from degrees to radians. sin(...) sin(x) Return the sine of x (measured in radians). sinh(...) sinh(x) Return the hyperbolic sine of x. sqrt(...) sqrt(x) Return the square root of x. tan(...) tan(x) Return the tangent of x (measured in radians). tanh(...) tanh(x) Return the hyperbolic tangent of x. trunc(...) trunc(x:Real) -> Integral Truncates x to the nearest Integral toward 0. Uses the __trunc__ magic method. DATA e = 2.718281828459045 inf = inf nan = nan pi = 3.141592653589793 tau = 6.283185307179586 FILE /usr/local/lib/python3.6/lib-dynload/math.cpython-36m-x86_64-linux-gnu.so Import specific items from a library module to shorten programs. Use from ... import ... to load only specific items from a library module. Then refer to them directly without library name as prefix. In [74]: from math import cos, pi print('cos(pi) is', cos(pi)) cos(pi) is -1.0 Create an alias for a library module when importing it to shorten programs. Use import ... as ... to give a library a short alias while importing it. Then refer to items in the library using that shortened name.","title":"Libraries\u00b6 {#Libraries}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#exercises-exercises_1","text":"","title":"Exercises\u00b6 {#Exercises}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#exploring-the-math-module-exploring-the-math-module","text":"What function from the math module can you use to calculate a square root without using sqrt? Since the library contains this function, why does sqrt exist?","title":"Exploring the Math Module\u00b6 {#Exploring-the-Math-Module}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#locating-the-right-module-locating-the-right-module","text":"You want to select a random character from a string: bases = 'ACTTGCTTGAC' Which standard library module could help you? Which function would you select from that module? Are there alternatives? Try to write a program that uses the function. In [78]: from random import randrange bases = 'ACTTGCTTGAC' print(bases[randrange(len(bases))]) A","title":"Locating the Right Module\u00b6 {#Locating-the-Right-Module}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#jigsaw-puzzle-parsons-problem-jigsaw-puzzle-parsons-problem","text":"Rearrange the following statements so that a random DNA base is printed and its index in the string. Not all statements may be needed. Feel free to use/add intermediate variables. In [ ]: bases=\"ACTTGCTTGAC\" import math import random ___ = random.randrange(n_bases) ___ = len(bases) print(\"random base \", bases[___], \"base index\", ___)","title":"Jigsaw Puzzle (Parson\u2019s Problem)\u00b6 {#Jigsaw-Puzzle-(Parson\u2019s-Problem)}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#importing-with-aliases-importing-with-aliases","text":"Fill in the blanks so that the program below prints 90.0. Rewrite the program so that it uses import without as. Which form do you find easier to read? In [80]: import math as m angle = m.degrees(m.pi / 2) print(angle) 90.0","title":"Importing with aliases\u00b6 {#Importing-with-aliases}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#importing-specific-items-importing-specific-items","text":"Fill in the blanks so that the program below prints 90.0. Do you find this version easier to read than preceding ones? Why wouldn\u2019t programmers always use this form of import? In [ ]: ____ math import ____, ____ angle = degrees(pi / 2) print(angle)","title":"Importing Specific Items\u00b6 {#Importing-Specific-Items}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#reading-error-messages-reading-error-messages","text":"Read the code below and try to identify what the errors are without running it. Run the code, and read the error message. What type of error is it? In [79]: from math import log log(0) ---------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-79-d72e1d780bab> in <module>() 1 from math import log ----> 2 log(0) ValueError: math domain error","title":"Reading Error Messages\u00b6 {#Reading-Error-Messages}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#keypoints-keypoints","text":"Most of the power of a programming language is in its libraries. A program must import a library module in order to use it. Use help to learn about the contents of a library module. Import specific items from a library to shorten programs. Create an alias for a library when importing it to shorten programs.","title":"Keypoints\u00b6 {#Keypoints}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#data-frames-data-frames","text":"","title":"Data Frames\u00b6 {#Data-Frames}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#reading-tabular-data-into-dataframe-reading-tabular-data-into-dataframe","text":"(15 min) Exercises (10 min)","title":"Reading tabular data into DataFrame\u00b6 {#Reading-tabular-data-into-DataFrame}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#pandas-pandas","text":"A widely know library for statistics, particularly on tabular data. Borrows many features from R\u00b4s dataframes. Dataframe: A 2-dimensional table whose columns have names and potentially have different data types. In [81]: #Let's import pandas import pandas as pd data = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_oceania.csv') print(data) country gdpPercap_1952 gdpPercap_1957 gdpPercap_1962 \\ 0 Australia 10039.59564 10949.64959 12217.22686 1 New Zealand 10556.57566 12247.39532 13175.67800 gdpPercap_1967 gdpPercap_1972 gdpPercap_1977 gdpPercap_1982 \\ 0 14526.12465 16788.62948 18334.19751 19477.00928 1 14463.91893 16046.03728 16233.71770 17632.41040 gdpPercap_1987 gdpPercap_1992 gdpPercap_1997 gdpPercap_2002 \\ 0 21888.88903 23424.76683 26997.93657 30687.75473 1 19007.19129 18363.32494 21050.41377 23189.80135 gdpPercap_2007 0 34435.36744 1 25185.00911","title":"Pandas\u00b6 {#Pandas}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#file-not-found-file-not-found","text":"Our lessons store their data files in a data sub-directory, which is why the path to the file is data/gapminder_gdp_oceania.csv. If you forget to include data/, or if you include it but your copy of the file is somewhere else, you will get a runtime error that ends with a line like this: ERROR OSError: File b'gapminder_gdp_oceania.csv' does not exist","title":"File not found\u00b6 {#File-not-found}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#use-index95col-to-specify-that-a-columns-values-should-be-used-as-row-headings-use-index_col-to-specify-that-a-columns-values-should-be-used-as-row-headings","text":"Row headings are numbers (0 and 1 in this case). Really want to index by country. Pass the name of the column to read_csv as its index_col parameter to do this. In [84]: data = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_oceania.csv', index_col='country') data Out[84]: gdpPercap_1952 gdpPercap_1957 gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 gdpPercap_1977 gdpPercap_1982 gdpPercap_1987 gdpPercap_1992 gdpPercap_1997 gdpPercap_2002 gdpPercap_2007 country Australia 10039.59564 10949.64959 12217.22686 14526.12465 16788.62948 18334.19751 19477.00928 21888.88903 23424.76683 26997.93657 30687.75473 34435.36744 New Zealand 10556.57566 12247.39532 13175.67800 14463.91893 16046.03728 16233.71770 17632.41040 19007.19129 18363.32494 21050.41377 23189.80135 25185.00911 Use DataFrame.info to explore a little the dataframe In [85]: data.info() <class 'pandas.core.frame.DataFrame'> Index: 2 entries, Australia to New Zealand Data columns (total 12 columns): gdpPercap_1952 2 non-null float64 gdpPercap_1957 2 non-null float64 gdpPercap_1962 2 non-null float64 gdpPercap_1967 2 non-null float64 gdpPercap_1972 2 non-null float64 gdpPercap_1977 2 non-null float64 gdpPercap_1982 2 non-null float64 gdpPercap_1987 2 non-null float64 gdpPercap_1992 2 non-null float64 gdpPercap_1997 2 non-null float64 gdpPercap_2002 2 non-null float64 gdpPercap_2007 2 non-null float64 dtypes: float64(12) memory usage: 208.0+ bytes What we know? data is a DataFrame It has two rows named 'Australia' and 'New Zealand' There are Twelve columns, each of which has two actual 64-bit floating point values. No null values, aka no missing observations. Uses 208.0 bytes of memory","title":"Use index_col to specify that a column\u2019s values should be used as row headings.\u00b6 {#Use-index_col-to-specify-that-a-column\u2019s-values-should-be-used-as-row-headings.}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#the-dataframecolumns-variable-stores-information-about-the-dataframes-columns-the-dataframecolumns-variable-stores-information-about-the-dataframes-columns","text":"Note that this is data, not a method. Like math.pi. So do not use () to try to call it. Called a member variable, or just member. data is the dataframe, not a method, don't use () to try to call it. In [86]: print(data.columns) Index(['gdpPercap_1952', 'gdpPercap_1957', 'gdpPercap_1962', 'gdpPercap_1967', 'gdpPercap_1972', 'gdpPercap_1977', 'gdpPercap_1982', 'gdpPercap_1987', 'gdpPercap_1992', 'gdpPercap_1997', 'gdpPercap_2002', 'gdpPercap_2007'], dtype='object') Use DataFrame.T to transpose a dataframe Sometimes want to treat columns as rows and vice versa. Transpose (written .T) doesn\u2019t copy the data, just changes the program\u2019s view of it. Like columns, it is a member variable. In [87]: print(data.T) data.T country Australia New Zealand gdpPercap_1952 10039.59564 10556.57566 gdpPercap_1957 10949.64959 12247.39532 gdpPercap_1962 12217.22686 13175.67800 gdpPercap_1967 14526.12465 14463.91893 gdpPercap_1972 16788.62948 16046.03728 gdpPercap_1977 18334.19751 16233.71770 gdpPercap_1982 19477.00928 17632.41040 gdpPercap_1987 21888.88903 19007.19129 gdpPercap_1992 23424.76683 18363.32494 gdpPercap_1997 26997.93657 21050.41377 gdpPercap_2002 30687.75473 23189.80135 gdpPercap_2007 34435.36744 25185.00911 Out[87]: country Australia New Zealand gdpPercap_1952 10039.59564 10556.57566 gdpPercap_1957 10949.64959 12247.39532 gdpPercap_1962 12217.22686 13175.67800 gdpPercap_1967 14526.12465 14463.91893 gdpPercap_1972 16788.62948 16046.03728 gdpPercap_1977 18334.19751 16233.71770 gdpPercap_1982 19477.00928 17632.41040 gdpPercap_1987 21888.88903 19007.19129 gdpPercap_1992 23424.76683 18363.32494 gdpPercap_1997 26997.93657 21050.41377 gdpPercap_2002 30687.75473 23189.80135 gdpPercap_2007 34435.36744 25185.00911","title":"The DataFrame.columns variable stores information about the dataframe's columns\u00b6 {#The-DataFrame.columns-variable-stores-information-about-the-dataframe's-columns}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#use-dataframedescribe-to-get-summary-statistics-about-data-use-dataframedescribe-to-get-summary-statistics-about-data","text":"DataFrame.describe() gets the summary statistics of only the columns that have numerical data. All other columns are ignored, unless you use the argument include='all' In [89]: data.describe() Out[89]: gdpPercap_1952 gdpPercap_1957 gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 gdpPercap_1977 gdpPercap_1982 gdpPercap_1987 gdpPercap_1992 gdpPercap_1997 gdpPercap_2002 gdpPercap_2007 count 2.000000 2.000000 2.000000 2.000000 2.00000 2.000000 2.000000 2.000000 2.000000 2.000000 2.000000 2.000000 mean 10298.085650 11598.522455 12696.452430 14495.021790 16417.33338 17283.957605 18554.709840 20448.040160 20894.045885 24024.175170 26938.778040 29810.188275 std 365.560078 917.644806 677.727301 43.986086 525.09198 1485.263517 1304.328377 2037.668013 3578.979883 4205.533703 5301.853680 6540.991104 min 10039.595640 10949.649590 12217.226860 14463.918930 16046.03728 16233.717700 17632.410400 19007.191290 18363.324940 21050.413770 23189.801350 25185.009110 25% 10168.840645 11274.086022 12456.839645 14479.470360 16231.68533 16758.837652 18093.560120 19727.615725 19628.685413 22537.294470 25064.289695 27497.598692 50% 10298.085650 11598.522455 12696.452430 14495.021790 16417.33338 17283.957605 18554.709840 20448.040160 20894.045885 24024.175170 26938.778040 29810.188275 75% 10427.330655 11922.958888 12936.065215 14510.573220 16602.98143 17809.077557 19015.859560 21168.464595 22159.406358 25511.055870 28813.266385 32122.777857 max 10556.575660 12247.395320 13175.678000 14526.124650 16788.62948 18334.197510 19477.009280 21888.889030 23424.766830 26997.936570 30687.754730 34435.367440","title":"Use DataFrame.describe to get summary statistics about data\u00b6 {#Use-DataFrame.describe-to-get-summary-statistics-about-data}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#not-particularly-useful-with-just-two-records-but-very-helpful-when-there-are-thousands-not-particularly-useful-with-just-two-records-but-very-helpful-when-there-are-thousands","text":"","title":"Not particularly useful with just two records, but very helpful when there are thousands\u00b6 {#Not-particularly-useful-with-just-two-records,-but-very-helpful-when-there-are-thousands}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#exercises-exercises_2","text":"Read the data in gapminder_gdp_americas.csv (should be in the same directory as gapminder_gdp_oceania.csv )","title":"Exercises\u00b6 {#Exercises}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#tip-tip","text":"check the parameters to define the index. Inspect the data. Use the function help(americas.head) and help(americas.tail) the answer: What method callwill display the first three rows of this data? What method call will display the last three columns of this data?","title":"Tip\u00b6 {#Tip}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#reading-files-in-other-directories-reading-files-in-other-directories","text":"The data for your current project is stored in a file called microbes.csv, which is located in a folder called field_data. You are doing analysis in a notebook called analysis.ipynb in a sibling folder called thesis: your_home_directory +-- field_data/ | +-- microbes.csv +-- thesis/ +-- analysis.ipynb What value(s) should you pass to read_csv to read microbes.csv in analysis.ipynb?","title":"Reading files in other directories\u00b6 {#Reading-files-in-other-directories}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#writing-data-writing-data","text":"As well as the read_csv function for reading data from a file, Pandas provides a to_csv function to write dataframes to files. Applying what you\u2019ve learned about reading from files, write one of your dataframes to a file called processed.csv. You can use help to get information on how to use to_csv. In [96]: data2 = data.copy() #pd.to_csv #data2.to_csv? data2.to_csv('/home/mcubero/dataSanJose19/data/processed.csv')","title":"Writing Data\u00b6 {#Writing-Data}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#key-points-key-points_1","text":"Use the Pandas library to get basic statistics out of tabular data. Use index_col to specify that a column\u2019s values should be used as row headings. Use DataFrame.info to find out more about a dataframe. The DataFrame.columns variable stores information about the dataframe\u2019s columns. Use DataFrame.T to transpose a dataframe. Use DataFrame.describe to get summary statistics about data.","title":"Key Points\u00b6 {#Key-Points}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#stretching-time-stretching-time_1","text":"","title":"STRETCHING TIME {#STRETCHING-TIME!}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#recap-recap","text":"","title":"Recap\u00b6 {#Recap}"},{"location":"DataSanJose2019/slides/Unit 4_ Introduction_to_Python_Session_1/#feedback-questions-feedback-questions","text":"","title":"Feedback &amp; questions\u00b6 {#Feedback-&amp;-questions}"},{"location":"DataSanJose2019/slides/Unit4_Introduction_to_Python_ Final_practice/","text":"In [ ]: #General practice #Wrap-up #Feedback Practice \u00b6 {#Practice} With the data in the file \"Salaries.csv\" in the folder data do the following: Load the data Check the structure of the file Check the type of variables in the file Remember the method .info() Select the numeric variables in a separate dataframe *Remember using columns Check if there are some missing values IF you have missing values, correct them Make a quick plot for one of the variables, be creative! Rescale the data using the method preprocessing.MinMaxScaler() In [7]: # Load the data import pandas as pd data = pd.read_csv('/home/mcubero/dataSanJose19/data/Salaries.csv') #Check the structure of the file data.head() Out[7]: Unnamed: 0 rank discipline yrs.since.phd yrs.service sex salary 0 1 Prof B 19 18 Male 139750 1 2 Prof B 20 16 Male 173200 2 3 AsstProf B 4 3 Male 79750 3 4 Prof B 45 39 Male 115000 4 5 Prof B 40 41 Male 141500 In [6]: # Check the type of variables in the file data.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 397 entries, 0 to 396 Data columns (total 7 columns): Unnamed: 0 397 non-null int64 rank 397 non-null object discipline 397 non-null object yrs.since.phd 397 non-null int64 yrs.service 397 non-null int64 sex 397 non-null object salary 397 non-null int64 dtypes: int64(4), object(3) memory usage: 21.8+ KB In [11]: #Select the numeric variables in a separate dataframe *Remember using columns num = data.iloc[:,[3,4,6]] num.head() Out[11]: yrs.since.phd yrs.service salary 0 19 18 139750 1 20 16 173200 2 4 3 79750 3 45 39 115000 4 40 41 141500 In [13]: # Check if there are some missing values num.isna().sum() Out[13]: yrs.since.phd 0 yrs.service 0 salary 0 dtype: int64 In [14]: #Rescale the data using the method preprocessing.MinMaxScaler() from sklearn import preprocessing #Save columns names names = num.columns #Create scaler scaler = preprocessing.MinMaxScaler() #StandardScaler() #MaxAbsScaler #Transform your data frame (numeric variables ) data1 = num data1 = scaler.fit_transform(data1) data1 = pd.DataFrame(data1, columns=names) print(data1.head()) print(num.head()) yrs.since.phd yrs.service salary 0 0.327273 0.300000 0.471668 1 0.345455 0.266667 0.664192 2 0.054545 0.050000 0.126335 3 0.800000 0.650000 0.329218 4 0.709091 0.683333 0.481740 yrs.since.phd yrs.service salary 0 19 18 139750 1 20 16 173200 2 4 3 79750 3 45 39 115000 4 40 41 141500 Wrap-up \u00b6 {#Wrap-up} 20 min Python supports a large and diverse community across academia and industry. \u00b6 {#Python-supports-a-large-and-diverse-community-across-academia-and-industry.} NumPy The Python 3 documentation covers the core language and the standard library. PyCon is the largest annual conference for the Python community. SciPy is a rich collection of scientific utilities. It is also the name of a series of annual conferences . Jupyter is the home of Project Jupyter. Pandas is the home of the Pandas data library. Stack Overflow\u2019s general Python section can be helpful, as well as the sections on NumPy , SciPy , and Pandas. KEY POINTS \u00b6 {#KEY-POINTS} Python supports a large and diverse community across academia and industry. Feedback \u00b6 {#Feedback} THANK YOU {#THANK-YOU!}","title":"Unit4 Introduction to Python  Final practice"},{"location":"DataSanJose2019/slides/Unit4_Introduction_to_Python_ Final_practice/#practice-practice","text":"With the data in the file \"Salaries.csv\" in the folder data do the following: Load the data Check the structure of the file Check the type of variables in the file Remember the method .info() Select the numeric variables in a separate dataframe *Remember using columns Check if there are some missing values IF you have missing values, correct them Make a quick plot for one of the variables, be creative! Rescale the data using the method preprocessing.MinMaxScaler() In [7]: # Load the data import pandas as pd data = pd.read_csv('/home/mcubero/dataSanJose19/data/Salaries.csv') #Check the structure of the file data.head() Out[7]: Unnamed: 0 rank discipline yrs.since.phd yrs.service sex salary 0 1 Prof B 19 18 Male 139750 1 2 Prof B 20 16 Male 173200 2 3 AsstProf B 4 3 Male 79750 3 4 Prof B 45 39 Male 115000 4 5 Prof B 40 41 Male 141500 In [6]: # Check the type of variables in the file data.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 397 entries, 0 to 396 Data columns (total 7 columns): Unnamed: 0 397 non-null int64 rank 397 non-null object discipline 397 non-null object yrs.since.phd 397 non-null int64 yrs.service 397 non-null int64 sex 397 non-null object salary 397 non-null int64 dtypes: int64(4), object(3) memory usage: 21.8+ KB In [11]: #Select the numeric variables in a separate dataframe *Remember using columns num = data.iloc[:,[3,4,6]] num.head() Out[11]: yrs.since.phd yrs.service salary 0 19 18 139750 1 20 16 173200 2 4 3 79750 3 45 39 115000 4 40 41 141500 In [13]: # Check if there are some missing values num.isna().sum() Out[13]: yrs.since.phd 0 yrs.service 0 salary 0 dtype: int64 In [14]: #Rescale the data using the method preprocessing.MinMaxScaler() from sklearn import preprocessing #Save columns names names = num.columns #Create scaler scaler = preprocessing.MinMaxScaler() #StandardScaler() #MaxAbsScaler #Transform your data frame (numeric variables ) data1 = num data1 = scaler.fit_transform(data1) data1 = pd.DataFrame(data1, columns=names) print(data1.head()) print(num.head()) yrs.since.phd yrs.service salary 0 0.327273 0.300000 0.471668 1 0.345455 0.266667 0.664192 2 0.054545 0.050000 0.126335 3 0.800000 0.650000 0.329218 4 0.709091 0.683333 0.481740 yrs.since.phd yrs.service salary 0 19 18 139750 1 20 16 173200 2 4 3 79750 3 45 39 115000 4 40 41 141500","title":"Practice\u00b6 {#Practice}"},{"location":"DataSanJose2019/slides/Unit4_Introduction_to_Python_ Final_practice/#wrap-up-wrap-up","text":"20 min","title":"Wrap-up\u00b6 {#Wrap-up}"},{"location":"DataSanJose2019/slides/Unit4_Introduction_to_Python_ Final_practice/#python-supports-a-large-and-diverse-community-across-academia-and-industry-python-supports-a-large-and-diverse-community-across-academia-and-industry","text":"NumPy The Python 3 documentation covers the core language and the standard library. PyCon is the largest annual conference for the Python community. SciPy is a rich collection of scientific utilities. It is also the name of a series of annual conferences . Jupyter is the home of Project Jupyter. Pandas is the home of the Pandas data library. Stack Overflow\u2019s general Python section can be helpful, as well as the sections on NumPy , SciPy , and Pandas.","title":"Python supports a large and diverse community across academia and industry.\u00b6 {#Python-supports-a-large-and-diverse-community-across-academia-and-industry.}"},{"location":"DataSanJose2019/slides/Unit4_Introduction_to_Python_ Final_practice/#key-points-key-points","text":"Python supports a large and diverse community across academia and industry.","title":"KEY POINTS\u00b6 {#KEY-POINTS}"},{"location":"DataSanJose2019/slides/Unit4_Introduction_to_Python_ Final_practice/#feedback-feedback","text":"","title":"Feedback\u00b6 {#Feedback}"},{"location":"DataSanJose2019/slides/Unit4_Introduction_to_Python_ Final_practice/#thank-you-thank-you","text":"","title":"THANK YOU {#THANK-YOU!}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/","text":"Pandas Dataframes/Series \u00b6 {#Pandas-Dataframes/Series} 20 min Exercies (10 min) A DataFrame is a collection of Series; The DataFrame is the way Pandas represents a table, and Series is the data-structure Pandas use to represent a column. Pandas is built on top of the Numpy library, which in practice means that most of the methods defined for Numpy Arrays apply to Pandas Series/DataFrames. What makes Pandas so attractive is the powerful interface to access individual records of the table, proper handling of missing values, and relational-databases operations between DataFrames. Selecting values (iloc[...,...]) \u00b6 {#Selecting-values-(iloc[...,...])} To access a value at the position [i,j] of a DataFrame, we have two options, depending on what is the meaning of i in use. Remember that a DataFrame provides a index as a way to identify the rows of the table; a row, then, has a position inside the table as well as a label, which uniquely identifies its entry in the DataFrame. dataframe.iloc can specify by numerical index analogously to 2D version of character selection in strings. dataframe.iloc[rows, columns] In [4]: import pandas as pd data = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_europe.csv', index_col='country') #data print(data.iloc[0:3, 0]) #With labels #print(data.loc[\"Albania\", \"gdpPercap_1952\"]) #All columns (just like usual slicing) #print(data.loc[\"Albania\", :]) country Albania 1601.056136 Austria 6137.076492 Belgium 8343.105127 Name: gdpPercap_1952, dtype: float64 Use DataFrame.loc[..., ...] to select values by their (entry) label. Can specify location by row name analogously to 2D version of dictionary keys. In [72]: data = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_europe.csv', index_col='country') print(data.loc[\"Albania\", \"gdpPercap_1952\"]) 1601.056136 Use : on its own to mean all columns or all rows. Just like Python\u2019s usual slicing notation. In [9]: print(data.loc[\"Italy\",:]) gdpPercap_1952 4931.404155 gdpPercap_1957 6248.656232 gdpPercap_1962 8243.582340 gdpPercap_1967 10022.401310 gdpPercap_1972 12269.273780 gdpPercap_1977 14255.984750 gdpPercap_1982 16537.483500 gdpPercap_1987 19207.234820 gdpPercap_1992 22013.644860 gdpPercap_1997 24675.024460 gdpPercap_2002 27968.098170 gdpPercap_2007 28569.719700 Name: Italy, dtype: float64 In [ ]: print(data.loc[\"Albania\", :]) Select multiple columns or rows using DataFrame.loc and a named slice. In [5]: print(data.loc['Italy':'Poland', 'gdpPercap_1962':'gdpPercap_1972']) gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 country Italy 8243.582340 10022.401310 12269.273780 Montenegro 4649.593785 5907.850937 7778.414017 Netherlands 12790.849560 15363.251360 18794.745670 Norway 13450.401510 16361.876470 18965.055510 Poland 5338.752143 6557.152776 8006.506993 In the above code, we discover that slicing using loc is inclusive at both ends , which differs from slicing using iloc , where slicing indicates everything up to but not including the final index. Result of slicing can be used in further operations. \u00b6 {#Result-of-slicing-can-be-used-in-further-operations.} Usually don\u2019t just print a slice. All the statistical operators that work on entire dataframes work the same way on slices. E.g., calculate max of a slice. In [10]: print(data.loc['Italy':'Poland', 'gdpPercap_1962':'gdpPercap_1972'].max()) gdpPercap_1962 13450.40151 gdpPercap_1967 16361.87647 gdpPercap_1972 18965.05551 dtype: float64 In [11]: # Calculate minimum of slice print(data.loc['Italy':'Poland', 'gdpPercap_1962':'gdpPercap_1972'].min()) gdpPercap_1962 4649.593785 gdpPercap_1967 5907.850937 gdpPercap_1972 7778.414017 dtype: float64 Use comparisons to select data based on value. Comparison is applied element by element. Returns a similarly-shaped dataframe of True and False. In [15]: subset = data.loc['Italy':'Poland', 'gdpPercap_1962':'gdpPercap_1972'] #print('Subset of data:\\n', subset) # Which values were greater than 10000 ? print('\\nWhere are values large?\\n', subset > 10000) #Select values or NaN using a Boolean mask. mask = subset > 10000 print(subset[mask]) Where are values large? gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 country Italy False True True Montenegro False False False Netherlands True True True Norway True True True Poland False False False gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 country Italy NaN 10022.40131 12269.27378 Montenegro NaN NaN NaN Netherlands 12790.84956 15363.25136 18794.74567 Norway 13450.40151 16361.87647 18965.05551 Poland NaN NaN NaN Get the value where the mask is true, and NaN (Not a Number) where it is false. Useful because NaNs are ignored by operations like max, min, average, etc. A frame full of Booleans is sometimes called a mask because of how it can be used. In [9]: mask = subset > 10000 print(subset[mask]) gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 country Italy NaN 10022.40131 12269.27378 Montenegro NaN NaN NaN Netherlands 12790.84956 15363.25136 18794.74567 Norway 13450.40151 16361.87647 18965.05551 Poland NaN NaN NaN Get the value where the mask is true, and NaN (Not a Number) where it is false. Useful because NaNs are ignored by operations like max, min, average, etc. Group By: split-apply-combine \u00b6 {#Group-By:-split-apply-combine} Pandas vectorizing methods and grouping operations are features that provide users much flexibility to analyse their data. We may have a glance by splitting the countries in two groups during the years surveyed, those who presented a GDP higher than the European average and those with a lower GDP. We then estimate a wealthy score based on the historical (from 1962 to 2007) values, where we account how many times a country has participated in the groups of lower or higher GDP In [21]: mask_higher = data > data.mean() wealth_score = mask_higher.aggregate('sum', axis=1) / len(data.columns) wealth_score Out[21]: country Albania 0.000000 Austria 1.000000 Belgium 1.000000 Bosnia and Herzegovina 0.000000 Bulgaria 0.000000 Croatia 0.000000 Czech Republic 0.500000 Denmark 1.000000 Finland 1.000000 France 1.000000 Germany 1.000000 Greece 0.333333 Hungary 0.000000 Iceland 1.000000 Ireland 0.333333 Italy 0.500000 Montenegro 0.000000 Netherlands 1.000000 Norway 1.000000 Poland 0.000000 Portugal 0.000000 Romania 0.000000 Serbia 0.000000 Slovak Republic 0.000000 Slovenia 0.333333 Spain 0.333333 Sweden 1.000000 Switzerland 1.000000 Turkey 0.000000 United Kingdom 1.000000 dtype: float64 Note : axis : (default 0) {0 or \u2018index\u2019, 1 or \u2018columns\u2019} 0 or \u2018index\u2019: apply function to each column. 1 or \u2018columns\u2019: apply function to each row. Finally, for each group in the wealth_score table, we sum their (financial) contribution across the years surveyed: In [22]: data.groupby(wealth_score).sum() Out[22]: gdpPercap_1952 gdpPercap_1957 gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 gdpPercap_1977 gdpPercap_1982 gdpPercap_1987 gdpPercap_1992 gdpPercap_1997 gdpPercap_2002 gdpPercap_2007 0.000000 36916.854200 46110.918793 56850.065437 71324.848786 88569.346898 104459.358438 113553.768507 119649.599409 92380.047256 103772.937598 118590.929863 149577.357928 0.333333 16790.046878 20942.456800 25744.935321 33567.667670 45277.839976 53860.456750 59679.634020 64436.912960 67918.093220 80876.051580 102086.795210 122803.729520 0.500000 11807.544405 14505.000150 18380.449470 21421.846200 25377.727380 29056.145370 31914.712050 35517.678220 36310.666080 40723.538700 45564.308390 51403.028210 1.000000 104317.277560 127332.008735 149989.154201 178000.350040 215162.343140 241143.412730 263388.781960 296825.131210 315238.235970 346930.926170 385109.939210 427850.333420 Exercises \u00b6 {#Exercises} Assume Pandas has been imported into your notebook and the Gapminder GDP data for Europe has been loaded: import pandas as pd df = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country') Write an expression to find the Per Capita GDP of Serbia in 2007. In [26]: df = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_europe.csv', index_col='country') print(df.loc['Serbia','gdpPercap_2007']) 9786.534714 In [27]: df.loc[\"Serbia\"][-1] Out[27]: 9786.534714 Explain in simple terms what idxmin and idxmax do in the short program below. When would you use these methods? data = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country') print(data.idxmin()) print(data.idxmax()) In [28]: print(data.idxmin()) gdpPercap_1952 Bosnia and Herzegovina gdpPercap_1957 Bosnia and Herzegovina gdpPercap_1962 Bosnia and Herzegovina gdpPercap_1967 Bosnia and Herzegovina gdpPercap_1972 Bosnia and Herzegovina gdpPercap_1977 Bosnia and Herzegovina gdpPercap_1982 Albania gdpPercap_1987 Albania gdpPercap_1992 Albania gdpPercap_1997 Albania gdpPercap_2002 Albania gdpPercap_2007 Albania dtype: object Key Points \u00b6 {#Key-Points} Use DataFrame.iloc[..., ...] to select values by integer location. Use : on its own to mean all columns or all rows. Select multiple columns or rows using DataFrame.loc and a named slice. Result of slicing can be used in further operations. Use comparisons to select data based on value. Select values or NaN using a Boolean mask. Data prep with Pandas \u00b6 {#Data-prep-with-Pandas} 20 min In [29]: import numpy as np import pandas as pd from numpy.random import randn np.random.seed(101) In [3]: df = pd.DataFrame(randn(5,4),index='A B C D E'.split(),columns='W X Y Z'.split()) df Out[3]: W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 Create new columns In [41]: df['K', :] = df[1,:] + df[1,:] df df.iloc[6,:] = df.iloc[1,:] + df.iloc[1,:] Out[41]: gdpPercap_1952 gdpPercap_1957 gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 gdpPercap_1977 gdpPercap_1982 gdpPercap_1987 gdpPercap_1992 gdpPercap_1997 gdpPercap_2002 gdpPercap_2007 6 country Sweden 8527.844662 9911.878226 12329.441920 15258.296970 17832.02464 18855.725210 20667.381250 23586.929270 23880.016830 25266.594990 29341.630930 33859.748350 NaN Switzerland 14734.232750 17909.489730 20431.092700 22966.144320 27195.11304 26982.290520 28397.715120 30281.704590 31871.530300 32135.323010 34480.957710 37506.419070 NaN Turkey 1969.100980 2218.754257 2322.869908 2826.356387 3450.69638 4269.122326 4241.356344 5089.043686 5678.348271 6601.429915 6508.085718 8458.276384 NaN United Kingdom 9979.508487 11283.177950 12477.177070 14142.850890 15895.11641 17428.748460 18232.424520 21664.787670 22705.092540 26074.531360 29478.999190 33203.261280 NaN 6 12274.152984 17685.196060 21501.442220 25669.204800 33323.25120 39498.844600 43194.167240 47375.652140 54084.037360 58191.841320 64835.215380 72252.985400 NaN Reorder columns in a data frame In [5]: df = df[['newColumn', 'W', 'X', 'Y', 'Z']] df Out[5]: newColumn W X Y Z A 1.131958 2.706850 0.628133 0.907969 0.503826 B 0.286647 0.651118 -0.319318 -0.848077 0.605965 C 0.151122 -2.018168 0.740122 0.528813 -0.589001 D 0.196184 0.188695 -0.758872 -0.933237 0.955057 E 2.662266 0.190794 1.978757 2.605967 0.683509 Group by \u00b6 {#Group-by} The method group-by allow you to group rows in a data frame and apply a function to it. In [65]: #Let's create a DF data = {'Company':['GOOG','GOOG','MSFT','MSFT','FB','FB'], 'Person':['Sam','Charlie','Amy','Vanessa','Carl','Sarah'], 'Sales':[200,120,340,124,243,350]} df = pd.DataFrame(data) print(df) #Group by company by_comp = df.groupby(\"Company\") #by_comp # Try some functions by_comp.mean() by_comp.count() by_comp.describe() by_comp.describe().transpose() Company Person Sales 0 GOOG Sam 200 1 GOOG Charlie 120 2 MSFT Amy 340 3 MSFT Vanessa 124 4 FB Carl 243 5 FB Sarah 350 Out[65]: Company FB GOOG MSFT Sales count 2.000000 2.000000 2.000000 mean 296.500000 160.000000 232.000000 std 75.660426 56.568542 152.735065 min 243.000000 120.000000 124.000000 25% 269.750000 140.000000 178.000000 50% 296.500000 160.000000 232.000000 75% 323.250000 180.000000 286.000000 max 350.000000 200.000000 340.000000 We can also merge data from different dataframes. It's very useful when we need a variable from a different file. You can use a \u2018left\u2019, \u2018right\u2019, \u2018outer\u2019, \u2018inner\u2019 Taken from In [56]: left = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'], 'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3'], 'C': ['C0', 'C1', 'C2', 'C3']}) right = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']}) print(left) print(right) ## Merge pd.merge(left, right, how='outer', on=['key']) A B C key 0 A0 B0 C0 K0 1 A1 B1 C1 K1 2 A2 B2 C2 K2 3 A3 B3 C3 K3 C D key 0 C0 D0 K0 1 C1 D1 K1 2 C2 D2 K2 3 C3 D3 K3 Out[56]: A B C_x key C_y D 0 A0 B0 C0 K0 C0 D0 1 A1 B1 C1 K1 C1 D1 2 A2 B2 C2 K2 C2 D2 3 A3 B3 C3 K3 C3 D3 Join (union) \u00b6 {#Join-(union)} In [58]: left = pd.DataFrame({'A': ['A0', 'A1', 'A2'], 'B': ['B0', 'B1', 'B2']}, index=['K0', 'K1', 'K2']) right = pd.DataFrame({'C': ['C0', 'C2', 'C3'], 'D': ['D0', 'D2', 'D3']}, index=['K0', 'K2', 'K3']) In [59]: left.join(right) Out[59]: A B C D K0 A0 B0 C0 D0 K1 A1 B1 NaN NaN K2 A2 B2 C2 D2 In [60]: right.join(left) Out[60]: C D A B K0 C0 D0 A0 B0 K2 C2 D2 A2 B2 K3 C3 D3 NaN NaN In [61]: left.join(right, how='outer') Out[61]: A B C D K0 A0 B0 C0 D0 K1 A1 B1 NaN NaN K2 A2 B2 C2 D2 K3 NaN NaN C3 D3 Some additional operations you can use with a pandas data frame unique: returns unique values in a series. nunique: returns the number of distinct observations over requested axis. value_counts: returns an object containing counts of unique values in sorted order. In [62]: df['Company'].unique() Out[62]: array(['GOOG', 'MSFT', 'FB'], dtype=object) In [63]: df['Company'].nunique() Out[63]: 3 In [20]: df['Company'].value_counts() Out[20]: FB 2 GOOG 2 MSFT 2 Name: Company, dtype: int64 There are some other very useful tricks you can do with pandas data frames. Such as profiling a dataframe. Profiling df.profile_report() is a simple and easy way to go furhter into knowing your data. Some other tips and tricks In [0]: #Install #pip install pandas-profiling In [0]: uploaded = files.upload() In [0]: import pandas as pd import pandas_profiling import io data = pd.read_csv(io.BytesIO(uploaded['gapminder_ In [0]: print(data.iloc[:,1:3]) Note: There are many other method we can use to explore the data and more effective exploration of a data set with pandas profiling. Check this out! In [0]: pandas_profiling.ProfileReport(data.iloc[:,0:6]) Some other useful tools to work with data frames \u00b6 {#Some-other-useful-tools-to-work-with-data-frames} When you are working with large data frames you might want to know if there are missing values and how many are there. .isna() will create a table with booleans. True if a value is NaN In [67]: df.isna().head() Out[67]: Company Person Sales 0 False False False 1 False False False 2 False False False 3 False False False 4 False False False You can count how many Nan values you have per variable In [68]: df.isna().sum() Out[68]: Company 0 Person 0 Sales 0 dtype: int64 In [69]: df1 = df.copy() You can discard these values In [71]: df.dropna(axis=0) #for rows df.dropna(axis= 1) #for columns Out[71]: Company Person Sales 0 GOOG Sam 200 1 GOOG Charlie 120 2 MSFT Amy 340 3 MSFT Vanessa 124 4 FB Carl 243 5 FB Sarah 350 Standardize and resize data directly in the dataframe \u00b6 {#Standardize-and-resize-data-directly-in-the-dataframe} Here we can do it manually (if like to do things like that) but we can also use methods already created. For example ScikitLearn provides: Simple and efficient tools for data mining and data analysis Accessible to everybody, and reusable in various contexts Built on NumPy, SciPy, and matplotlib The sklearn.preprocessing package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators. In general, learning algorithms benefit from standardization of the data set. If some outliers are present in the set, robust scalers or transformers are more appropriate. In [73]: from sklearn import preprocessing #Save columns names names = data.iloc[:,2:8].columns #Create scaler scaler = preprocessing.MinMaxScaler() #StandardScaler() #MaxAbsScaler #Transform your data frame (numeric variables ) data1 = data.iloc[:,2:8] data1 = scaler.fit_transform(data1) data1 = pd.DataFrame(data1, columns=names) print(data1.head()) print(data.iloc[:,2:8].head()) gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 gdpPercap_1977 \\ 0 0.032220 0.028270 0.018626 0.000193 1 0.482925 0.512761 0.567146 0.691612 2 0.495771 0.527883 0.567578 0.664689 3 0.000000 0.000000 0.000000 0.000000 4 0.135922 0.163734 0.153579 0.174119 gdpPercap_1982 gdpPercap_1987 0 0.000000 0.000000 1 0.725414 0.717533 2 0.700492 0.675728 3 0.020016 0.020688 4 0.185462 0.161892 gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 \\ country Albania 2312.888958 2760.196931 3313.422188 Austria 10750.721110 12834.602400 16661.625600 Belgium 10991.206760 13149.041190 16672.143560 Bosnia and Herzegovina 1709.683679 2172.352423 2860.169750 Bulgaria 4254.337839 5577.002800 6597.494398 gdpPercap_1977 gdpPercap_1982 gdpPercap_1987 country Albania 3533.003910 3630.880722 3738.932735 Austria 19749.422300 21597.083620 23687.826070 Belgium 19117.974480 20979.845890 22525.563080 Bosnia and Herzegovina 3528.481305 4126.613157 4314.114757 Bulgaria 7612.240438 8224.191647 8239.854824 Exercise \u00b6 {#Exercise} With the file gapminder_all.csv try to: Filter only those countries located in Latin America. Select the columns corresponding to the gdpPercap and the population Explore the data frame using 3 different methods 4 Show how many contries had a gdpPercap higher than the mean in 1977. Check if there are some missing values (NaN) in the data Lists \u00b6 {#Lists} 15 min Exercises (10 min) A list stores many values in a single structure. Doing calculations with a hundred variables called pressure_001, pressure_002, etc., would be at least as slow as doing them by hand. Use a list to store many values together. Contained within square brackets [...]. Values separated by commas ,. Use len to find out how many values are in a list. In [74]: pressures = [0.273, 0.275, 0.277, 0.275, 0.276] print('pressures:', pressures) print('length:', len(pressures)) pressures: [0.273, 0.275, 0.277, 0.275, 0.276] length: 5 Use an item\u2019s index to fetch it from a list. In [22]: print('zeroth item of pressures:', pressures[0]) zeroth item of pressures: 0.273 Lists\u2019 values can be replaced by assigning to them. In [23]: pressures[0] = 0.265 print('pressures is now:', pressures) pressures is now: [0.265, 0.275, 0.277, 0.275, 0.276] Use list_name.append to add items to the end of a list. In [75]: primes = [2, 3, 5] print('primes is initially:', primes) primes.append(7) #primes.append(9) #print('primes has become:', primes) primes is initially: [2, 3, 5] Use del to remove items from a list entirely. In [76]: primes = [2, 3, 5, 7, 9] print('primes before removing last item:', primes) del primes[4] print('primes after removing last item:', primes) primes before removing last item: [2, 3, 5, 7, 9] primes after removing last item: [2, 3, 5, 7] The empty list contains no values. Use [ ] on its own to represent a list that doesn\u2019t contain any values. Lists may contain values of different types. In [26]: goals = [1, 'Create lists.', 2, 'Extract items from lists.', 3, 'Modify lists.'] Character strings can be indexed like lists. In [27]: element = 'carbon' print('zeroth character:', element[0]) print('third character:', element[3]) zeroth character: c third character: b Character strings are immutable. Cannot change the characters in a string after it has been created. Immutable: can\u2019t be changed after creation. In contrast, lists are mutable: they can be modified in place. Python considers the string to be a single value with parts, not a collection of values. In [28]: element[0] = 'C' --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-28-6dc46761ce07> in <module>() ----> 1 element[0] = 'C' TypeError: 'str' object does not support item assignment Exercises \u00b6 {#Exercises} Given this: print('string to list:', list('tin')) print('list to string:', ''.join(['g', 'o', 'l', 'd'])) What does list('some string') do? What does '-'.join(['x', 'y', 'z']) generate? In [77]: print(list('YaQueremosComer')) ['Y', 'a', 'Q', 'u', 'e', 'r', 'e', 'm', 'o', 's', 'C', 'o', 'm', 'e', 'r'] What does the following program print? element = 'helium' print(element[-1]) How does Python interpret a negative index? If a list or string has N elements, what is the most negative index that can safely be used with it, and what location does that index represent? If values is a list, what does del values[-1] do? How can you display all elements but the last one without changing values? (Hint: you will need to combine slicing and negative indexing.) What does the following program print? element = 'fluorine' print(element[::2]) print(element[::-1]) If we write a slice as low:high:stride, what does stride do? What expression would select all of the even-numbered items from a collection? Key Points \u00b6 {#Key-Points} A list stores many values in a single structure. Use an item\u2019s index to fetch it from a list. Lists\u2019 values can be replaced by assigning to them. Appending items to a list lengthens it. Use del to remove items from a list entirely. The empty list contains no values. Lists may contain values of different types. Character strings can be indexed like lists. Character strings are immutable. Indexing beyond the end of the collection is an error. For Loops \u00b6 {#For-Loops} 10 min Exercises (15 min) A for loop executes commands once for each value in a collection. \u201cfor each thing in this group, do these operations\u201d In [78]: for number in [2, 3, 5]: print(number) 2 3 5 This for loop is equivalent to: In [30]: print(2) print(3) print(5) 2 3 5 A for loop is made up of a collection, loop variable and a body. Parts of a for loop The collection, [2, 3, 5], is what the loop is being run on. The body, print(number), specifies what to do for each value in the collection. The loop variable, number, is what changes for each iteration of the loop. The \u201ccurrent thing\u201d. Python uses indentation rather than {} or begin/end to show nesting. Use range to iterate over a sequence of numbers. The first line of the for loop must end with a colon, and the body must be indented. The colon at the end of the first line signals the start of a block of statements. Python uses indentation rather than {} or begin/end to show nesting. Any consistent indentation is legal, but almost everyone uses four spaces. In [80]: for number in [2, 3, 5]: print(number) 2 3 5 Indentation is always meaningful in Python. In [81]: firstName = \"Jon\" lastName = \"Smith\" File \"<ipython-input-81-6966a7c3a64d>\", line 2 lastName = \"Smith\" ^ IndentationError: unexpected indent Loop variables can be called anything. As with all variables, loop variables are: Created on demand. Meaningless: their names can be anything at all. In [33]: for kitten in [2, 3, 5]: print(kitten) 2 3 5 The body of a loop can contain many statements. But no loop should be more than a few lines long. Hard for human beings to keep larger chunks of code in mind. In [82]: primes = [2, 3, 5] for p in primes: squared = p ** 2 cubed = p ** 3 print(p, squared, cubed) 2 4 8 3 9 27 5 25 125 Use range to iterate over a sequence of numbers. The built-in function range produces a sequence of numbers. Not a list: the numbers are produced on demand to make looping over large ranges more efficient. range(N) is the numbers 0..N-1 Exactly the legal indices of a list or character string of length N In [83]: print('a range is not a list: range(0, 3)') for number in range(0, 3): print(number) a range is not a list: range(0, 3) 0 1 2 The Accumulator pattern turns many values into one. Initialize an accumulator variable to zero, the empty string, or the empty list. In [86]: total = 0 for number in range(10): total = total + (number + 1) print(total) 1 3 6 10 15 21 28 36 45 55 Exercises \u00b6 {#Exercises} Create a table showing the numbers of the lines that are executed when this program runs, and the values of the variables after each line is executed. total = 0 for char in \"tin\": total = total + 1 Fill in the blanks in the program below so that it prints \u201cnit\u201d (the reverse of the original character string \u201ctin\u201d). original = \"tin\" result = ____ for char in original: result = ____ print(result) In [0]: original = \"tin\" result = \"\" for char in original: result = char + result print(result) t it nit Fill in the blanks in each of the programs below to produce the indicated result. In [0]: # Total length of the strings in the list: [\"red\", \"green\", \"blue\"] => 12 total = 0 for word in [\"red\", \"green\", \"blue\"]: ____ = ____ + len(word) print(total) In [87]: # List of word lengths: [\"red\", \"green\", \"blue\"] => [3, 5, 4] lengths = [] for word in [\"red\", \"green\", \"blue\"]: lengths.append(len(word)) print(lengths) [3, 5, 4] In [0]: # Concatenate all words: [\"red\", \"green\", \"blue\"] => \"redgreenblue\" words = [\"red\", \"green\", \"blue\"] result = ____ for ____ in ____: ____ print(result) In [0]: # Create acronym: [\"red\", \"green\", \"blue\"] => \"RGB\" # write the whole thing Find the error to the following code students = ['Ana', 'Juan', 'Susan'] for m in students: print(m) Cumulative sum. Reorder and properly indent the lines of code below so that they print a list with the cumulative sum of data. The result should be [1, 3, 5, 10]. In [0]: cumulative.append(sum) for number in data: cumulative = [] sum += number sum = 0 print(cumulative) data = [1,2,2,5] Key Points \u00b6 {#Key-Points} A for loop executes commands once for each value in a collection. A for loop is made up of a collection, a loop variable, and a body. The first line of the for loop must end with a colon, and the body must be indented. Indentation is always meaningful in Python. Loop variables can be called anything (but it is strongly advised to have a meaningful name to the looping variable). The body of a loop can contain many statements. Use range to iterate over a sequence of numbers. The Accumulator pattern turns many values into one. Looping Over Data Sets \u00b6 {#Looping-Over-Data-Sets} 5 min Exercises (10 min) Use a for loop to process files given a list of their names. In [88]: import pandas as pd for filename in ['/home/mcubero/dataSanJose19/data/gapminder_gdp_africa.csv', '/home/mcubero/dataSanJose19/data/gapminder_gdp_asia.csv']: data = pd.read_csv(filename, index_col='country') print(filename, data.min()) /home/mcubero/dataSanJose19/data/gapminder_gdp_africa.csv gdpPercap_1952 298.846212 gdpPercap_1957 335.997115 gdpPercap_1962 355.203227 gdpPercap_1967 412.977514 gdpPercap_1972 464.099504 gdpPercap_1977 502.319733 gdpPercap_1982 462.211415 gdpPercap_1987 389.876185 gdpPercap_1992 410.896824 gdpPercap_1997 312.188423 gdpPercap_2002 241.165877 gdpPercap_2007 277.551859 dtype: float64 /home/mcubero/dataSanJose19/data/gapminder_gdp_asia.csv gdpPercap_1952 331.0 gdpPercap_1957 350.0 gdpPercap_1962 388.0 gdpPercap_1967 349.0 gdpPercap_1972 357.0 gdpPercap_1977 371.0 gdpPercap_1982 424.0 gdpPercap_1987 385.0 gdpPercap_1992 347.0 gdpPercap_1997 415.0 gdpPercap_2002 611.0 gdpPercap_2007 944.0 dtype: float64 Use glob.glob to find sets of files whose names match a pattern. In Unix, the term \u201cglobbing\u201d means \u201cmatching a set of files with a pattern\u201d. '*' meaning \u201cmatch zero or more characters\u201d Python contains the glob library to provide pattern matching functionality. In [90]: import glob print('all csv files in data directory:', glob.glob('/home/mcubero/dataSanJose19/data/*.csv')) all csv files in data directory: ['/home/mcubero/dataSanJose19/data/gapminder_all.csv', '/home/mcubero/dataSanJose19/data/gapminder_gdp_africa.csv', '/home/mcubero/dataSanJose19/data/gapminder_gdp_americas.csv', '/home/mcubero/dataSanJose19/data/gapminder_gdp_asia.csv', '/home/mcubero/dataSanJose19/data/gapminder_gdp_europe.csv', '/home/mcubero/dataSanJose19/data/gapminder_gdp_oceania.csv', '/home/mcubero/dataSanJose19/data/processed.csv'] In [91]: print('all PDB files:', glob.glob('*.pdb')) all PDB files: [] Use glob and for to process batches of files. In [92]: for filename in glob.glob('/home/mcubero/dataSanJose19/data/gapminder_*.csv'): data = pd.read_csv(filename) print(filename, data['gdpPercap_1952'].min()) /home/mcubero/dataSanJose19/data/gapminder_all.csv 298.8462121 /home/mcubero/dataSanJose19/data/gapminder_gdp_africa.csv 298.8462121 /home/mcubero/dataSanJose19/data/gapminder_gdp_americas.csv 1397.7171369999999 /home/mcubero/dataSanJose19/data/gapminder_gdp_asia.csv 331.0 /home/mcubero/dataSanJose19/data/gapminder_gdp_europe.csv 973.5331947999999 /home/mcubero/dataSanJose19/data/gapminder_gdp_oceania.csv 10039.595640000001 Exercises \u00b6 {#Exercises} Which of these files is not matched by the expression glob.glob('data/ as .csv')? data/gapminder_gdp_africa.csv data/gapminder_gdp_americas.csv data/gapminder_gdp_asia.csv 1 and 2 are not matched. Key Points \u00b6 {#Key-Points} Use a for loop to process files given a list of their names. Use glob.glob to find sets of files whose names match a pattern. Use glob and for to process batches of files. STRETCHING TIME {#STRETCHING-TIME!} Writing functions \u00b6 {#Writing-functions} 15 min Exercises (20 min) Break programs down into functions to make them easier to understand. Human beings can only keep a few items in working memory at a time. Encapsulate complexity so that we can treat it as a single \u201cthing\u201d. Write one time, use many times. To define a function use def then the name of the function like this: def say_hi(parameter1, parameter2): print('Hello') Remember, defining a function does not run it, you must call the function to execute it. In [93]: def print_date(year, month, day): joined = str(year) + '/' + str(month) + '/' + str(day) print(joined) print_date(1871, 3, 19) 1871/3/19 In [43]: print_date(month=3, day=19, year=1871) 1871/3/19 Use return ... to give a value back to the caller. May occur anywhere in the function. In [94]: def average(values): if len(values) == 0: return None return sum(values) / len(values) Remember: every function returns something A function that doesn\u2019t explicitly return a value automatically returns None. Exercises \u00b6 {#Exercises} What does the following program print? def report(pressure): print('pressure is', pressure) print('calling', report, 22.5) In [96]: def report(pressure): print('pressure is', pressure) print('calling', report(22.5)) pressure is 22.5 calling None Fill in the blanks to create a function that takes a single filename as an argument, loads the data in the file named by the argument, and returns the minimum value in that data. import pandas as pd def min_in_data(____): data = ____ return ____ In [98]: import pandas as pd def min_in_data(data): data = pd.read_csv(data) return data.min() min_in_data('/home/mcubero/dataSanJose19/data/gapminder_gdp_africa.csv') Out[98]: country Algeria gdpPercap_1952 298.846 gdpPercap_1957 335.997 gdpPercap_1962 355.203 gdpPercap_1967 412.978 gdpPercap_1972 464.1 gdpPercap_1977 502.32 gdpPercap_1982 462.211 gdpPercap_1987 389.876 gdpPercap_1992 410.897 gdpPercap_1997 312.188 gdpPercap_2002 241.166 gdpPercap_2007 277.552 dtype: object The code below will run on a label-printer for chicken eggs. A digital scale will report a chicken egg mass (in grams) to the computer and then the computer will print a label. Please re-write the code so that the if-block is folded into a function. import random for i in range(10): # simulating the mass of a chicken egg # the (random) mass will be 70 +/- 20 grams mass=70+20.0*(2.0*random.random()-1.0) print(mass) #egg sizing machinery prints a label if(mass>=85): print(\"jumbo\") elif(mass>=70): print(\"large\") elif(mass<70 and mass>=55): print(\"medium\") else: print(\"small\") Assume that the following code has been executed: In [46]: import pandas as pd df = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_asia.csv', index_col=0) japan = df.loc['Japan'] japan Out[46]: gdpPercap_1952 3216.956347 gdpPercap_1957 4317.694365 gdpPercap_1962 6576.649461 gdpPercap_1967 9847.788607 gdpPercap_1972 14778.786360 gdpPercap_1977 16610.377010 gdpPercap_1982 19384.105710 gdpPercap_1987 22375.941890 gdpPercap_1992 26824.895110 gdpPercap_1997 28816.584990 gdpPercap_2002 28604.591900 gdpPercap_2007 31656.068060 Name: Japan, dtype: float64 1.Complete the statements below to obtain the average GDP for Japan across the years reported for the 1980s. year = 1983 gdp_decade = 'gdpPercap_' + str(year // ____) avg = (japan.loc[gdp_decade + ___] + japan.loc[gdp_decade + ___]) / 2 2.Abstract the code above into a single function. def avg_gdp_in_decade(country, continent, year): df = pd.read_csv('data/gapminder_gdp_'+___+'.csv',delimiter=',',index_col=0) ____ ____ ____ return avg .How would you generalize this function if you did not know beforehand which specific years occurred as columns in the data? For instance, what if we also had data from years ending in 1 and 9 for each decade? (Hint: use the columns to filter out the ones that correspond to the decade, instead of enumerating them in the code.) Key Points \u00b6 {#Key-Points} Break programs down into functions to make them easier to understand. Define a function using def with a name, parameters, and a block of code. Defining a function does not run it. Arguments in call are matched to parameters in definition. Functions may return a result to their caller using return. Variable Scope \u00b6 {#Variable-Scope} 10 min Exercise (10 min) The scope of a variable is the part of a program that can \u2018see\u2019 that variable. There are only so many sensible names for variables. People using functions shouldn\u2019t have to worry about what variable names the author of the function used. People writing functions shouldn\u2019t have to worry about what variable names the function\u2019s caller uses. The part of a program in which a variable is visible is called its scope. In [99]: pressure = 103.9 def adjust(t): temperature = t * 1.43 / pressure return temperature pressure is a global variable. Defined outside any particular function. Visible everywhere. t and temperature are local variables in adjust. Defined in the function. Not visible in the main program. Remember: a function parameter is a variable that is automatically assigned a value when the function is called. In [100]: print('adjusted:', adjust(0.9)) print('temperature after call:', temperature) adjusted: 0.01238691049085659 ---------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-100-e73c01f89950> in <module>() 1 print('adjusted:', adjust(0.9)) ----> 2 print('temperature after call:', temperature) NameError: name 'temperature' is not defined Exercises \u00b6 {#Exercises} Trace the values of all variables in this program as it is executed. (Use \u2018\u2014\u2019 as the value of variables before and after they exist.) limit = 100 def clip(value): return min(max(0.0, value), limit) value = -22.5 print(clip(value)) Read the traceback below, and identify the following: How many levels does the traceback have? What is the file name where the error occurred? What is the function name where the error occurred? On which line number in this function did the error occur? What is the type of error? What is the error message? --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-2-e4c4cbafeeb5> in <module>() 1 import errors_02 ----> 2 errors_02.print_friday_message() /Users/ghopper/thesis/code/errors_02.py in print_friday_message() 13 14 def print_friday_message(): ---> 15 print_message(\"Friday\") /Users/ghopper/thesis/code/errors_02.py in print_message(day) 9 \"sunday\": \"Aw, the weekend is almost over.\" 10 } ---> 11 print(messages[day]) 12 13 KeyError: 'Friday' Key Points \u00b6 {#Key-Points} The scope of a variable is the part of a program that can \u2018see\u2019 that variable. Conditionals \u00b6 {#Conditionals} 15 min Exercise (15 min) Use if statements to control whether or not a block of code is executed. An if statement (more properly called a conditional statement) controls whether some block of code is executed or not. Structure is similar to a for statement: First line opens with if and ends with a colon Body containing one or more statements is indented (usually by 4 spaces) In [52]: mass = 2.07 if mass > 3.0: print (mass, 'is large') In [102]: masses = [3.54, 2.07, 9.22, 1.86, 1.71] for m in masses: if m > 3.0: print(m, 'is large') else: print(m, 'is small') 3.54 is large 2.07 is small 9.22 is large 1.86 is small 1.71 is small In [104]: thing1 = [3.54, 2.07, 9.22] if masses > thing1: print (masses, 'is large') [3.54, 2.07, 9.22, 1.86, 1.71] is large Conditionals are often used inside loops. Not much point using a conditional when we know the value (as above). But useful when we have a collection to process. In [54]: masses = [3.54, 2.07, 9.22, 1.86, 1.71] for m in masses: if m > 3.0: print(m, 'is large') 3.54 is large 9.22 is large Use else to execute a block of code when an if condition is not true. else can be used following an if. Allows us to specify an alternative to execute when the if branch isn\u2019t taken. In [55]: masses = [3.54, 2.07, 9.22, 1.86, 1.71] for m in masses: if m > 3.0: print(m, 'is large') else: print(m, 'is small') 3.54 is large 2.07 is small 9.22 is large 1.86 is small 1.71 is small Use elif to specify additional tests. May want to provide several alternative choices, each with its own test. Use elif (short for \u201celse if\u201d) and a condition to specify these. Always associated with an if. Must come before the else (which is the \u201ccatch all\u201d). Complete the next conditional In [56]: masses = [3.54, 2.07, 9.22, 1.86, 1.71] for m in ____: if m > 9.0: print(__, 'is HUGE') elif m > 3.0: print(m, 'is large') ___: print(m, 'is small') File \"<ipython-input-56-97e8fa260561>\", line 7 ___: ^ SyntaxError: invalid syntax Conditions are tested once, in order. Python steps through the branches of the conditional in order, testing each in turn. So ordering matters. In [0]: grade = 85 if grade >= 70: print('grade is C') elif grade >= 80: print('grade is B') elif grade >= 90: print('grade is A') grade is C Does not automatically go back and re-evaluate if values change. In [0]: velocity = 10.0 if velocity > 20.0: print('moving too fast') else: print('adjusting velocity') velocity = 50.0 Often use conditionals in a loop to \u201cevolve\u201d the values of variables. In [105]: velocity = 10.0 for i in range(5): # execute the loop 5 times print(i, ':', velocity) if velocity > 20.0: print('moving too fast') velocity = velocity - 5.0 else: print('moving too slow') velocity = velocity + 10.0 print('final velocity:', velocity) 0 : 10.0 moving too slow 1 : 20.0 moving too slow 2 : 30.0 moving too fast 3 : 25.0 moving too fast 4 : 20.0 moving too slow final velocity: 30.0 Conditionals are useful to check for errors! Often, you want some combination of things to be true. You can combine relations within a conditional using and and or. Continuing the example above, suppose you have In [0]: mass = [ 3.54, 2.07, 9.22, 1.86, 1.71] velocity = [10.00, 20.00, 30.00, 25.00, 20.00] i = 0 for i in range(5): if mass[i] > 5 and velocity[i] > 20: print(\"Fast heavy object. Duck!\") elif mass[i] > 2 and mass[i] <= 5 and velocity[i] <= 20: print(\"Normal traffic\") elif mass[i] <= 2 and velocity[i] <= 20: print(\"Slow light object. Ignore it\") else: print(\"Whoa! Something is up with the data. Check it\") Normal traffic Normal traffic Fast heavy object. Duck! Whoa! Something is up with the data. Check it Slow light object. Ignore it Just like with arithmetic, you can and should use parentheses whenever there is possible ambiguity. A good general rule is to always use parentheses when mixing and and or in the same condition. That is, instead of: In [0]: if mass[i] <= 2 or mass[i] >= 5 and velocity[i] > 20: write one of these: In [0]: if (mass[i] <= 2 or mass[i] >= 5) and velocity[i] > 20: if mass[i] <= 2 or (mass[i] >= 5 and velocity[i] > 20): so it is perfectly clear to a reader (and to Python) what you really mean. Exercise \u00b6 {#Exercise} What does this program print? pressure = 71.9 if pressure > 50.0: pressure = 25.0 elif pressure <= 50.0: pressure = 0.0 print(pressure) In [106]: pressure = 71.9 if pressure > 50.0: pressure = 25.0 elif pressure <= 50.0: pressure = 0.0 print(pressure) 25.0 Trimming Values Fill in the blanks so that this program creates a new list containing zeroes where the original list\u2019s values were negative and ones where the original list\u2019s values were positive. In [107]: original = [-1.5, 0.2, 0.4, 0.0, -1.3, 0.4] result = [] for value in original: if value < 0.0: result.append(0) else: result.append(1) print(result) [0, 1, 1, 1, 0, 1] Modify this program so that it only processes files with fewer than 50 records. In [108]: import glob import pandas as pd for filename in glob.glob('/home/mcubero/dataSanJose19/data/*.csv'): contents = pd.read_csv(filename) if len(contents) < 50: print(filename, len(contents)) /home/mcubero/dataSanJose19/data/gapminder_gdp_americas.csv 25 /home/mcubero/dataSanJose19/data/gapminder_gdp_asia.csv 33 /home/mcubero/dataSanJose19/data/gapminder_gdp_europe.csv 30 /home/mcubero/dataSanJose19/data/gapminder_gdp_oceania.csv 2 /home/mcubero/dataSanJose19/data/processed.csv 2 Modify this program so that it finds the largest and smallest values in the list no matter what the range of values originally is. values = [...some test data...] smallest, largest = None, None for v in values: if ____: smallest, largest = v, v ____: smallest = min(____, v) largest = max(____, v) print(smallest, largest) What are the advantages and disadvantages of using this method to find the range of the data? Using functions with conditionals in Pandas Functions will often contain conditionals. Here is a short example that will indicate which quartile the argument is in based on hand-coded values for the quartile cut points. In [9]: def calculate_life_quartile(exp): if exp < 58.41: # This observation is in the first quartile return 1 elif exp >= 58.41 and exp < 67.05: # This observation is in the second quartile return 2 elif exp >= 67.05 and exp < 71.70: # This observation is in the third quartile return 3 elif exp >= 71.70: # This observation is in the fourth quartile return 4 else: # This observation has bad data return None calculate_life_quartile(62.5) Out[9]: 2 That function would typically be used within a for loop, but Pandas has a different, more efficient way of doing the same thing, and that is by applying a function to a dataframe or a portion of a dataframe. Here is an example, using the definition above. In [59]: data = pd.read_csv('/home/mcubero/dataSanJose19/data/all-Americas.csv') data #data['life_qrtl'] = data['lifeExp'].apply(calculate_life_quartile) Out[59]: continent country gdpPercap_1952 gdpPercap_1957 gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 gdpPercap_1977 gdpPercap_1982 gdpPercap_1987 gdpPercap_1992 gdpPercap_1997 gdpPercap_2002 gdpPercap_2007 0 Americas Argentina 5911.315053 6856.856212 7133.166023 8052.953021 9443.038526 10079.026740 8997.897412 9139.671389 9308.418710 10967.281950 8797.640716 12779.379640 1 Americas Bolivia 2677.326347 2127.686326 2180.972546 2586.886053 2980.331339 3548.097832 3156.510452 2753.691490 2961.699694 3326.143191 3413.262690 3822.137084 2 Americas Brazil 2108.944355 2487.365989 3336.585802 3429.864357 4985.711467 6660.118654 7030.835878 7807.095818 6950.283021 7957.980824 8131.212843 9065.800825 3 Americas Canada 11367.161120 12489.950060 13462.485550 16076.588030 18970.570860 22090.883060 22898.792140 26626.515030 26342.884260 28954.925890 33328.965070 36319.235010 4 Americas Chile 3939.978789 4315.622723 4519.094331 5106.654313 5494.024437 4756.763836 5095.665738 5547.063754 7596.125964 10118.053180 10778.783850 13171.638850 5 Americas Colombia 2144.115096 2323.805581 2492.351109 2678.729839 3264.660041 3815.807870 4397.575659 4903.219100 5444.648617 6117.361746 5755.259962 7006.580419 6 Americas Costa Rica 2627.009471 2990.010802 3460.937025 4161.727834 5118.146939 5926.876967 5262.734751 5629.915318 6160.416317 6677.045314 7723.447195 9645.061420 7 Americas Cuba 5586.538780 6092.174359 5180.755910 5690.268015 5305.445256 6380.494966 7316.918107 7532.924763 5592.843963 5431.990415 6340.646683 8948.102923 8 Americas Dominican Republic 1397.717137 1544.402995 1662.137359 1653.723003 2189.874499 2681.988900 2861.092386 2899.842175 3044.214214 3614.101285 4563.808154 6025.374752 9 Americas Ecuador 3522.110717 3780.546651 4086.114078 4579.074215 5280.994710 6679.623260 7213.791267 6481.776993 7103.702595 7429.455877 5773.044512 6873.262326 10 Americas El Salvador 3048.302900 3421.523218 3776.803627 4358.595393 4520.246008 5138.922374 4098.344175 4140.442097 4444.231700 5154.825496 5351.568666 5728.353514 11 Americas Guatemala 2428.237769 2617.155967 2750.364446 3242.531147 4031.408271 4879.992748 4820.494790 4246.485974 4439.450840 4684.313807 4858.347495 5186.050003 12 Americas Haiti 1840.366939 1726.887882 1796.589032 1452.057666 1654.456946 1874.298931 2011.159549 1823.015995 1456.309517 1341.726931 1270.364932 1201.637154 13 Americas Honduras 2194.926204 2220.487682 2291.156835 2538.269358 2529.842345 3203.208066 3121.760794 3023.096699 3081.694603 3160.454906 3099.728660 3548.330846 14 Americas Jamaica 2898.530881 4756.525781 5246.107524 6124.703451 7433.889293 6650.195573 6068.051350 6351.237495 7404.923685 7121.924704 6994.774861 7320.880262 15 Americas Mexico 3478.125529 4131.546641 4581.609385 5754.733883 6809.406690 7674.929108 9611.147541 8688.156003 9472.384295 9767.297530 10742.440530 11977.574960 16 Americas Nicaragua 3112.363948 3457.415947 3634.364406 4643.393534 4688.593267 5486.371089 3470.338156 2955.984375 2170.151724 2253.023004 2474.548819 2749.320965 17 Americas Panama 2480.380334 2961.800905 3536.540301 4421.009084 5364.249663 5351.912144 7009.601598 7034.779161 6618.743050 7113.692252 7356.031934 9809.185636 18 Americas Paraguay 1952.308701 2046.154706 2148.027146 2299.376311 2523.337977 3248.373311 4258.503604 3998.875695 4196.411078 4247.400261 3783.674243 4172.838464 19 Americas Peru 3758.523437 4245.256698 4957.037982 5788.093330 5937.827283 6281.290855 6434.501797 6360.943444 4446.380924 5838.347657 5909.020073 7408.905561 20 Americas Puerto Rico 3081.959785 3907.156189 5108.344630 6929.277714 9123.041742 9770.524921 10330.989150 12281.341910 14641.587110 16999.433300 18855.606180 19328.709010 21 Americas Trinidad and Tobago 3023.271928 4100.393400 4997.523971 5621.368472 6619.551419 7899.554209 9119.528607 7388.597823 7370.990932 8792.573126 11460.600230 18008.509240 22 Americas United States 13990.482080 14847.127120 16173.145860 19530.365570 21806.035940 24072.632130 25009.559140 29884.350410 32003.932240 35767.433030 39097.099550 42951.653090 23 Americas Uruguay 5716.766744 6150.772969 5603.357717 5444.619620 5703.408898 6504.339663 6920.223051 7452.398969 8137.004775 9230.240708 7727.002004 10611.462990 24 Americas Venezuela 7689.799761 9802.466526 8422.974165 9541.474188 10505.259660 13143.950950 11152.410110 9883.584648 10733.926310 10165.495180 8605.047831 11415.805690 There is a lot in that second line, so let\u2019s take it piece by piece. On the right side of the = we start with data['lifeExp'], which is the column in the dataframe called data labeled lifExp. We use the apply() to do what it says, apply the calculate_life_quartile to the value of this column for every row in the dataframe. Key Points \u00b6 {#Key-Points} Use if statements to control whether or not a block of code is executed. Conditionals are often used inside loops. Use else to execute a block of code when an if condition is not true. Use elif to specify additional tests. Conditions are tested once, in order. Create a table showing variables\u2019 values to trace a program\u2019s execution. Plotting \u00b6 {#Plotting} 20 min Exercises (15 min) We are going to use matplotlib. matplotlib is the most widely used scientific plotting library in Python. Commonly use a sub-library called matplotlib.pyplot. The Jupyter Notebook will render plots inline if we ask it to using a \u201cmagic\u201d command. In [61]: #%matplotlib inline import matplotlib.pyplot as plt Simple plots are then (fairly) simple to create In [62]: time = [0, 1, 2, 3] position = [0, 100, 200, 300] plt.plot(time, position) plt.xlabel('Time (hr)') plt.ylabel('Position (km)') Out[62]: Text(0,0.5,'Position (km)') Plot data directly from a Pandas dataframe \u00b6 {#Plot-data-directly-from-a-Pandas-dataframe} We can also plot Pandas dataframes. This implicitly uses matplotlib.pyplot. Before plotting, we convert the column headings from a string to integer data type, since they represent numerical values In [64]: import pandas as pd data = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_oceania.csv', index_col='country') # Extract year from last 4 characters of each column name years = data.columns.str.strip('gdpPercap_') # Convert year values to integers, saving results back to dataframe data.columns = years.astype(int) data.loc['Australia'].plot() Out[64]: <matplotlib.axes._subplots.AxesSubplot at 0x7fdc600bc588> Select and transform data, then plot it \u00b6 {#Select-and-transform-data,-then-plot-it} By default, DataFrame.plot plots with the rows as the X axis. We can transpose the data in order to plot multiple series. In [65]: data.T.plot() plt.ylabel('GDP per capita') Out[65]: Text(0,0.5,'GDP per capita') Many styles of plot are available. \u00b6 {#Many-styles-of-plot-are-available.} For example, do a bar plot using a fancier style. In [66]: plt.style.use('ggplot') data.T.plot(kind='bar') plt.ylabel('GDP per capita') Out[66]: Text(0,0.5,'GDP per capita') Data can also be plotted by calling the matplotlib plot function directly. \u00b6 {#Data-can-also-be-plotted-by-calling-the-matplotlib-plot-function-directly.} The command is plt.plot(x, y) The color / format of markers can also be specified as an optical argument: e.g. \u2018b-\u2018 is a blue line, \u2018g\u2013\u2019 is a green dashed line. Get Australia data from dataframe In [67]: years = data.columns gdp_australia = data.loc['Australia'] plt.plot(years, gdp_australia, 'g--') Out[67]: [<matplotlib.lines.Line2D at 0x7fdc5fece550>] Can plot many sets of data together. In [68]: # Select two countries' worth of data. gdp_australia = data.loc['Australia'] gdp_nz = data.loc['New Zealand'] # Plot with differently-colored markers. plt.plot(years, gdp_australia, 'b-', label='Australia') plt.plot(years, gdp_nz, 'g-', label='New Zealand') # Create legend. plt.legend(loc='upper left') plt.xlabel('Year') plt.ylabel('GDP per capita ($)') Out[68]: Text(0,0.5,'GDP per capita ($)') Add a legend \u00b6 {#Add-a-legend} Often when plotting multiple datasets on the same figure it is desirable to have a legend describing the data. This can be done in matplotlib in two stages: Provide a label for each dataset in the figure: In [69]: plt.plot(years, gdp_australia, label='Australia') plt.plot(years, gdp_nz, label='New Zealand') Out[69]: [<matplotlib.lines.Line2D at 0x7fdc5ff24b70>] Instruct matplotlib to create the legend. In [70]: plt.legend() No handles with labels found to put in legend. Out[70]: <matplotlib.legend.Legend at 0x7fdc5fde26d8> By default matplotlib will attempt to place the legend in a suitable position. If you would rather specify a position this can be done with the loc= argument, e.g to place the legend in the upper left corner of the plot, specify loc='upper left' Plot a scatter plot correlating the GDP of Australia and New Zealand Use either plt.scatter or DataFrame.plot.scatter In [71]: plt.scatter(gdp_australia, gdp_nz) Out[71]: <matplotlib.collections.PathCollection at 0x7fdc5fd5f6d8> In [72]: data.T.plot.scatter(x = 'Australia', y = 'New Zealand') Out[72]: <matplotlib.axes._subplots.AxesSubplot at 0x7fdc5fd99860> Exercises \u00b6 {#Exercises} Fill in the blanks below to plot the minimum GDP per capita over time for all the countries in Europe. Modify it again to plot the maximum GDP per capita over time for Europe. In [73]: data_europe = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_europe.csv', index_col='country') data_europe.____.plot(label='min') data_europe.____ plt.legend(loc='best') plt.xticks(rotation=90) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-73-e4db3abbc459> in <module>() 1 data_europe = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_europe.csv', index_col='country') ----> 2 data_europe.____.plot(label='min') 3 data_europe.____ 4 plt.legend(loc='best') 5 plt.xticks(rotation=90) /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in __getattr__(self, name) 3612 if name in self._info_axis: 3613 return self[name] -> 3614 return object.__getattribute__(self, name) 3615 3616 def __setattr__(self, name, value): AttributeError: 'DataFrame' object has no attribute '____' Modify the example in the notes to create a scatter plot showing the relationship between the minimum and maximum GDP per capita among the countries in Asia for each year in the data set. What relationship do you see (if any)? In [0]: data_asia = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_asia.csv', index_col='country') data_asia.describe().T.plot(kind='scatter', x='min', y='max') You might note that the variability in the maximum is much higher than that of the minimum. Take a look at the maximum and the max indexes: In [0]: data_asia = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_asia.csv', index_col='country') data_asia.max().plot() print(data_asia.idxmax()) print(data_asia.idxmin()) Saving your plot to a file \u00b6 {#Saving-your-plot-to-a-file} If you are satisfied with the plot you see you may want to save it to a file, perhaps to include it in a publication. There is a function in the matplotlib.pyplot module that accomplishes this: savefig. Calling this function, e.g. with In [0]: plt.savefig('my_figure.png') will save the current figure to the file my_figure.png. The file format will automatically be deduced from the file name extension (other formats are pdf, ps, eps and svg). Note that functions in plt refer to a global figure variable and after a figure has been displayed to the screen (e.g. with plt.show) matplotlib will make this variable refer to a new empty figure. Therefore, make sure you call plt.savefig before the plot is displayed to the screen, otherwise you may find a file with an empty plot. When using dataframes, data is often generated and plotted to screen in one line, and plt.savefig seems not to be a possible approach. One possibility to save the figure to file is then to save a reference to the current figure in a local variable (with plt.gcf) call the savefig class method from that variable. In [0]: fig = plt.gcf() # get current figure data.plot(kind='bar') fig.savefig('my_figure.png') Making your plots accessible \u00b6 {#Making-your-plots-accessible} Whenever you are generating plots to go into a paper or a presentation, there are a few things you can do to make sure that everyone can understand your plots. Always make sure your text is large enough to read. Use the fontsize parameter in xlabel, ylabel, title, and legend, and tick_params with labelsize to increase the text size of the numbers on your axes. Similarly, you should make your graph elements easy to see. Use s to increase the size of your scatterplot markers and linewidth to increase the sizes of your plot lines. Using color (and nothing else) to distinguish between different plot elements will make your plots unreadable to anyone who is colorblind, or who happens to have a black-and-white office printer. For lines, the linestyle parameter lets you use different types of lines. For scatterplots, marker lets you change the shape of your points. If you\u2019re unsure about your colors, you can use Coblis or Color Oracle to simulate what your plots would look like to those with colorblindness. Key Points \u00b6 {#Key-Points} matplotlib is the most widely used scientific plotting library in Python. Plot data directly from a Pandas dataframe. Select and transform data, then plot it. Many styles of plot are available: see the Python Graph Gallery for more options. Can plot many sets of data together. Programming Style \u00b6 {#Programming-Style} 15 minutes Exercises (15 min) Coding style \u00b6 {#Coding-style} Coding style helps us to understand the code better. It helps to maintain and change the code. Python relies strongly on coding style, as we may notice by the indentation we apply to lines to define different blocks of code. Python proposes a standard style through one of its first Python Enhancement Proposals (PEP), PEP8, and highlight the importance of readability in the Zen of Python. Keep in mind: document your code use clear, meaningful variable names use white-space, not tabs, to indent lines Follow standard Python style in your code. PEP8: a style guide for Python that discusses topics such as how you should name variables, how you should use indentation in your code, how you should structure your import statements, etc. Adhering to PEP8 makes it easier for other Python developers to read and understand your code, and to understand what their contributions should look like. The PEP8 application and Python library can check your code for compliance with PEP8. Google style guide on Python supports the use of PEP8 and extend the coding style to more specific structure of a Python code, which may be interesting also to follow. Use assertions to check for internal errors. \u00b6 {#Use-assertions-to-check-for-internal-errors.} Assertions are a simple, but powerful method for making sure that the context in which your code is executing is as you expect. In [109]: def calc_bulk_density(mass, volume): '''Return dry bulk density = powder mass / powder volume.''' assert volume > 0 return mass / volume In [110]: calc_bulk_density(60, -50) ---------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-110-b0873c16a0ba> in <module>() ----> 1 calc_bulk_density(60, -50) <ipython-input-109-fa5af01ee7ed> in calc_bulk_density(mass, volume) 1 def calc_bulk_density(mass, volume): 2 '''Return dry bulk density = powder mass / powder volume.''' ----> 3 assert volume > 0 4 return mass / volume AssertionError: If the assertion is False, the Python interpreter raises an AssertionError runtime exception. The source code for the expression that failed will be displayed as part of the error message. To ignore assertions in your code run the interpreter with the \u2018-O\u2019 (optimize) switch. Assertions should contain only simple checks and never change the state of the program. For example, an assertion should never contain an assignment. Use docstrings to provide online help. \u00b6 {#Use-docstrings-to-provide-online-help.} If the first thing in a function is a character string that is not assignd to a variable, Python attaches it to the function as thee online help. Called a docstring (short fo \"documentation string\"). In [111]: def average(values): \"Return average of values, or None if no values are supplied.\" if len(values) == 0: return None return sum(values) / len(values) help(average) Help on function average in module __main__: average(values) Return average of values, or None if no values are supplied. Also, you can comment your code using multiline strings. These start and end with three quote characters (either single or double) and end with three matching characters. In [112]: import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! In [77]: \"\"\"This string spans multiple lines. Blank lines are allowed.\"\"\" Out[77]: 'This string spans\\nmultiple lines.\\n\\nBlank lines are allowed.' Exercises \u00b6 {#Exercises} Highlight the lines in the code below that will be available as online help. Are there lines that should be made available, but won\u2019t be? Will any lines produce a syntax error or a runtime error? \"Find maximum edit distance between multiple sequences.\" # This finds the maximum distance between all sequences. def overall_max(sequences): '''Determine overall maximum edit distance.''' highest = 0 for left in sequences: for right in sequences: '''Avoid checking sequence against itself.''' if left != right: this = edit_distance(left, right) highest = max(highest, this) # Report. return highest Turn the comment on the following function into a docstring and check that help displays it properly. In [0]: def middle(a, b, c): # Return the middle value of three. # Assumes the values can actually be compared. values = [a, b, c] values.sort() return values[1] Clean up this code! Read this short program and try to predict what it does. Run it: how accurate was your prediction? Refactor the program to make it more readable. Remember to run it after each change to ensure its behavior hasn\u2019t changed. Compare your rewrite with your neighbor\u2019s. What did you do the same? What did you do differently, and why? n = 10 s = 'et cetera' print(s) i = 0 while i < n: # print('at', j) new = '' for j in range(len(s)): left = j-1 right = (j+1)%len(s) if s[left]==s[right]: new += '-' else: new += '*' s=''.join(new) print(s) i += 1 Key Points \u00b6 {#Key-Points} Follow standard Python style in your code. Use docstrings to provide online help.","title":"Unit4 IntrotoPythonSession2"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#pandas-dataframesseries-pandas-dataframesseries","text":"20 min Exercies (10 min) A DataFrame is a collection of Series; The DataFrame is the way Pandas represents a table, and Series is the data-structure Pandas use to represent a column. Pandas is built on top of the Numpy library, which in practice means that most of the methods defined for Numpy Arrays apply to Pandas Series/DataFrames. What makes Pandas so attractive is the powerful interface to access individual records of the table, proper handling of missing values, and relational-databases operations between DataFrames.","title":"Pandas Dataframes/Series\u00b6 {#Pandas-Dataframes/Series}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#selecting-values-iloc-selecting-values-iloc","text":"To access a value at the position [i,j] of a DataFrame, we have two options, depending on what is the meaning of i in use. Remember that a DataFrame provides a index as a way to identify the rows of the table; a row, then, has a position inside the table as well as a label, which uniquely identifies its entry in the DataFrame. dataframe.iloc can specify by numerical index analogously to 2D version of character selection in strings. dataframe.iloc[rows, columns] In [4]: import pandas as pd data = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_europe.csv', index_col='country') #data print(data.iloc[0:3, 0]) #With labels #print(data.loc[\"Albania\", \"gdpPercap_1952\"]) #All columns (just like usual slicing) #print(data.loc[\"Albania\", :]) country Albania 1601.056136 Austria 6137.076492 Belgium 8343.105127 Name: gdpPercap_1952, dtype: float64 Use DataFrame.loc[..., ...] to select values by their (entry) label. Can specify location by row name analogously to 2D version of dictionary keys. In [72]: data = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_europe.csv', index_col='country') print(data.loc[\"Albania\", \"gdpPercap_1952\"]) 1601.056136 Use : on its own to mean all columns or all rows. Just like Python\u2019s usual slicing notation. In [9]: print(data.loc[\"Italy\",:]) gdpPercap_1952 4931.404155 gdpPercap_1957 6248.656232 gdpPercap_1962 8243.582340 gdpPercap_1967 10022.401310 gdpPercap_1972 12269.273780 gdpPercap_1977 14255.984750 gdpPercap_1982 16537.483500 gdpPercap_1987 19207.234820 gdpPercap_1992 22013.644860 gdpPercap_1997 24675.024460 gdpPercap_2002 27968.098170 gdpPercap_2007 28569.719700 Name: Italy, dtype: float64 In [ ]: print(data.loc[\"Albania\", :]) Select multiple columns or rows using DataFrame.loc and a named slice. In [5]: print(data.loc['Italy':'Poland', 'gdpPercap_1962':'gdpPercap_1972']) gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 country Italy 8243.582340 10022.401310 12269.273780 Montenegro 4649.593785 5907.850937 7778.414017 Netherlands 12790.849560 15363.251360 18794.745670 Norway 13450.401510 16361.876470 18965.055510 Poland 5338.752143 6557.152776 8006.506993 In the above code, we discover that slicing using loc is inclusive at both ends , which differs from slicing using iloc , where slicing indicates everything up to but not including the final index.","title":"Selecting values (iloc[...,...])\u00b6 {#Selecting-values-(iloc[...,...])}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#result-of-slicing-can-be-used-in-further-operations-result-of-slicing-can-be-used-in-further-operations","text":"Usually don\u2019t just print a slice. All the statistical operators that work on entire dataframes work the same way on slices. E.g., calculate max of a slice. In [10]: print(data.loc['Italy':'Poland', 'gdpPercap_1962':'gdpPercap_1972'].max()) gdpPercap_1962 13450.40151 gdpPercap_1967 16361.87647 gdpPercap_1972 18965.05551 dtype: float64 In [11]: # Calculate minimum of slice print(data.loc['Italy':'Poland', 'gdpPercap_1962':'gdpPercap_1972'].min()) gdpPercap_1962 4649.593785 gdpPercap_1967 5907.850937 gdpPercap_1972 7778.414017 dtype: float64 Use comparisons to select data based on value. Comparison is applied element by element. Returns a similarly-shaped dataframe of True and False. In [15]: subset = data.loc['Italy':'Poland', 'gdpPercap_1962':'gdpPercap_1972'] #print('Subset of data:\\n', subset) # Which values were greater than 10000 ? print('\\nWhere are values large?\\n', subset > 10000) #Select values or NaN using a Boolean mask. mask = subset > 10000 print(subset[mask]) Where are values large? gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 country Italy False True True Montenegro False False False Netherlands True True True Norway True True True Poland False False False gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 country Italy NaN 10022.40131 12269.27378 Montenegro NaN NaN NaN Netherlands 12790.84956 15363.25136 18794.74567 Norway 13450.40151 16361.87647 18965.05551 Poland NaN NaN NaN Get the value where the mask is true, and NaN (Not a Number) where it is false. Useful because NaNs are ignored by operations like max, min, average, etc. A frame full of Booleans is sometimes called a mask because of how it can be used. In [9]: mask = subset > 10000 print(subset[mask]) gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 country Italy NaN 10022.40131 12269.27378 Montenegro NaN NaN NaN Netherlands 12790.84956 15363.25136 18794.74567 Norway 13450.40151 16361.87647 18965.05551 Poland NaN NaN NaN Get the value where the mask is true, and NaN (Not a Number) where it is false. Useful because NaNs are ignored by operations like max, min, average, etc.","title":"Result of slicing can be used in further operations.\u00b6 {#Result-of-slicing-can-be-used-in-further-operations.}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#group-by-split-apply-combine-group-by-split-apply-combine","text":"Pandas vectorizing methods and grouping operations are features that provide users much flexibility to analyse their data. We may have a glance by splitting the countries in two groups during the years surveyed, those who presented a GDP higher than the European average and those with a lower GDP. We then estimate a wealthy score based on the historical (from 1962 to 2007) values, where we account how many times a country has participated in the groups of lower or higher GDP In [21]: mask_higher = data > data.mean() wealth_score = mask_higher.aggregate('sum', axis=1) / len(data.columns) wealth_score Out[21]: country Albania 0.000000 Austria 1.000000 Belgium 1.000000 Bosnia and Herzegovina 0.000000 Bulgaria 0.000000 Croatia 0.000000 Czech Republic 0.500000 Denmark 1.000000 Finland 1.000000 France 1.000000 Germany 1.000000 Greece 0.333333 Hungary 0.000000 Iceland 1.000000 Ireland 0.333333 Italy 0.500000 Montenegro 0.000000 Netherlands 1.000000 Norway 1.000000 Poland 0.000000 Portugal 0.000000 Romania 0.000000 Serbia 0.000000 Slovak Republic 0.000000 Slovenia 0.333333 Spain 0.333333 Sweden 1.000000 Switzerland 1.000000 Turkey 0.000000 United Kingdom 1.000000 dtype: float64 Note : axis : (default 0) {0 or \u2018index\u2019, 1 or \u2018columns\u2019} 0 or \u2018index\u2019: apply function to each column. 1 or \u2018columns\u2019: apply function to each row. Finally, for each group in the wealth_score table, we sum their (financial) contribution across the years surveyed: In [22]: data.groupby(wealth_score).sum() Out[22]: gdpPercap_1952 gdpPercap_1957 gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 gdpPercap_1977 gdpPercap_1982 gdpPercap_1987 gdpPercap_1992 gdpPercap_1997 gdpPercap_2002 gdpPercap_2007 0.000000 36916.854200 46110.918793 56850.065437 71324.848786 88569.346898 104459.358438 113553.768507 119649.599409 92380.047256 103772.937598 118590.929863 149577.357928 0.333333 16790.046878 20942.456800 25744.935321 33567.667670 45277.839976 53860.456750 59679.634020 64436.912960 67918.093220 80876.051580 102086.795210 122803.729520 0.500000 11807.544405 14505.000150 18380.449470 21421.846200 25377.727380 29056.145370 31914.712050 35517.678220 36310.666080 40723.538700 45564.308390 51403.028210 1.000000 104317.277560 127332.008735 149989.154201 178000.350040 215162.343140 241143.412730 263388.781960 296825.131210 315238.235970 346930.926170 385109.939210 427850.333420","title":"Group By: split-apply-combine\u00b6 {#Group-By:-split-apply-combine}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#exercises-exercises","text":"Assume Pandas has been imported into your notebook and the Gapminder GDP data for Europe has been loaded: import pandas as pd df = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country') Write an expression to find the Per Capita GDP of Serbia in 2007. In [26]: df = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_europe.csv', index_col='country') print(df.loc['Serbia','gdpPercap_2007']) 9786.534714 In [27]: df.loc[\"Serbia\"][-1] Out[27]: 9786.534714 Explain in simple terms what idxmin and idxmax do in the short program below. When would you use these methods? data = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country') print(data.idxmin()) print(data.idxmax()) In [28]: print(data.idxmin()) gdpPercap_1952 Bosnia and Herzegovina gdpPercap_1957 Bosnia and Herzegovina gdpPercap_1962 Bosnia and Herzegovina gdpPercap_1967 Bosnia and Herzegovina gdpPercap_1972 Bosnia and Herzegovina gdpPercap_1977 Bosnia and Herzegovina gdpPercap_1982 Albania gdpPercap_1987 Albania gdpPercap_1992 Albania gdpPercap_1997 Albania gdpPercap_2002 Albania gdpPercap_2007 Albania dtype: object","title":"Exercises\u00b6 {#Exercises}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#key-points-key-points","text":"Use DataFrame.iloc[..., ...] to select values by integer location. Use : on its own to mean all columns or all rows. Select multiple columns or rows using DataFrame.loc and a named slice. Result of slicing can be used in further operations. Use comparisons to select data based on value. Select values or NaN using a Boolean mask.","title":"Key Points\u00b6 {#Key-Points}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#data-prep-with-pandas-data-prep-with-pandas","text":"20 min In [29]: import numpy as np import pandas as pd from numpy.random import randn np.random.seed(101) In [3]: df = pd.DataFrame(randn(5,4),index='A B C D E'.split(),columns='W X Y Z'.split()) df Out[3]: W X Y Z A 2.706850 0.628133 0.907969 0.503826 B 0.651118 -0.319318 -0.848077 0.605965 C -2.018168 0.740122 0.528813 -0.589001 D 0.188695 -0.758872 -0.933237 0.955057 E 0.190794 1.978757 2.605967 0.683509 Create new columns In [41]: df['K', :] = df[1,:] + df[1,:] df df.iloc[6,:] = df.iloc[1,:] + df.iloc[1,:] Out[41]: gdpPercap_1952 gdpPercap_1957 gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 gdpPercap_1977 gdpPercap_1982 gdpPercap_1987 gdpPercap_1992 gdpPercap_1997 gdpPercap_2002 gdpPercap_2007 6 country Sweden 8527.844662 9911.878226 12329.441920 15258.296970 17832.02464 18855.725210 20667.381250 23586.929270 23880.016830 25266.594990 29341.630930 33859.748350 NaN Switzerland 14734.232750 17909.489730 20431.092700 22966.144320 27195.11304 26982.290520 28397.715120 30281.704590 31871.530300 32135.323010 34480.957710 37506.419070 NaN Turkey 1969.100980 2218.754257 2322.869908 2826.356387 3450.69638 4269.122326 4241.356344 5089.043686 5678.348271 6601.429915 6508.085718 8458.276384 NaN United Kingdom 9979.508487 11283.177950 12477.177070 14142.850890 15895.11641 17428.748460 18232.424520 21664.787670 22705.092540 26074.531360 29478.999190 33203.261280 NaN 6 12274.152984 17685.196060 21501.442220 25669.204800 33323.25120 39498.844600 43194.167240 47375.652140 54084.037360 58191.841320 64835.215380 72252.985400 NaN Reorder columns in a data frame In [5]: df = df[['newColumn', 'W', 'X', 'Y', 'Z']] df Out[5]: newColumn W X Y Z A 1.131958 2.706850 0.628133 0.907969 0.503826 B 0.286647 0.651118 -0.319318 -0.848077 0.605965 C 0.151122 -2.018168 0.740122 0.528813 -0.589001 D 0.196184 0.188695 -0.758872 -0.933237 0.955057 E 2.662266 0.190794 1.978757 2.605967 0.683509","title":"Data prep with Pandas\u00b6 {#Data-prep-with-Pandas}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#group-by-group-by","text":"The method group-by allow you to group rows in a data frame and apply a function to it. In [65]: #Let's create a DF data = {'Company':['GOOG','GOOG','MSFT','MSFT','FB','FB'], 'Person':['Sam','Charlie','Amy','Vanessa','Carl','Sarah'], 'Sales':[200,120,340,124,243,350]} df = pd.DataFrame(data) print(df) #Group by company by_comp = df.groupby(\"Company\") #by_comp # Try some functions by_comp.mean() by_comp.count() by_comp.describe() by_comp.describe().transpose() Company Person Sales 0 GOOG Sam 200 1 GOOG Charlie 120 2 MSFT Amy 340 3 MSFT Vanessa 124 4 FB Carl 243 5 FB Sarah 350 Out[65]: Company FB GOOG MSFT Sales count 2.000000 2.000000 2.000000 mean 296.500000 160.000000 232.000000 std 75.660426 56.568542 152.735065 min 243.000000 120.000000 124.000000 25% 269.750000 140.000000 178.000000 50% 296.500000 160.000000 232.000000 75% 323.250000 180.000000 286.000000 max 350.000000 200.000000 340.000000 We can also merge data from different dataframes. It's very useful when we need a variable from a different file. You can use a \u2018left\u2019, \u2018right\u2019, \u2018outer\u2019, \u2018inner\u2019 Taken from In [56]: left = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'], 'A': ['A0', 'A1', 'A2', 'A3'], 'B': ['B0', 'B1', 'B2', 'B3'], 'C': ['C0', 'C1', 'C2', 'C3']}) right = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'], 'C': ['C0', 'C1', 'C2', 'C3'], 'D': ['D0', 'D1', 'D2', 'D3']}) print(left) print(right) ## Merge pd.merge(left, right, how='outer', on=['key']) A B C key 0 A0 B0 C0 K0 1 A1 B1 C1 K1 2 A2 B2 C2 K2 3 A3 B3 C3 K3 C D key 0 C0 D0 K0 1 C1 D1 K1 2 C2 D2 K2 3 C3 D3 K3 Out[56]: A B C_x key C_y D 0 A0 B0 C0 K0 C0 D0 1 A1 B1 C1 K1 C1 D1 2 A2 B2 C2 K2 C2 D2 3 A3 B3 C3 K3 C3 D3","title":"Group by\u00b6 {#Group-by}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#join-union-join-union","text":"In [58]: left = pd.DataFrame({'A': ['A0', 'A1', 'A2'], 'B': ['B0', 'B1', 'B2']}, index=['K0', 'K1', 'K2']) right = pd.DataFrame({'C': ['C0', 'C2', 'C3'], 'D': ['D0', 'D2', 'D3']}, index=['K0', 'K2', 'K3']) In [59]: left.join(right) Out[59]: A B C D K0 A0 B0 C0 D0 K1 A1 B1 NaN NaN K2 A2 B2 C2 D2 In [60]: right.join(left) Out[60]: C D A B K0 C0 D0 A0 B0 K2 C2 D2 A2 B2 K3 C3 D3 NaN NaN In [61]: left.join(right, how='outer') Out[61]: A B C D K0 A0 B0 C0 D0 K1 A1 B1 NaN NaN K2 A2 B2 C2 D2 K3 NaN NaN C3 D3 Some additional operations you can use with a pandas data frame unique: returns unique values in a series. nunique: returns the number of distinct observations over requested axis. value_counts: returns an object containing counts of unique values in sorted order. In [62]: df['Company'].unique() Out[62]: array(['GOOG', 'MSFT', 'FB'], dtype=object) In [63]: df['Company'].nunique() Out[63]: 3 In [20]: df['Company'].value_counts() Out[20]: FB 2 GOOG 2 MSFT 2 Name: Company, dtype: int64 There are some other very useful tricks you can do with pandas data frames. Such as profiling a dataframe. Profiling df.profile_report() is a simple and easy way to go furhter into knowing your data. Some other tips and tricks In [0]: #Install #pip install pandas-profiling In [0]: uploaded = files.upload() In [0]: import pandas as pd import pandas_profiling import io data = pd.read_csv(io.BytesIO(uploaded['gapminder_ In [0]: print(data.iloc[:,1:3]) Note: There are many other method we can use to explore the data and more effective exploration of a data set with pandas profiling. Check this out! In [0]: pandas_profiling.ProfileReport(data.iloc[:,0:6])","title":"Join (union)\u00b6 {#Join-(union)}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#some-other-useful-tools-to-work-with-data-frames-some-other-useful-tools-to-work-with-data-frames","text":"When you are working with large data frames you might want to know if there are missing values and how many are there. .isna() will create a table with booleans. True if a value is NaN In [67]: df.isna().head() Out[67]: Company Person Sales 0 False False False 1 False False False 2 False False False 3 False False False 4 False False False You can count how many Nan values you have per variable In [68]: df.isna().sum() Out[68]: Company 0 Person 0 Sales 0 dtype: int64 In [69]: df1 = df.copy() You can discard these values In [71]: df.dropna(axis=0) #for rows df.dropna(axis= 1) #for columns Out[71]: Company Person Sales 0 GOOG Sam 200 1 GOOG Charlie 120 2 MSFT Amy 340 3 MSFT Vanessa 124 4 FB Carl 243 5 FB Sarah 350","title":"Some other useful tools to work with data frames\u00b6 {#Some-other-useful-tools-to-work-with-data-frames}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#standardize-and-resize-data-directly-in-the-dataframe-standardize-and-resize-data-directly-in-the-dataframe","text":"Here we can do it manually (if like to do things like that) but we can also use methods already created. For example ScikitLearn provides: Simple and efficient tools for data mining and data analysis Accessible to everybody, and reusable in various contexts Built on NumPy, SciPy, and matplotlib The sklearn.preprocessing package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators. In general, learning algorithms benefit from standardization of the data set. If some outliers are present in the set, robust scalers or transformers are more appropriate. In [73]: from sklearn import preprocessing #Save columns names names = data.iloc[:,2:8].columns #Create scaler scaler = preprocessing.MinMaxScaler() #StandardScaler() #MaxAbsScaler #Transform your data frame (numeric variables ) data1 = data.iloc[:,2:8] data1 = scaler.fit_transform(data1) data1 = pd.DataFrame(data1, columns=names) print(data1.head()) print(data.iloc[:,2:8].head()) gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 gdpPercap_1977 \\ 0 0.032220 0.028270 0.018626 0.000193 1 0.482925 0.512761 0.567146 0.691612 2 0.495771 0.527883 0.567578 0.664689 3 0.000000 0.000000 0.000000 0.000000 4 0.135922 0.163734 0.153579 0.174119 gdpPercap_1982 gdpPercap_1987 0 0.000000 0.000000 1 0.725414 0.717533 2 0.700492 0.675728 3 0.020016 0.020688 4 0.185462 0.161892 gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 \\ country Albania 2312.888958 2760.196931 3313.422188 Austria 10750.721110 12834.602400 16661.625600 Belgium 10991.206760 13149.041190 16672.143560 Bosnia and Herzegovina 1709.683679 2172.352423 2860.169750 Bulgaria 4254.337839 5577.002800 6597.494398 gdpPercap_1977 gdpPercap_1982 gdpPercap_1987 country Albania 3533.003910 3630.880722 3738.932735 Austria 19749.422300 21597.083620 23687.826070 Belgium 19117.974480 20979.845890 22525.563080 Bosnia and Herzegovina 3528.481305 4126.613157 4314.114757 Bulgaria 7612.240438 8224.191647 8239.854824","title":"Standardize and resize data directly in the dataframe\u00b6 {#Standardize-and-resize-data-directly-in-the-dataframe}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#exercise-exercise","text":"With the file gapminder_all.csv try to: Filter only those countries located in Latin America. Select the columns corresponding to the gdpPercap and the population Explore the data frame using 3 different methods 4 Show how many contries had a gdpPercap higher than the mean in 1977. Check if there are some missing values (NaN) in the data","title":"Exercise\u00b6 {#Exercise}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#lists-lists","text":"15 min Exercises (10 min) A list stores many values in a single structure. Doing calculations with a hundred variables called pressure_001, pressure_002, etc., would be at least as slow as doing them by hand. Use a list to store many values together. Contained within square brackets [...]. Values separated by commas ,. Use len to find out how many values are in a list. In [74]: pressures = [0.273, 0.275, 0.277, 0.275, 0.276] print('pressures:', pressures) print('length:', len(pressures)) pressures: [0.273, 0.275, 0.277, 0.275, 0.276] length: 5 Use an item\u2019s index to fetch it from a list. In [22]: print('zeroth item of pressures:', pressures[0]) zeroth item of pressures: 0.273 Lists\u2019 values can be replaced by assigning to them. In [23]: pressures[0] = 0.265 print('pressures is now:', pressures) pressures is now: [0.265, 0.275, 0.277, 0.275, 0.276] Use list_name.append to add items to the end of a list. In [75]: primes = [2, 3, 5] print('primes is initially:', primes) primes.append(7) #primes.append(9) #print('primes has become:', primes) primes is initially: [2, 3, 5] Use del to remove items from a list entirely. In [76]: primes = [2, 3, 5, 7, 9] print('primes before removing last item:', primes) del primes[4] print('primes after removing last item:', primes) primes before removing last item: [2, 3, 5, 7, 9] primes after removing last item: [2, 3, 5, 7] The empty list contains no values. Use [ ] on its own to represent a list that doesn\u2019t contain any values. Lists may contain values of different types. In [26]: goals = [1, 'Create lists.', 2, 'Extract items from lists.', 3, 'Modify lists.'] Character strings can be indexed like lists. In [27]: element = 'carbon' print('zeroth character:', element[0]) print('third character:', element[3]) zeroth character: c third character: b Character strings are immutable. Cannot change the characters in a string after it has been created. Immutable: can\u2019t be changed after creation. In contrast, lists are mutable: they can be modified in place. Python considers the string to be a single value with parts, not a collection of values. In [28]: element[0] = 'C' --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-28-6dc46761ce07> in <module>() ----> 1 element[0] = 'C' TypeError: 'str' object does not support item assignment","title":"Lists\u00b6 {#Lists}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#exercises-exercises_1","text":"Given this: print('string to list:', list('tin')) print('list to string:', ''.join(['g', 'o', 'l', 'd'])) What does list('some string') do? What does '-'.join(['x', 'y', 'z']) generate? In [77]: print(list('YaQueremosComer')) ['Y', 'a', 'Q', 'u', 'e', 'r', 'e', 'm', 'o', 's', 'C', 'o', 'm', 'e', 'r'] What does the following program print? element = 'helium' print(element[-1]) How does Python interpret a negative index? If a list or string has N elements, what is the most negative index that can safely be used with it, and what location does that index represent? If values is a list, what does del values[-1] do? How can you display all elements but the last one without changing values? (Hint: you will need to combine slicing and negative indexing.) What does the following program print? element = 'fluorine' print(element[::2]) print(element[::-1]) If we write a slice as low:high:stride, what does stride do? What expression would select all of the even-numbered items from a collection?","title":"Exercises\u00b6 {#Exercises}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#key-points-key-points_1","text":"A list stores many values in a single structure. Use an item\u2019s index to fetch it from a list. Lists\u2019 values can be replaced by assigning to them. Appending items to a list lengthens it. Use del to remove items from a list entirely. The empty list contains no values. Lists may contain values of different types. Character strings can be indexed like lists. Character strings are immutable. Indexing beyond the end of the collection is an error.","title":"Key Points\u00b6 {#Key-Points}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#for-loops-for-loops","text":"10 min Exercises (15 min) A for loop executes commands once for each value in a collection. \u201cfor each thing in this group, do these operations\u201d In [78]: for number in [2, 3, 5]: print(number) 2 3 5 This for loop is equivalent to: In [30]: print(2) print(3) print(5) 2 3 5 A for loop is made up of a collection, loop variable and a body. Parts of a for loop The collection, [2, 3, 5], is what the loop is being run on. The body, print(number), specifies what to do for each value in the collection. The loop variable, number, is what changes for each iteration of the loop. The \u201ccurrent thing\u201d. Python uses indentation rather than {} or begin/end to show nesting. Use range to iterate over a sequence of numbers. The first line of the for loop must end with a colon, and the body must be indented. The colon at the end of the first line signals the start of a block of statements. Python uses indentation rather than {} or begin/end to show nesting. Any consistent indentation is legal, but almost everyone uses four spaces. In [80]: for number in [2, 3, 5]: print(number) 2 3 5 Indentation is always meaningful in Python. In [81]: firstName = \"Jon\" lastName = \"Smith\" File \"<ipython-input-81-6966a7c3a64d>\", line 2 lastName = \"Smith\" ^ IndentationError: unexpected indent Loop variables can be called anything. As with all variables, loop variables are: Created on demand. Meaningless: their names can be anything at all. In [33]: for kitten in [2, 3, 5]: print(kitten) 2 3 5 The body of a loop can contain many statements. But no loop should be more than a few lines long. Hard for human beings to keep larger chunks of code in mind. In [82]: primes = [2, 3, 5] for p in primes: squared = p ** 2 cubed = p ** 3 print(p, squared, cubed) 2 4 8 3 9 27 5 25 125 Use range to iterate over a sequence of numbers. The built-in function range produces a sequence of numbers. Not a list: the numbers are produced on demand to make looping over large ranges more efficient. range(N) is the numbers 0..N-1 Exactly the legal indices of a list or character string of length N In [83]: print('a range is not a list: range(0, 3)') for number in range(0, 3): print(number) a range is not a list: range(0, 3) 0 1 2 The Accumulator pattern turns many values into one. Initialize an accumulator variable to zero, the empty string, or the empty list. In [86]: total = 0 for number in range(10): total = total + (number + 1) print(total) 1 3 6 10 15 21 28 36 45 55","title":"For Loops\u00b6 {#For-Loops}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#exercises-exercises_2","text":"Create a table showing the numbers of the lines that are executed when this program runs, and the values of the variables after each line is executed. total = 0 for char in \"tin\": total = total + 1 Fill in the blanks in the program below so that it prints \u201cnit\u201d (the reverse of the original character string \u201ctin\u201d). original = \"tin\" result = ____ for char in original: result = ____ print(result) In [0]: original = \"tin\" result = \"\" for char in original: result = char + result print(result) t it nit Fill in the blanks in each of the programs below to produce the indicated result. In [0]: # Total length of the strings in the list: [\"red\", \"green\", \"blue\"] => 12 total = 0 for word in [\"red\", \"green\", \"blue\"]: ____ = ____ + len(word) print(total) In [87]: # List of word lengths: [\"red\", \"green\", \"blue\"] => [3, 5, 4] lengths = [] for word in [\"red\", \"green\", \"blue\"]: lengths.append(len(word)) print(lengths) [3, 5, 4] In [0]: # Concatenate all words: [\"red\", \"green\", \"blue\"] => \"redgreenblue\" words = [\"red\", \"green\", \"blue\"] result = ____ for ____ in ____: ____ print(result) In [0]: # Create acronym: [\"red\", \"green\", \"blue\"] => \"RGB\" # write the whole thing Find the error to the following code students = ['Ana', 'Juan', 'Susan'] for m in students: print(m) Cumulative sum. Reorder and properly indent the lines of code below so that they print a list with the cumulative sum of data. The result should be [1, 3, 5, 10]. In [0]: cumulative.append(sum) for number in data: cumulative = [] sum += number sum = 0 print(cumulative) data = [1,2,2,5]","title":"Exercises\u00b6 {#Exercises}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#key-points-key-points_2","text":"A for loop executes commands once for each value in a collection. A for loop is made up of a collection, a loop variable, and a body. The first line of the for loop must end with a colon, and the body must be indented. Indentation is always meaningful in Python. Loop variables can be called anything (but it is strongly advised to have a meaningful name to the looping variable). The body of a loop can contain many statements. Use range to iterate over a sequence of numbers. The Accumulator pattern turns many values into one.","title":"Key Points\u00b6 {#Key-Points}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#looping-over-data-sets-looping-over-data-sets","text":"5 min Exercises (10 min) Use a for loop to process files given a list of their names. In [88]: import pandas as pd for filename in ['/home/mcubero/dataSanJose19/data/gapminder_gdp_africa.csv', '/home/mcubero/dataSanJose19/data/gapminder_gdp_asia.csv']: data = pd.read_csv(filename, index_col='country') print(filename, data.min()) /home/mcubero/dataSanJose19/data/gapminder_gdp_africa.csv gdpPercap_1952 298.846212 gdpPercap_1957 335.997115 gdpPercap_1962 355.203227 gdpPercap_1967 412.977514 gdpPercap_1972 464.099504 gdpPercap_1977 502.319733 gdpPercap_1982 462.211415 gdpPercap_1987 389.876185 gdpPercap_1992 410.896824 gdpPercap_1997 312.188423 gdpPercap_2002 241.165877 gdpPercap_2007 277.551859 dtype: float64 /home/mcubero/dataSanJose19/data/gapminder_gdp_asia.csv gdpPercap_1952 331.0 gdpPercap_1957 350.0 gdpPercap_1962 388.0 gdpPercap_1967 349.0 gdpPercap_1972 357.0 gdpPercap_1977 371.0 gdpPercap_1982 424.0 gdpPercap_1987 385.0 gdpPercap_1992 347.0 gdpPercap_1997 415.0 gdpPercap_2002 611.0 gdpPercap_2007 944.0 dtype: float64 Use glob.glob to find sets of files whose names match a pattern. In Unix, the term \u201cglobbing\u201d means \u201cmatching a set of files with a pattern\u201d. '*' meaning \u201cmatch zero or more characters\u201d Python contains the glob library to provide pattern matching functionality. In [90]: import glob print('all csv files in data directory:', glob.glob('/home/mcubero/dataSanJose19/data/*.csv')) all csv files in data directory: ['/home/mcubero/dataSanJose19/data/gapminder_all.csv', '/home/mcubero/dataSanJose19/data/gapminder_gdp_africa.csv', '/home/mcubero/dataSanJose19/data/gapminder_gdp_americas.csv', '/home/mcubero/dataSanJose19/data/gapminder_gdp_asia.csv', '/home/mcubero/dataSanJose19/data/gapminder_gdp_europe.csv', '/home/mcubero/dataSanJose19/data/gapminder_gdp_oceania.csv', '/home/mcubero/dataSanJose19/data/processed.csv'] In [91]: print('all PDB files:', glob.glob('*.pdb')) all PDB files: [] Use glob and for to process batches of files. In [92]: for filename in glob.glob('/home/mcubero/dataSanJose19/data/gapminder_*.csv'): data = pd.read_csv(filename) print(filename, data['gdpPercap_1952'].min()) /home/mcubero/dataSanJose19/data/gapminder_all.csv 298.8462121 /home/mcubero/dataSanJose19/data/gapminder_gdp_africa.csv 298.8462121 /home/mcubero/dataSanJose19/data/gapminder_gdp_americas.csv 1397.7171369999999 /home/mcubero/dataSanJose19/data/gapminder_gdp_asia.csv 331.0 /home/mcubero/dataSanJose19/data/gapminder_gdp_europe.csv 973.5331947999999 /home/mcubero/dataSanJose19/data/gapminder_gdp_oceania.csv 10039.595640000001","title":"Looping Over Data Sets\u00b6 {#Looping-Over-Data-Sets}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#exercises-exercises_3","text":"Which of these files is not matched by the expression glob.glob('data/ as .csv')? data/gapminder_gdp_africa.csv data/gapminder_gdp_americas.csv data/gapminder_gdp_asia.csv 1 and 2 are not matched.","title":"Exercises\u00b6 {#Exercises}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#key-points-key-points_3","text":"Use a for loop to process files given a list of their names. Use glob.glob to find sets of files whose names match a pattern. Use glob and for to process batches of files.","title":"Key Points\u00b6 {#Key-Points}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#stretching-time-stretching-time","text":"","title":"STRETCHING TIME {#STRETCHING-TIME!}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#writing-functions-writing-functions","text":"15 min Exercises (20 min) Break programs down into functions to make them easier to understand. Human beings can only keep a few items in working memory at a time. Encapsulate complexity so that we can treat it as a single \u201cthing\u201d. Write one time, use many times. To define a function use def then the name of the function like this: def say_hi(parameter1, parameter2): print('Hello') Remember, defining a function does not run it, you must call the function to execute it. In [93]: def print_date(year, month, day): joined = str(year) + '/' + str(month) + '/' + str(day) print(joined) print_date(1871, 3, 19) 1871/3/19 In [43]: print_date(month=3, day=19, year=1871) 1871/3/19 Use return ... to give a value back to the caller. May occur anywhere in the function. In [94]: def average(values): if len(values) == 0: return None return sum(values) / len(values) Remember: every function returns something A function that doesn\u2019t explicitly return a value automatically returns None.","title":"Writing functions\u00b6 {#Writing-functions}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#exercises-exercises_4","text":"What does the following program print? def report(pressure): print('pressure is', pressure) print('calling', report, 22.5) In [96]: def report(pressure): print('pressure is', pressure) print('calling', report(22.5)) pressure is 22.5 calling None Fill in the blanks to create a function that takes a single filename as an argument, loads the data in the file named by the argument, and returns the minimum value in that data. import pandas as pd def min_in_data(____): data = ____ return ____ In [98]: import pandas as pd def min_in_data(data): data = pd.read_csv(data) return data.min() min_in_data('/home/mcubero/dataSanJose19/data/gapminder_gdp_africa.csv') Out[98]: country Algeria gdpPercap_1952 298.846 gdpPercap_1957 335.997 gdpPercap_1962 355.203 gdpPercap_1967 412.978 gdpPercap_1972 464.1 gdpPercap_1977 502.32 gdpPercap_1982 462.211 gdpPercap_1987 389.876 gdpPercap_1992 410.897 gdpPercap_1997 312.188 gdpPercap_2002 241.166 gdpPercap_2007 277.552 dtype: object The code below will run on a label-printer for chicken eggs. A digital scale will report a chicken egg mass (in grams) to the computer and then the computer will print a label. Please re-write the code so that the if-block is folded into a function. import random for i in range(10): # simulating the mass of a chicken egg # the (random) mass will be 70 +/- 20 grams mass=70+20.0*(2.0*random.random()-1.0) print(mass) #egg sizing machinery prints a label if(mass>=85): print(\"jumbo\") elif(mass>=70): print(\"large\") elif(mass<70 and mass>=55): print(\"medium\") else: print(\"small\") Assume that the following code has been executed: In [46]: import pandas as pd df = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_asia.csv', index_col=0) japan = df.loc['Japan'] japan Out[46]: gdpPercap_1952 3216.956347 gdpPercap_1957 4317.694365 gdpPercap_1962 6576.649461 gdpPercap_1967 9847.788607 gdpPercap_1972 14778.786360 gdpPercap_1977 16610.377010 gdpPercap_1982 19384.105710 gdpPercap_1987 22375.941890 gdpPercap_1992 26824.895110 gdpPercap_1997 28816.584990 gdpPercap_2002 28604.591900 gdpPercap_2007 31656.068060 Name: Japan, dtype: float64 1.Complete the statements below to obtain the average GDP for Japan across the years reported for the 1980s. year = 1983 gdp_decade = 'gdpPercap_' + str(year // ____) avg = (japan.loc[gdp_decade + ___] + japan.loc[gdp_decade + ___]) / 2 2.Abstract the code above into a single function. def avg_gdp_in_decade(country, continent, year): df = pd.read_csv('data/gapminder_gdp_'+___+'.csv',delimiter=',',index_col=0) ____ ____ ____ return avg .How would you generalize this function if you did not know beforehand which specific years occurred as columns in the data? For instance, what if we also had data from years ending in 1 and 9 for each decade? (Hint: use the columns to filter out the ones that correspond to the decade, instead of enumerating them in the code.)","title":"Exercises\u00b6 {#Exercises}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#key-points-key-points_4","text":"Break programs down into functions to make them easier to understand. Define a function using def with a name, parameters, and a block of code. Defining a function does not run it. Arguments in call are matched to parameters in definition. Functions may return a result to their caller using return.","title":"Key Points\u00b6 {#Key-Points}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#variable-scope-variable-scope","text":"10 min Exercise (10 min) The scope of a variable is the part of a program that can \u2018see\u2019 that variable. There are only so many sensible names for variables. People using functions shouldn\u2019t have to worry about what variable names the author of the function used. People writing functions shouldn\u2019t have to worry about what variable names the function\u2019s caller uses. The part of a program in which a variable is visible is called its scope. In [99]: pressure = 103.9 def adjust(t): temperature = t * 1.43 / pressure return temperature pressure is a global variable. Defined outside any particular function. Visible everywhere. t and temperature are local variables in adjust. Defined in the function. Not visible in the main program. Remember: a function parameter is a variable that is automatically assigned a value when the function is called. In [100]: print('adjusted:', adjust(0.9)) print('temperature after call:', temperature) adjusted: 0.01238691049085659 ---------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-100-e73c01f89950> in <module>() 1 print('adjusted:', adjust(0.9)) ----> 2 print('temperature after call:', temperature) NameError: name 'temperature' is not defined","title":"Variable Scope\u00b6 {#Variable-Scope}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#exercises-exercises_5","text":"Trace the values of all variables in this program as it is executed. (Use \u2018\u2014\u2019 as the value of variables before and after they exist.) limit = 100 def clip(value): return min(max(0.0, value), limit) value = -22.5 print(clip(value)) Read the traceback below, and identify the following: How many levels does the traceback have? What is the file name where the error occurred? What is the function name where the error occurred? On which line number in this function did the error occur? What is the type of error? What is the error message? --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-2-e4c4cbafeeb5> in <module>() 1 import errors_02 ----> 2 errors_02.print_friday_message() /Users/ghopper/thesis/code/errors_02.py in print_friday_message() 13 14 def print_friday_message(): ---> 15 print_message(\"Friday\") /Users/ghopper/thesis/code/errors_02.py in print_message(day) 9 \"sunday\": \"Aw, the weekend is almost over.\" 10 } ---> 11 print(messages[day]) 12 13 KeyError: 'Friday'","title":"Exercises\u00b6 {#Exercises}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#key-points-key-points_5","text":"The scope of a variable is the part of a program that can \u2018see\u2019 that variable.","title":"Key Points\u00b6 {#Key-Points}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#conditionals-conditionals","text":"15 min Exercise (15 min) Use if statements to control whether or not a block of code is executed. An if statement (more properly called a conditional statement) controls whether some block of code is executed or not. Structure is similar to a for statement: First line opens with if and ends with a colon Body containing one or more statements is indented (usually by 4 spaces) In [52]: mass = 2.07 if mass > 3.0: print (mass, 'is large') In [102]: masses = [3.54, 2.07, 9.22, 1.86, 1.71] for m in masses: if m > 3.0: print(m, 'is large') else: print(m, 'is small') 3.54 is large 2.07 is small 9.22 is large 1.86 is small 1.71 is small In [104]: thing1 = [3.54, 2.07, 9.22] if masses > thing1: print (masses, 'is large') [3.54, 2.07, 9.22, 1.86, 1.71] is large Conditionals are often used inside loops. Not much point using a conditional when we know the value (as above). But useful when we have a collection to process. In [54]: masses = [3.54, 2.07, 9.22, 1.86, 1.71] for m in masses: if m > 3.0: print(m, 'is large') 3.54 is large 9.22 is large Use else to execute a block of code when an if condition is not true. else can be used following an if. Allows us to specify an alternative to execute when the if branch isn\u2019t taken. In [55]: masses = [3.54, 2.07, 9.22, 1.86, 1.71] for m in masses: if m > 3.0: print(m, 'is large') else: print(m, 'is small') 3.54 is large 2.07 is small 9.22 is large 1.86 is small 1.71 is small Use elif to specify additional tests. May want to provide several alternative choices, each with its own test. Use elif (short for \u201celse if\u201d) and a condition to specify these. Always associated with an if. Must come before the else (which is the \u201ccatch all\u201d). Complete the next conditional In [56]: masses = [3.54, 2.07, 9.22, 1.86, 1.71] for m in ____: if m > 9.0: print(__, 'is HUGE') elif m > 3.0: print(m, 'is large') ___: print(m, 'is small') File \"<ipython-input-56-97e8fa260561>\", line 7 ___: ^ SyntaxError: invalid syntax Conditions are tested once, in order. Python steps through the branches of the conditional in order, testing each in turn. So ordering matters. In [0]: grade = 85 if grade >= 70: print('grade is C') elif grade >= 80: print('grade is B') elif grade >= 90: print('grade is A') grade is C Does not automatically go back and re-evaluate if values change. In [0]: velocity = 10.0 if velocity > 20.0: print('moving too fast') else: print('adjusting velocity') velocity = 50.0 Often use conditionals in a loop to \u201cevolve\u201d the values of variables. In [105]: velocity = 10.0 for i in range(5): # execute the loop 5 times print(i, ':', velocity) if velocity > 20.0: print('moving too fast') velocity = velocity - 5.0 else: print('moving too slow') velocity = velocity + 10.0 print('final velocity:', velocity) 0 : 10.0 moving too slow 1 : 20.0 moving too slow 2 : 30.0 moving too fast 3 : 25.0 moving too fast 4 : 20.0 moving too slow final velocity: 30.0 Conditionals are useful to check for errors! Often, you want some combination of things to be true. You can combine relations within a conditional using and and or. Continuing the example above, suppose you have In [0]: mass = [ 3.54, 2.07, 9.22, 1.86, 1.71] velocity = [10.00, 20.00, 30.00, 25.00, 20.00] i = 0 for i in range(5): if mass[i] > 5 and velocity[i] > 20: print(\"Fast heavy object. Duck!\") elif mass[i] > 2 and mass[i] <= 5 and velocity[i] <= 20: print(\"Normal traffic\") elif mass[i] <= 2 and velocity[i] <= 20: print(\"Slow light object. Ignore it\") else: print(\"Whoa! Something is up with the data. Check it\") Normal traffic Normal traffic Fast heavy object. Duck! Whoa! Something is up with the data. Check it Slow light object. Ignore it Just like with arithmetic, you can and should use parentheses whenever there is possible ambiguity. A good general rule is to always use parentheses when mixing and and or in the same condition. That is, instead of: In [0]: if mass[i] <= 2 or mass[i] >= 5 and velocity[i] > 20: write one of these: In [0]: if (mass[i] <= 2 or mass[i] >= 5) and velocity[i] > 20: if mass[i] <= 2 or (mass[i] >= 5 and velocity[i] > 20): so it is perfectly clear to a reader (and to Python) what you really mean.","title":"Conditionals\u00b6 {#Conditionals}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#exercise-exercise_1","text":"What does this program print? pressure = 71.9 if pressure > 50.0: pressure = 25.0 elif pressure <= 50.0: pressure = 0.0 print(pressure) In [106]: pressure = 71.9 if pressure > 50.0: pressure = 25.0 elif pressure <= 50.0: pressure = 0.0 print(pressure) 25.0 Trimming Values Fill in the blanks so that this program creates a new list containing zeroes where the original list\u2019s values were negative and ones where the original list\u2019s values were positive. In [107]: original = [-1.5, 0.2, 0.4, 0.0, -1.3, 0.4] result = [] for value in original: if value < 0.0: result.append(0) else: result.append(1) print(result) [0, 1, 1, 1, 0, 1] Modify this program so that it only processes files with fewer than 50 records. In [108]: import glob import pandas as pd for filename in glob.glob('/home/mcubero/dataSanJose19/data/*.csv'): contents = pd.read_csv(filename) if len(contents) < 50: print(filename, len(contents)) /home/mcubero/dataSanJose19/data/gapminder_gdp_americas.csv 25 /home/mcubero/dataSanJose19/data/gapminder_gdp_asia.csv 33 /home/mcubero/dataSanJose19/data/gapminder_gdp_europe.csv 30 /home/mcubero/dataSanJose19/data/gapminder_gdp_oceania.csv 2 /home/mcubero/dataSanJose19/data/processed.csv 2 Modify this program so that it finds the largest and smallest values in the list no matter what the range of values originally is. values = [...some test data...] smallest, largest = None, None for v in values: if ____: smallest, largest = v, v ____: smallest = min(____, v) largest = max(____, v) print(smallest, largest) What are the advantages and disadvantages of using this method to find the range of the data? Using functions with conditionals in Pandas Functions will often contain conditionals. Here is a short example that will indicate which quartile the argument is in based on hand-coded values for the quartile cut points. In [9]: def calculate_life_quartile(exp): if exp < 58.41: # This observation is in the first quartile return 1 elif exp >= 58.41 and exp < 67.05: # This observation is in the second quartile return 2 elif exp >= 67.05 and exp < 71.70: # This observation is in the third quartile return 3 elif exp >= 71.70: # This observation is in the fourth quartile return 4 else: # This observation has bad data return None calculate_life_quartile(62.5) Out[9]: 2 That function would typically be used within a for loop, but Pandas has a different, more efficient way of doing the same thing, and that is by applying a function to a dataframe or a portion of a dataframe. Here is an example, using the definition above. In [59]: data = pd.read_csv('/home/mcubero/dataSanJose19/data/all-Americas.csv') data #data['life_qrtl'] = data['lifeExp'].apply(calculate_life_quartile) Out[59]: continent country gdpPercap_1952 gdpPercap_1957 gdpPercap_1962 gdpPercap_1967 gdpPercap_1972 gdpPercap_1977 gdpPercap_1982 gdpPercap_1987 gdpPercap_1992 gdpPercap_1997 gdpPercap_2002 gdpPercap_2007 0 Americas Argentina 5911.315053 6856.856212 7133.166023 8052.953021 9443.038526 10079.026740 8997.897412 9139.671389 9308.418710 10967.281950 8797.640716 12779.379640 1 Americas Bolivia 2677.326347 2127.686326 2180.972546 2586.886053 2980.331339 3548.097832 3156.510452 2753.691490 2961.699694 3326.143191 3413.262690 3822.137084 2 Americas Brazil 2108.944355 2487.365989 3336.585802 3429.864357 4985.711467 6660.118654 7030.835878 7807.095818 6950.283021 7957.980824 8131.212843 9065.800825 3 Americas Canada 11367.161120 12489.950060 13462.485550 16076.588030 18970.570860 22090.883060 22898.792140 26626.515030 26342.884260 28954.925890 33328.965070 36319.235010 4 Americas Chile 3939.978789 4315.622723 4519.094331 5106.654313 5494.024437 4756.763836 5095.665738 5547.063754 7596.125964 10118.053180 10778.783850 13171.638850 5 Americas Colombia 2144.115096 2323.805581 2492.351109 2678.729839 3264.660041 3815.807870 4397.575659 4903.219100 5444.648617 6117.361746 5755.259962 7006.580419 6 Americas Costa Rica 2627.009471 2990.010802 3460.937025 4161.727834 5118.146939 5926.876967 5262.734751 5629.915318 6160.416317 6677.045314 7723.447195 9645.061420 7 Americas Cuba 5586.538780 6092.174359 5180.755910 5690.268015 5305.445256 6380.494966 7316.918107 7532.924763 5592.843963 5431.990415 6340.646683 8948.102923 8 Americas Dominican Republic 1397.717137 1544.402995 1662.137359 1653.723003 2189.874499 2681.988900 2861.092386 2899.842175 3044.214214 3614.101285 4563.808154 6025.374752 9 Americas Ecuador 3522.110717 3780.546651 4086.114078 4579.074215 5280.994710 6679.623260 7213.791267 6481.776993 7103.702595 7429.455877 5773.044512 6873.262326 10 Americas El Salvador 3048.302900 3421.523218 3776.803627 4358.595393 4520.246008 5138.922374 4098.344175 4140.442097 4444.231700 5154.825496 5351.568666 5728.353514 11 Americas Guatemala 2428.237769 2617.155967 2750.364446 3242.531147 4031.408271 4879.992748 4820.494790 4246.485974 4439.450840 4684.313807 4858.347495 5186.050003 12 Americas Haiti 1840.366939 1726.887882 1796.589032 1452.057666 1654.456946 1874.298931 2011.159549 1823.015995 1456.309517 1341.726931 1270.364932 1201.637154 13 Americas Honduras 2194.926204 2220.487682 2291.156835 2538.269358 2529.842345 3203.208066 3121.760794 3023.096699 3081.694603 3160.454906 3099.728660 3548.330846 14 Americas Jamaica 2898.530881 4756.525781 5246.107524 6124.703451 7433.889293 6650.195573 6068.051350 6351.237495 7404.923685 7121.924704 6994.774861 7320.880262 15 Americas Mexico 3478.125529 4131.546641 4581.609385 5754.733883 6809.406690 7674.929108 9611.147541 8688.156003 9472.384295 9767.297530 10742.440530 11977.574960 16 Americas Nicaragua 3112.363948 3457.415947 3634.364406 4643.393534 4688.593267 5486.371089 3470.338156 2955.984375 2170.151724 2253.023004 2474.548819 2749.320965 17 Americas Panama 2480.380334 2961.800905 3536.540301 4421.009084 5364.249663 5351.912144 7009.601598 7034.779161 6618.743050 7113.692252 7356.031934 9809.185636 18 Americas Paraguay 1952.308701 2046.154706 2148.027146 2299.376311 2523.337977 3248.373311 4258.503604 3998.875695 4196.411078 4247.400261 3783.674243 4172.838464 19 Americas Peru 3758.523437 4245.256698 4957.037982 5788.093330 5937.827283 6281.290855 6434.501797 6360.943444 4446.380924 5838.347657 5909.020073 7408.905561 20 Americas Puerto Rico 3081.959785 3907.156189 5108.344630 6929.277714 9123.041742 9770.524921 10330.989150 12281.341910 14641.587110 16999.433300 18855.606180 19328.709010 21 Americas Trinidad and Tobago 3023.271928 4100.393400 4997.523971 5621.368472 6619.551419 7899.554209 9119.528607 7388.597823 7370.990932 8792.573126 11460.600230 18008.509240 22 Americas United States 13990.482080 14847.127120 16173.145860 19530.365570 21806.035940 24072.632130 25009.559140 29884.350410 32003.932240 35767.433030 39097.099550 42951.653090 23 Americas Uruguay 5716.766744 6150.772969 5603.357717 5444.619620 5703.408898 6504.339663 6920.223051 7452.398969 8137.004775 9230.240708 7727.002004 10611.462990 24 Americas Venezuela 7689.799761 9802.466526 8422.974165 9541.474188 10505.259660 13143.950950 11152.410110 9883.584648 10733.926310 10165.495180 8605.047831 11415.805690 There is a lot in that second line, so let\u2019s take it piece by piece. On the right side of the = we start with data['lifeExp'], which is the column in the dataframe called data labeled lifExp. We use the apply() to do what it says, apply the calculate_life_quartile to the value of this column for every row in the dataframe.","title":"Exercise\u00b6 {#Exercise}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#key-points-key-points_6","text":"Use if statements to control whether or not a block of code is executed. Conditionals are often used inside loops. Use else to execute a block of code when an if condition is not true. Use elif to specify additional tests. Conditions are tested once, in order. Create a table showing variables\u2019 values to trace a program\u2019s execution.","title":"Key Points\u00b6 {#Key-Points}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#plotting-plotting","text":"20 min Exercises (15 min) We are going to use matplotlib. matplotlib is the most widely used scientific plotting library in Python. Commonly use a sub-library called matplotlib.pyplot. The Jupyter Notebook will render plots inline if we ask it to using a \u201cmagic\u201d command. In [61]: #%matplotlib inline import matplotlib.pyplot as plt Simple plots are then (fairly) simple to create In [62]: time = [0, 1, 2, 3] position = [0, 100, 200, 300] plt.plot(time, position) plt.xlabel('Time (hr)') plt.ylabel('Position (km)') Out[62]: Text(0,0.5,'Position (km)')","title":"Plotting\u00b6 {#Plotting}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#plot-data-directly-from-a-pandas-dataframe-plot-data-directly-from-a-pandas-dataframe","text":"We can also plot Pandas dataframes. This implicitly uses matplotlib.pyplot. Before plotting, we convert the column headings from a string to integer data type, since they represent numerical values In [64]: import pandas as pd data = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_oceania.csv', index_col='country') # Extract year from last 4 characters of each column name years = data.columns.str.strip('gdpPercap_') # Convert year values to integers, saving results back to dataframe data.columns = years.astype(int) data.loc['Australia'].plot() Out[64]: <matplotlib.axes._subplots.AxesSubplot at 0x7fdc600bc588>","title":"Plot data directly from a Pandas dataframe\u00b6 {#Plot-data-directly-from-a-Pandas-dataframe}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#select-and-transform-data-then-plot-it-select-and-transform-data-then-plot-it","text":"By default, DataFrame.plot plots with the rows as the X axis. We can transpose the data in order to plot multiple series. In [65]: data.T.plot() plt.ylabel('GDP per capita') Out[65]: Text(0,0.5,'GDP per capita')","title":"Select and transform data, then plot it\u00b6 {#Select-and-transform-data,-then-plot-it}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#many-styles-of-plot-are-available-many-styles-of-plot-are-available","text":"For example, do a bar plot using a fancier style. In [66]: plt.style.use('ggplot') data.T.plot(kind='bar') plt.ylabel('GDP per capita') Out[66]: Text(0,0.5,'GDP per capita')","title":"Many styles of plot are available.\u00b6 {#Many-styles-of-plot-are-available.}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#data-can-also-be-plotted-by-calling-the-matplotlib-plot-function-directly-data-can-also-be-plotted-by-calling-the-matplotlib-plot-function-directly","text":"The command is plt.plot(x, y) The color / format of markers can also be specified as an optical argument: e.g. \u2018b-\u2018 is a blue line, \u2018g\u2013\u2019 is a green dashed line. Get Australia data from dataframe In [67]: years = data.columns gdp_australia = data.loc['Australia'] plt.plot(years, gdp_australia, 'g--') Out[67]: [<matplotlib.lines.Line2D at 0x7fdc5fece550>] Can plot many sets of data together. In [68]: # Select two countries' worth of data. gdp_australia = data.loc['Australia'] gdp_nz = data.loc['New Zealand'] # Plot with differently-colored markers. plt.plot(years, gdp_australia, 'b-', label='Australia') plt.plot(years, gdp_nz, 'g-', label='New Zealand') # Create legend. plt.legend(loc='upper left') plt.xlabel('Year') plt.ylabel('GDP per capita ($)') Out[68]: Text(0,0.5,'GDP per capita ($)')","title":"Data can also be plotted by calling the matplotlib plot function directly.\u00b6 {#Data-can-also-be-plotted-by-calling-the-matplotlib-plot-function-directly.}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#add-a-legend-add-a-legend","text":"Often when plotting multiple datasets on the same figure it is desirable to have a legend describing the data. This can be done in matplotlib in two stages: Provide a label for each dataset in the figure: In [69]: plt.plot(years, gdp_australia, label='Australia') plt.plot(years, gdp_nz, label='New Zealand') Out[69]: [<matplotlib.lines.Line2D at 0x7fdc5ff24b70>] Instruct matplotlib to create the legend. In [70]: plt.legend() No handles with labels found to put in legend. Out[70]: <matplotlib.legend.Legend at 0x7fdc5fde26d8> By default matplotlib will attempt to place the legend in a suitable position. If you would rather specify a position this can be done with the loc= argument, e.g to place the legend in the upper left corner of the plot, specify loc='upper left' Plot a scatter plot correlating the GDP of Australia and New Zealand Use either plt.scatter or DataFrame.plot.scatter In [71]: plt.scatter(gdp_australia, gdp_nz) Out[71]: <matplotlib.collections.PathCollection at 0x7fdc5fd5f6d8> In [72]: data.T.plot.scatter(x = 'Australia', y = 'New Zealand') Out[72]: <matplotlib.axes._subplots.AxesSubplot at 0x7fdc5fd99860>","title":"Add a legend\u00b6 {#Add-a-legend}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#exercises-exercises_6","text":"Fill in the blanks below to plot the minimum GDP per capita over time for all the countries in Europe. Modify it again to plot the maximum GDP per capita over time for Europe. In [73]: data_europe = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_europe.csv', index_col='country') data_europe.____.plot(label='min') data_europe.____ plt.legend(loc='best') plt.xticks(rotation=90) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-73-e4db3abbc459> in <module>() 1 data_europe = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_europe.csv', index_col='country') ----> 2 data_europe.____.plot(label='min') 3 data_europe.____ 4 plt.legend(loc='best') 5 plt.xticks(rotation=90) /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in __getattr__(self, name) 3612 if name in self._info_axis: 3613 return self[name] -> 3614 return object.__getattribute__(self, name) 3615 3616 def __setattr__(self, name, value): AttributeError: 'DataFrame' object has no attribute '____' Modify the example in the notes to create a scatter plot showing the relationship between the minimum and maximum GDP per capita among the countries in Asia for each year in the data set. What relationship do you see (if any)? In [0]: data_asia = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_asia.csv', index_col='country') data_asia.describe().T.plot(kind='scatter', x='min', y='max') You might note that the variability in the maximum is much higher than that of the minimum. Take a look at the maximum and the max indexes: In [0]: data_asia = pd.read_csv('/home/mcubero/dataSanJose19/data/gapminder_gdp_asia.csv', index_col='country') data_asia.max().plot() print(data_asia.idxmax()) print(data_asia.idxmin())","title":"Exercises\u00b6 {#Exercises}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#saving-your-plot-to-a-file-saving-your-plot-to-a-file","text":"If you are satisfied with the plot you see you may want to save it to a file, perhaps to include it in a publication. There is a function in the matplotlib.pyplot module that accomplishes this: savefig. Calling this function, e.g. with In [0]: plt.savefig('my_figure.png') will save the current figure to the file my_figure.png. The file format will automatically be deduced from the file name extension (other formats are pdf, ps, eps and svg). Note that functions in plt refer to a global figure variable and after a figure has been displayed to the screen (e.g. with plt.show) matplotlib will make this variable refer to a new empty figure. Therefore, make sure you call plt.savefig before the plot is displayed to the screen, otherwise you may find a file with an empty plot. When using dataframes, data is often generated and plotted to screen in one line, and plt.savefig seems not to be a possible approach. One possibility to save the figure to file is then to save a reference to the current figure in a local variable (with plt.gcf) call the savefig class method from that variable. In [0]: fig = plt.gcf() # get current figure data.plot(kind='bar') fig.savefig('my_figure.png')","title":"Saving your plot to a file\u00b6 {#Saving-your-plot-to-a-file}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#making-your-plots-accessible-making-your-plots-accessible","text":"Whenever you are generating plots to go into a paper or a presentation, there are a few things you can do to make sure that everyone can understand your plots. Always make sure your text is large enough to read. Use the fontsize parameter in xlabel, ylabel, title, and legend, and tick_params with labelsize to increase the text size of the numbers on your axes. Similarly, you should make your graph elements easy to see. Use s to increase the size of your scatterplot markers and linewidth to increase the sizes of your plot lines. Using color (and nothing else) to distinguish between different plot elements will make your plots unreadable to anyone who is colorblind, or who happens to have a black-and-white office printer. For lines, the linestyle parameter lets you use different types of lines. For scatterplots, marker lets you change the shape of your points. If you\u2019re unsure about your colors, you can use Coblis or Color Oracle to simulate what your plots would look like to those with colorblindness.","title":"Making your plots accessible\u00b6 {#Making-your-plots-accessible}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#key-points-key-points_7","text":"matplotlib is the most widely used scientific plotting library in Python. Plot data directly from a Pandas dataframe. Select and transform data, then plot it. Many styles of plot are available: see the Python Graph Gallery for more options. Can plot many sets of data together.","title":"Key Points\u00b6 {#Key-Points}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#programming-style-programming-style","text":"15 minutes Exercises (15 min)","title":"Programming Style\u00b6 {#Programming-Style}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#coding-style-coding-style","text":"Coding style helps us to understand the code better. It helps to maintain and change the code. Python relies strongly on coding style, as we may notice by the indentation we apply to lines to define different blocks of code. Python proposes a standard style through one of its first Python Enhancement Proposals (PEP), PEP8, and highlight the importance of readability in the Zen of Python. Keep in mind: document your code use clear, meaningful variable names use white-space, not tabs, to indent lines Follow standard Python style in your code. PEP8: a style guide for Python that discusses topics such as how you should name variables, how you should use indentation in your code, how you should structure your import statements, etc. Adhering to PEP8 makes it easier for other Python developers to read and understand your code, and to understand what their contributions should look like. The PEP8 application and Python library can check your code for compliance with PEP8. Google style guide on Python supports the use of PEP8 and extend the coding style to more specific structure of a Python code, which may be interesting also to follow.","title":"Coding style\u00b6 {#Coding-style}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#use-assertions-to-check-for-internal-errors-use-assertions-to-check-for-internal-errors","text":"Assertions are a simple, but powerful method for making sure that the context in which your code is executing is as you expect. In [109]: def calc_bulk_density(mass, volume): '''Return dry bulk density = powder mass / powder volume.''' assert volume > 0 return mass / volume In [110]: calc_bulk_density(60, -50) ---------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-110-b0873c16a0ba> in <module>() ----> 1 calc_bulk_density(60, -50) <ipython-input-109-fa5af01ee7ed> in calc_bulk_density(mass, volume) 1 def calc_bulk_density(mass, volume): 2 '''Return dry bulk density = powder mass / powder volume.''' ----> 3 assert volume > 0 4 return mass / volume AssertionError: If the assertion is False, the Python interpreter raises an AssertionError runtime exception. The source code for the expression that failed will be displayed as part of the error message. To ignore assertions in your code run the interpreter with the \u2018-O\u2019 (optimize) switch. Assertions should contain only simple checks and never change the state of the program. For example, an assertion should never contain an assignment.","title":"Use assertions to check for internal errors.\u00b6 {#Use-assertions-to-check-for-internal-errors.}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#use-docstrings-to-provide-online-help-use-docstrings-to-provide-online-help","text":"If the first thing in a function is a character string that is not assignd to a variable, Python attaches it to the function as thee online help. Called a docstring (short fo \"documentation string\"). In [111]: def average(values): \"Return average of values, or None if no values are supplied.\" if len(values) == 0: return None return sum(values) / len(values) help(average) Help on function average in module __main__: average(values) Return average of values, or None if no values are supplied. Also, you can comment your code using multiline strings. These start and end with three quote characters (either single or double) and end with three matching characters. In [112]: import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! In [77]: \"\"\"This string spans multiple lines. Blank lines are allowed.\"\"\" Out[77]: 'This string spans\\nmultiple lines.\\n\\nBlank lines are allowed.'","title":"Use docstrings to provide online help.\u00b6 {#Use-docstrings-to-provide-online-help.}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#exercises-exercises_7","text":"Highlight the lines in the code below that will be available as online help. Are there lines that should be made available, but won\u2019t be? Will any lines produce a syntax error or a runtime error? \"Find maximum edit distance between multiple sequences.\" # This finds the maximum distance between all sequences. def overall_max(sequences): '''Determine overall maximum edit distance.''' highest = 0 for left in sequences: for right in sequences: '''Avoid checking sequence against itself.''' if left != right: this = edit_distance(left, right) highest = max(highest, this) # Report. return highest Turn the comment on the following function into a docstring and check that help displays it properly. In [0]: def middle(a, b, c): # Return the middle value of three. # Assumes the values can actually be compared. values = [a, b, c] values.sort() return values[1] Clean up this code! Read this short program and try to predict what it does. Run it: how accurate was your prediction? Refactor the program to make it more readable. Remember to run it after each change to ensure its behavior hasn\u2019t changed. Compare your rewrite with your neighbor\u2019s. What did you do the same? What did you do differently, and why? n = 10 s = 'et cetera' print(s) i = 0 while i < n: # print('at', j) new = '' for j in range(len(s)): left = j-1 right = (j+1)%len(s) if s[left]==s[right]: new += '-' else: new += '*' s=''.join(new) print(s) i += 1","title":"Exercises\u00b6 {#Exercises}"},{"location":"DataSanJose2019/slides/Unit4_IntrotoPythonSession2/#key-points-key-points_8","text":"Follow standard Python style in your code. Use docstrings to provide online help.","title":"Key Points\u00b6 {#Key-Points}"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/","text":"Visualisation using Seaborn The notes listed here are based on this DataCamp tutorial on Seaborn by Karlijn Willems and this CODATA-RDA module on visualisation by Sara El Jadid . During this module we'll be making use of Seaborn , which provides a high-level interface to draw statistical graphics. Seaborn vs Matplotlib Seaborn is complimentary to Matplotlib and it specifically targets statistical data visualization. But it goes even further than that: Seaborn extends Matplotlib and that\u2019s why it can address the two biggest frustrations of working with Matplotlib. Or, as Michael Waskom says in the \u201c introduction to Seaborn \u201d: \u201cIf matplotlib \u201ctries to make easy things easy and hard things possible\u201d, seaborn tries to make a well-defined set of hard things easy too.\u201d One of these hard things or frustrations had to do with the default Matplotlib parameters. Seaborn works with different parameters, which undoubtedly speaks to those users that don\u2019t use the default looks of the Matplotlib plots. During this module we'll also be making some use of Pandas to extract features of the data that we need. Getting started In the first instance please get yourself set up with a notebook on the Google colab site. Please go to https://colab.research.google.com/notebooks/welcome.ipynb and then click on File and New Python 3 notebook. OR log into the Kabre jupyter server and then click on New and then New Python 3. We'll start with importing a set of libraries that will be useful for us and the gapminder data set. import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set(style=\"darkgrid\") The last line is a choice about how things look - you may want to leave that out. Now we'll read in the data. We will again use the gapminder data set but with the columns labelled differently. Please do not use the version you have used previously. The version is stored on a github repository which has been shortened using bit.ly. url=\"http://bit.ly/2PbVBcR\" #url=\"https://raw.githubusercontent.com/CODATA-RDA-DataScienceSchools/Materials/master/docs/DataSanJose2019/slides/Visualisation/gapminder.csv\" gapminder=pd.read_csv(url) So we are using pandas to read in the data set. The gapminder data set is a Data Frame . Exploring the gapminder data set The gapminder data set is a set of socioeconomic data about populations, GDP per capita and expected life span for a large number of countries over a number of years. We can have a look at it using the head command. gapminder.head() You should get something like this. Unnamed: 0 country continent year lifeExp pop gdpPercap 0 1 Afghanistan Asia 1952 28.801 8425333 779.445314 1 2 Afghanistan Asia 1957 30.332 9240934 820.853030 2 3 Afghanistan Asia 1962 31.997 10267083 853.100710 3 4 Afghanistan Asia 1967 34.020 11537966 836.197138 4 5 Afghanistan Asia 1972 36.088 13079460 739.981106 So it is a combination of categorical data (countries and continents) and quantitative data (year, lifeExp etc.). It's also nice (though unrealistic) that it doesn't have missing values or malformed data e.g. Ireland is written sometimes as \"Ireland\" and sometimes \"ireland\" and sometimes \"Republic of Ireland\" or even \"Eire\"!! Dealing with those kinds of issues is not what we're going to focus on here. We can do a statistical summary of the numerical data using the describe function gapminder.describe() Unnamed: 0 year lifeExp pop gdpPercap count 1704.000000 1704.00000 1704.000000 1.704000e+03 1704.000000 mean 852.500000 1979.50000 59.474439 2.960121e+07 7215.327081 std 492.046746 17.26533 12.917107 1.061579e+08 9857.454543 min 1.000000 1952.00000 23.599000 6.001100e+04 241.165877 25% 426.750000 1965.75000 48.198000 2.793664e+06 1202.060309 50% 852.500000 1979.50000 60.712500 7.023596e+06 3531.846989 75% 1278.250000 1993.25000 70.845500 1.958522e+07 9325.462346 max 1704.000000 2007.00000 82.603000 1.318683e+09 113523.132900 One can find the names of the continents by executing the following. list(set(gapminder.continent)) We note that gapminder.continent gives us the list with the column corresponding to the continent entry. The set command converts the list into a set (which only has unique entries) and then list turns that back into a list again. We can list these entries alphabetically from the command sorted(list(set(gapminder.continent))) Exercise What does the function sorted do? Do the same for the countries. The gsapminder data set also presents lots of questions such as Is there a relationship between gdpPercap (roughly a measure of the average wealth of each person in a country) and their average life span? Is the average life span changing over time? How does picture change over different countries or comntinents? Visualisation allows us to explore all of this! Getting started with seaborn Let us start with doing box plots which count the number of entries that we have for each continent. sns.countplot(x=\"continent\", data=gapminder) You should get the folllowing. Note that generically seaborn generally looks like this. sns. (x= , y= , ... , data =< a data frame>) We use countplot here to just count entries and plot them as a box plot. Exercise What happens if we do the following? sns.countplot(x=\"Continent\", data=gapminder) What does that tell us? Printing out You can save a figure as a PNG or as a PDF then in the same cell as the command you run to plot use the savefig command. sns.countplot(x=\"continent\", data=gapminder) plt.savefig(\"Histogram.png\") plt.savefig(\"Histogram.pdf\") Looking at 1-d distributions We can use the command catplot to just look at the distribution of life expenctancies. sns.catplot(y=\"lifeExp\", data=gapminder) You should get something like this. The points are jittered i.e. randomly moved in the horizontal axis to make things clearer. We can switch that off if we wish. sns.catplot(y=\"lifeExp\", data=gapminder,jitter=False) This scatterplot is not very informative! We can create a box plot of the data sns.boxplot(y=\"lifeExp\", data=gapminder) This should give the following. Exercise One can use another type of plot called a violin plot which tries to summarise the distribution better than a boxplot. The width of the violin plot represents how big the distribution is at that value. It is quite useful for picking out multi-modal (one with a distribution that has more than one peak). The command in seaborn is violinplot. Try and implement this for this data. Diving deeper into the data Just looking at the life expectancy for all of the countries isn't very informative. The first thing we can do is ask how does this vary across continents. Seaborn does this easily by introducing an x-axis which is the continent. Again, let's try with just the points. sns.catplot(x=\"continent\", y=\"lifeExp\", data=gapminder) Exercise Repeat this using box plots (and violin plots if you wish). Repeat the above steps using GDP per capita (gdpPercap) instead of life expectancy. You can also try the swarmplot function as another way to represent this data. Can we just draw a distribution or a histogram? What about dividing it as continents? Yes! But we'll get to that in a bit. Ordering Plotting the box plots with the continents in alphabetical order is quite easy. orderedContinents = sorted(list(set(gapminder.continent))) orderedContinents orderedContinents is a list with the continents ordered. sns.boxplot(x=\"continent\", y=\"lifeExp\", order=orderedContinents, data=gapminder) On the other hand we may want to order the box plots in ascending order of the medians of the life expectancy. This is more involved but is a good exercise in manipulating the data. #Create an empty dictionary medianLifeExps = {} # Loop through all the continents for val in gapminder.continent: # Create a new key which is the median life expectancy of that continent # gapminder[gapminder.continent == val] pulls out the continent in that loop # the .lifeExp.median() part then computes the median # of the remaining life expectancy data key = gapminder[gapminder.continent==val].lifeExp.median() # create a new entry in the dictionary with the continent as the value and the key # as the median. medianLifeExps[key] = val # Create a sorted list of the medians (in ascending order) sortedKeys = sorted(medianLifeExps) # Finally return the list of continents in that order orderedMedianContinents = [] for m in orderedMedians: orderedMedianContinents.append(medianLifeExps[m]) sns.boxplot(x=\"continent\", y=\"lifeExp\", order=orderedMedianContinents, data=gapminder) Exercise Do the same plot for GDP per capita. Line and scatter plots Instead of having a categorical variable on the horizontal access we now do scatter and line plots. Let's start with the whole data set. sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data=gapminder) This is hard to grasp as a whole, so we'll just consider one country - China. We can select data corresponding to China by the following. gapminder[gapminder.country==\"China\"] sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) Exercise Do the same for your country - if it isn't listed in gapminder then pick another. A line plot works in the same way. It's possible to overlay these in the same cell. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) Expressing more variables with different attributes It is hard to make out the points from the lines. We can change the colour (color) of the points accordingly. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",color=\"red\",data= gapminder[gapminder.country==\"China\"]) We can use the colour of the points to represent a different column - e.g. the year. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\", data= gapminder[gapminder.country==\"China\"]) The problem here is that only a certain number of years have been picked out. We need to tell seaborn how many years there are and how to set a palette of colours for this (the default palette has six colours). # Find the number of years (why only set?) n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) # We use a rainbow-like palette but there are others. sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"]) We can use the size of the point to represent an additional column - the population. n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",size=\"pop\", palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"]) The problem now is that there is too much detail in the legend - so we'll switch that off. n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",size=\"pop\", palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"], legend=False) How useful is adding these attributes? Exercise Pick another country and try this out. Costa Rica is interesting. Does anybody have an explanation? Summarising We will now examine how life expenctancy has varied over time. sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"country\",data= gapminder,legend=False) There are so many countries here I haven't even tried to construct a palette! Looking at this many countries are generally increasing but some are not following that trend. We could explore those outliers but here we will focus on trying to summarise what is going on (is a particular continent not going with the trend of increasing life span over time?) To do this we need to use another pandas command groupby which creates a new data frame for particular variables. Once we have created that new data frame we can then plot the data. # First create a new data frame which has the medians, by year and continent, of the life expectancy medianGapminder = gapminder.groupby(['continent','year']).lifeExp.median() # Now plot it sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"continent\",data= medianGapminder) What happened? The groupby command makes continent and year indices of the data (you can see this if you print out the data frame). Having created the data frame we need to reset the indices. # First create a new data frame which has the medians, by year and continent, of the life expectancy medianGapminder = gapminder.groupby(['continent','year']).lifeExp.median() # Now plot it sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"continent\",data= medianGapminder.reset_index()) Exercise Repeat this using the GDP per capita. Use a different statistical summary apart from the median. Regression The line plots just \"join the dots\". It is more intesting to try and fit the data to a curve. We also want to do the fit and distinguish between the different continents. We can do this using the lmplot command. sns.lmplot(x=\"year\",y=\"lifeExp\",hue=\"continent\", data= medianGapminder.reset_index()) This does a simple linear regression. We can do more sophisticated types of fit, for example Loess (or Lowess). sns.lmplot(x=\"year\",y=\"lifeExp\",hue=\"continent\", lowess=True, data= medianGapminder.reset_index()) Exercise Repeat the above using the GDP per capita. More regression Now that we know how to fit curves through data with a number of different variables we can go back to the case of where we plotted the life expectancy against the GDP per capita. Instead of just doing a scatter plot we can now do regression (curve fitting) as function of the continent as well so we can see how the life expectancy varies between GDP per capita and the continent. sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True,data=gapminder) The problem here is that points are too large - we cannot see the trend. When you have many points make the point size smaller, indeed way smaller (someone described it as 'dust size') to see the trend better. Since Seaborn is based on Matplotlib we need to use a slightly different notation in the arguments to what was used previously. # scatter_kws is passed onto the underlying matplotlib plotting routine. sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True,data=gapminder,scatter_kws={\"s\":5}) The GDP per capita varies over a wide range and it would be good in the first instance to plot the x-axis on a logarithmic scale. Again since Seaborn is based on Matplotlib we need to use a slightly different notation to what was used previously. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xscale=\"log\") We now get a much better spread of the data but it's still hard to see how the different continents are behaving. To do that we make use of facet plots . These are simply plots of different but related variables that are organised on the same screen for easy comparison. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\") We note that the axes of all of these plots are the same so we can do a valid comparison. Still these plots are quite squashed as thy try and fit to the width of the page. Instead we can wrap them around. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", col_wrap=2, lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\") Finally, we can adjust the colour of the individual plots. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", hue=\"continent\", col_wrap=2, lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\") Exercise Do the same type of plots for life expectancy against year. Distributions We can plot the distribution of a list of data (one column from a data frame) using a kernal density approach and/or a histogram. sns.distplot(gapminder.lifeExp) distplot only allows a single column from a data frame! You can also add the raw data into this plot as well (although this isn't very useful in this case as there's so much data). sns.distplot(gapminder.lifeExp,rug=True) Exercise Do the same type of plot with the GDP per capita data. Again we would like to break this down in separate continents. Again we will make use of facet plots. lmplot is designed to create facet plots but distplot isn't so we need to use a specific function called FacetGrid to do this. In the call below we also adjust the height and aspect (the height/width ratio) of the figures. # Create a facet of the gapminder data based on the continents ordered alphabetically (orderedContinents) g = sns.FacetGrid(gapminder, row=\"continent\", row_order=orderedContinents, height=2, aspect=4) # Plot on the facets the distribution of the life expctancy data, but don't plot the histogram. g.map(sns.distplot, \"lifeExp\", hist=False) Finally the facet plot can also be used with just one facet! # Create a facet plot with one facet but colour on continent. g = sns.FacetGrid(gapminder, hue=\"continent\",height=2, aspect=4) # Plot the distributions with no histogram g.map(sns.distplot, \"lifeExp\", hist=False) # Give the colour scheme in a legend g.add_legend() Exercise Do the same type of plot with the GDP per capita data (Longer) Select the data from the gapminder data set for a particular continent and now create facet plots for those countries.","title":"Visualisation using Seaborn"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#visualisation-using-seaborn","text":"The notes listed here are based on this DataCamp tutorial on Seaborn by Karlijn Willems and this CODATA-RDA module on visualisation by Sara El Jadid . During this module we'll be making use of Seaborn , which provides a high-level interface to draw statistical graphics.","title":"Visualisation using Seaborn"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#seaborn-vs-matplotlib","text":"Seaborn is complimentary to Matplotlib and it specifically targets statistical data visualization. But it goes even further than that: Seaborn extends Matplotlib and that\u2019s why it can address the two biggest frustrations of working with Matplotlib. Or, as Michael Waskom says in the \u201c introduction to Seaborn \u201d: \u201cIf matplotlib \u201ctries to make easy things easy and hard things possible\u201d, seaborn tries to make a well-defined set of hard things easy too.\u201d One of these hard things or frustrations had to do with the default Matplotlib parameters. Seaborn works with different parameters, which undoubtedly speaks to those users that don\u2019t use the default looks of the Matplotlib plots. During this module we'll also be making some use of Pandas to extract features of the data that we need.","title":"Seaborn vs Matplotlib"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#getting-started","text":"In the first instance please get yourself set up with a notebook on the Google colab site. Please go to https://colab.research.google.com/notebooks/welcome.ipynb and then click on File and New Python 3 notebook.","title":"Getting started"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#or","text":"log into the Kabre jupyter server and then click on New and then New Python 3. We'll start with importing a set of libraries that will be useful for us and the gapminder data set. import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set(style=\"darkgrid\") The last line is a choice about how things look - you may want to leave that out. Now we'll read in the data. We will again use the gapminder data set but with the columns labelled differently. Please do not use the version you have used previously. The version is stored on a github repository which has been shortened using bit.ly. url=\"http://bit.ly/2PbVBcR\" #url=\"https://raw.githubusercontent.com/CODATA-RDA-DataScienceSchools/Materials/master/docs/DataSanJose2019/slides/Visualisation/gapminder.csv\" gapminder=pd.read_csv(url) So we are using pandas to read in the data set. The gapminder data set is a Data Frame .","title":"OR"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#exploring-the-gapminder-data-set","text":"The gapminder data set is a set of socioeconomic data about populations, GDP per capita and expected life span for a large number of countries over a number of years. We can have a look at it using the head command. gapminder.head() You should get something like this. Unnamed: 0 country continent year lifeExp pop gdpPercap 0 1 Afghanistan Asia 1952 28.801 8425333 779.445314 1 2 Afghanistan Asia 1957 30.332 9240934 820.853030 2 3 Afghanistan Asia 1962 31.997 10267083 853.100710 3 4 Afghanistan Asia 1967 34.020 11537966 836.197138 4 5 Afghanistan Asia 1972 36.088 13079460 739.981106 So it is a combination of categorical data (countries and continents) and quantitative data (year, lifeExp etc.). It's also nice (though unrealistic) that it doesn't have missing values or malformed data e.g. Ireland is written sometimes as \"Ireland\" and sometimes \"ireland\" and sometimes \"Republic of Ireland\" or even \"Eire\"!! Dealing with those kinds of issues is not what we're going to focus on here. We can do a statistical summary of the numerical data using the describe function gapminder.describe() Unnamed: 0 year lifeExp pop gdpPercap count 1704.000000 1704.00000 1704.000000 1.704000e+03 1704.000000 mean 852.500000 1979.50000 59.474439 2.960121e+07 7215.327081 std 492.046746 17.26533 12.917107 1.061579e+08 9857.454543 min 1.000000 1952.00000 23.599000 6.001100e+04 241.165877 25% 426.750000 1965.75000 48.198000 2.793664e+06 1202.060309 50% 852.500000 1979.50000 60.712500 7.023596e+06 3531.846989 75% 1278.250000 1993.25000 70.845500 1.958522e+07 9325.462346 max 1704.000000 2007.00000 82.603000 1.318683e+09 113523.132900 One can find the names of the continents by executing the following. list(set(gapminder.continent)) We note that gapminder.continent gives us the list with the column corresponding to the continent entry. The set command converts the list into a set (which only has unique entries) and then list turns that back into a list again. We can list these entries alphabetically from the command sorted(list(set(gapminder.continent)))","title":"Exploring the gapminder data set"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#exercise","text":"What does the function sorted do? Do the same for the countries. The gsapminder data set also presents lots of questions such as Is there a relationship between gdpPercap (roughly a measure of the average wealth of each person in a country) and their average life span? Is the average life span changing over time? How does picture change over different countries or comntinents? Visualisation allows us to explore all of this!","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#getting-started-with-seaborn","text":"Let us start with doing box plots which count the number of entries that we have for each continent. sns.countplot(x=\"continent\", data=gapminder) You should get the folllowing. Note that generically seaborn generally looks like this. sns. (x= , y= , ... , data =< a data frame>) We use countplot here to just count entries and plot them as a box plot.","title":"Getting started with seaborn"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#exercise_1","text":"What happens if we do the following? sns.countplot(x=\"Continent\", data=gapminder) What does that tell us?","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#printing-out","text":"You can save a figure as a PNG or as a PDF then in the same cell as the command you run to plot use the savefig command. sns.countplot(x=\"continent\", data=gapminder) plt.savefig(\"Histogram.png\") plt.savefig(\"Histogram.pdf\")","title":"Printing out"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#looking-at-1-d-distributions","text":"We can use the command catplot to just look at the distribution of life expenctancies. sns.catplot(y=\"lifeExp\", data=gapminder) You should get something like this. The points are jittered i.e. randomly moved in the horizontal axis to make things clearer. We can switch that off if we wish. sns.catplot(y=\"lifeExp\", data=gapminder,jitter=False) This scatterplot is not very informative! We can create a box plot of the data sns.boxplot(y=\"lifeExp\", data=gapminder) This should give the following.","title":"Looking at 1-d distributions"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#exercise_2","text":"One can use another type of plot called a violin plot which tries to summarise the distribution better than a boxplot. The width of the violin plot represents how big the distribution is at that value. It is quite useful for picking out multi-modal (one with a distribution that has more than one peak). The command in seaborn is violinplot. Try and implement this for this data.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#diving-deeper-into-the-data","text":"Just looking at the life expectancy for all of the countries isn't very informative. The first thing we can do is ask how does this vary across continents. Seaborn does this easily by introducing an x-axis which is the continent. Again, let's try with just the points. sns.catplot(x=\"continent\", y=\"lifeExp\", data=gapminder)","title":"Diving deeper into the data"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#exercise_3","text":"Repeat this using box plots (and violin plots if you wish). Repeat the above steps using GDP per capita (gdpPercap) instead of life expectancy. You can also try the swarmplot function as another way to represent this data. Can we just draw a distribution or a histogram? What about dividing it as continents? Yes! But we'll get to that in a bit.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#ordering","text":"Plotting the box plots with the continents in alphabetical order is quite easy. orderedContinents = sorted(list(set(gapminder.continent))) orderedContinents orderedContinents is a list with the continents ordered. sns.boxplot(x=\"continent\", y=\"lifeExp\", order=orderedContinents, data=gapminder) On the other hand we may want to order the box plots in ascending order of the medians of the life expectancy. This is more involved but is a good exercise in manipulating the data. #Create an empty dictionary medianLifeExps = {} # Loop through all the continents for val in gapminder.continent: # Create a new key which is the median life expectancy of that continent # gapminder[gapminder.continent == val] pulls out the continent in that loop # the .lifeExp.median() part then computes the median # of the remaining life expectancy data key = gapminder[gapminder.continent==val].lifeExp.median() # create a new entry in the dictionary with the continent as the value and the key # as the median. medianLifeExps[key] = val # Create a sorted list of the medians (in ascending order) sortedKeys = sorted(medianLifeExps) # Finally return the list of continents in that order orderedMedianContinents = [] for m in orderedMedians: orderedMedianContinents.append(medianLifeExps[m]) sns.boxplot(x=\"continent\", y=\"lifeExp\", order=orderedMedianContinents, data=gapminder)","title":"Ordering"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#exercise_4","text":"Do the same plot for GDP per capita.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#line-and-scatter-plots","text":"Instead of having a categorical variable on the horizontal access we now do scatter and line plots. Let's start with the whole data set. sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data=gapminder) This is hard to grasp as a whole, so we'll just consider one country - China. We can select data corresponding to China by the following. gapminder[gapminder.country==\"China\"] sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"])","title":"Line and scatter plots"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#exercise_5","text":"Do the same for your country - if it isn't listed in gapminder then pick another. A line plot works in the same way. It's possible to overlay these in the same cell. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"])","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#expressing-more-variables-with-different-attributes","text":"It is hard to make out the points from the lines. We can change the colour (color) of the points accordingly. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",color=\"red\",data= gapminder[gapminder.country==\"China\"]) We can use the colour of the points to represent a different column - e.g. the year. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\", data= gapminder[gapminder.country==\"China\"]) The problem here is that only a certain number of years have been picked out. We need to tell seaborn how many years there are and how to set a palette of colours for this (the default palette has six colours). # Find the number of years (why only set?) n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) # We use a rainbow-like palette but there are others. sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"]) We can use the size of the point to represent an additional column - the population. n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",size=\"pop\", palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"]) The problem now is that there is too much detail in the legend - so we'll switch that off. n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",size=\"pop\", palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"], legend=False) How useful is adding these attributes?","title":"Expressing more variables with different attributes"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#exercise_6","text":"Pick another country and try this out. Costa Rica is interesting. Does anybody have an explanation?","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#summarising","text":"We will now examine how life expenctancy has varied over time. sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"country\",data= gapminder,legend=False) There are so many countries here I haven't even tried to construct a palette! Looking at this many countries are generally increasing but some are not following that trend. We could explore those outliers but here we will focus on trying to summarise what is going on (is a particular continent not going with the trend of increasing life span over time?) To do this we need to use another pandas command groupby which creates a new data frame for particular variables. Once we have created that new data frame we can then plot the data. # First create a new data frame which has the medians, by year and continent, of the life expectancy medianGapminder = gapminder.groupby(['continent','year']).lifeExp.median() # Now plot it sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"continent\",data= medianGapminder) What happened? The groupby command makes continent and year indices of the data (you can see this if you print out the data frame). Having created the data frame we need to reset the indices. # First create a new data frame which has the medians, by year and continent, of the life expectancy medianGapminder = gapminder.groupby(['continent','year']).lifeExp.median() # Now plot it sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"continent\",data= medianGapminder.reset_index())","title":"Summarising"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#exercise_7","text":"Repeat this using the GDP per capita. Use a different statistical summary apart from the median.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#regression","text":"The line plots just \"join the dots\". It is more intesting to try and fit the data to a curve. We also want to do the fit and distinguish between the different continents. We can do this using the lmplot command. sns.lmplot(x=\"year\",y=\"lifeExp\",hue=\"continent\", data= medianGapminder.reset_index()) This does a simple linear regression. We can do more sophisticated types of fit, for example Loess (or Lowess). sns.lmplot(x=\"year\",y=\"lifeExp\",hue=\"continent\", lowess=True, data= medianGapminder.reset_index())","title":"Regression"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#exercise_8","text":"Repeat the above using the GDP per capita.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#more-regression","text":"Now that we know how to fit curves through data with a number of different variables we can go back to the case of where we plotted the life expectancy against the GDP per capita. Instead of just doing a scatter plot we can now do regression (curve fitting) as function of the continent as well so we can see how the life expectancy varies between GDP per capita and the continent. sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True,data=gapminder) The problem here is that points are too large - we cannot see the trend. When you have many points make the point size smaller, indeed way smaller (someone described it as 'dust size') to see the trend better. Since Seaborn is based on Matplotlib we need to use a slightly different notation in the arguments to what was used previously. # scatter_kws is passed onto the underlying matplotlib plotting routine. sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True,data=gapminder,scatter_kws={\"s\":5}) The GDP per capita varies over a wide range and it would be good in the first instance to plot the x-axis on a logarithmic scale. Again since Seaborn is based on Matplotlib we need to use a slightly different notation to what was used previously. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xscale=\"log\") We now get a much better spread of the data but it's still hard to see how the different continents are behaving. To do that we make use of facet plots . These are simply plots of different but related variables that are organised on the same screen for easy comparison. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\") We note that the axes of all of these plots are the same so we can do a valid comparison. Still these plots are quite squashed as thy try and fit to the width of the page. Instead we can wrap them around. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", col_wrap=2, lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\") Finally, we can adjust the colour of the individual plots. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", hue=\"continent\", col_wrap=2, lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\")","title":"More regression"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#exercise_9","text":"Do the same type of plots for life expectancy against year.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#distributions","text":"We can plot the distribution of a list of data (one column from a data frame) using a kernal density approach and/or a histogram. sns.distplot(gapminder.lifeExp) distplot only allows a single column from a data frame! You can also add the raw data into this plot as well (although this isn't very useful in this case as there's so much data). sns.distplot(gapminder.lifeExp,rug=True)","title":"Distributions"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#exercise_10","text":"Do the same type of plot with the GDP per capita data. Again we would like to break this down in separate continents. Again we will make use of facet plots. lmplot is designed to create facet plots but distplot isn't so we need to use a specific function called FacetGrid to do this. In the call below we also adjust the height and aspect (the height/width ratio) of the figures. # Create a facet of the gapminder data based on the continents ordered alphabetically (orderedContinents) g = sns.FacetGrid(gapminder, row=\"continent\", row_order=orderedContinents, height=2, aspect=4) # Plot on the facets the distribution of the life expctancy data, but don't plot the histogram. g.map(sns.distplot, \"lifeExp\", hist=False) Finally the facet plot can also be used with just one facet! # Create a facet plot with one facet but colour on continent. g = sns.FacetGrid(gapminder, hue=\"continent\",height=2, aspect=4) # Plot the distributions with no histogram g.map(sns.distplot, \"lifeExp\", hist=False) # Give the colour scheme in a legend g.add_legend()","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation_using_Seaborn/#exercise_11","text":"Do the same type of plot with the GDP per capita data (Longer) Select the data from the gapminder data set for a particular continent and now create facet plots for those countries.","title":"Exercise"},{"location":"DataSaoPaulo2018/","text":"Sao Paulo School of Research Data Science December 3 \u2013 14, 2018 ICTP-SAIFR, Sao Paulo, Brazil Background and purpose The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, ICTP and SAIFR, was conducted at the ICTP-SAIFR, Sao Paulo to introduce participants to the skills of RDS. Materials for the 2018 School of Research Data Science in Sao Paulo Day 1 - Introduction, Open Science , UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R , Reports Knitr Day 4 - Data Visualization Day 5 - Research Data Management , Data Management Plans Day 6 - Open Science , Information Security Day 7 - Overview of Machine Learning Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Lecture 4 Day 10 - Computational Infrastructures Wrap-Up - Lecture 5 , Lecture 6 School Close Out","title":"Sao Paulo School of Research Data Science"},{"location":"DataSaoPaulo2018/#sao-paulo-school-of-research-data-science","text":"December 3 \u2013 14, 2018 ICTP-SAIFR, Sao Paulo, Brazil","title":"Sao Paulo School of Research Data Science"},{"location":"DataSaoPaulo2018/#background-and-purpose","text":"The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, ICTP and SAIFR, was conducted at the ICTP-SAIFR, Sao Paulo to introduce participants to the skills of RDS.","title":"Background and purpose"},{"location":"DataSaoPaulo2018/#materials-for-the-2018-school-of-research-data-science-in-sao-paulo","text":"Day 1 - Introduction, Open Science , UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R , Reports Knitr Day 4 - Data Visualization Day 5 - Research Data Management , Data Management Plans Day 6 - Open Science , Information Security Day 7 - Overview of Machine Learning Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Lecture 4 Day 10 - Computational Infrastructures Wrap-Up - Lecture 5 , Lecture 6 School Close Out","title":"Materials for the 2018 School of Research Data Science in Sao Paulo"},{"location":"DataSaoPaulo2018/word_clouds/Readme/","text":"Code and instructions to create the word clouds can be found in here","title":"Readme"},{"location":"DataTrieste2019/","text":"Trieste School of Research Data Science 5-16 August, 2019 ICTP, Trieste, Italy Background and purpose The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA and ICTP, was conducted at the ICTP, Trieste to introduce participants to the skills of RDS. Materials for the 2019 School of Research Data Science in Trieste Day 1 - Introduction Open Science UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R Day 4 - Data Visualization Day 5 - Research Data Management , Data Management Plans Day 6 - Open Science , Information Security Day 7 - Overview of Machine Learning Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Lecture 4 Day 10 - Computational Infrastructures Wrap-Up - Lecture 5 , Lecture 6 School Close Out","title":"Trieste School of Research Data Science"},{"location":"DataTrieste2019/#trieste-school-of-research-data-science","text":"5-16 August, 2019 ICTP, Trieste, Italy","title":"Trieste School of Research Data Science"},{"location":"DataTrieste2019/#background-and-purpose","text":"The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA and ICTP, was conducted at the ICTP, Trieste to introduce participants to the skills of RDS.","title":"Background and purpose"},{"location":"DataTrieste2019/#materials-for-the-2019-school-of-research-data-science-in-trieste","text":"Day 1 - Introduction Open Science UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R Day 4 - Data Visualization Day 5 - Research Data Management , Data Management Plans Day 6 - Open Science , Information Security Day 7 - Overview of Machine Learning Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Lecture 4 Day 10 - Computational Infrastructures Wrap-Up - Lecture 5 , Lecture 6 School Close Out","title":"Materials for the 2019 School of Research Data Science in Trieste"}]}