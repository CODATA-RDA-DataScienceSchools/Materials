{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CODATA/RDA Schools for Research Data Science Materials from DataKigali 2018 Materials from DataSaoPaulo 2018 Materials from DataTrieste 2019 Materials from DataSanJose 2019","title":"Home"},{"location":"#codatarda-schools-for-research-data-science","text":"Materials from DataKigali 2018 Materials from DataSaoPaulo 2018 Materials from DataTrieste 2019 Materials from DataSanJose 2019","title":"CODATA/RDA Schools for Research Data Science"},{"location":"DataKigali2018/","text":"Kigali Foundational School in Research Data Science 22 October \u2013 2 November, 2018 University of Rwanda, Kigali, Rwanda Background and purpose The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, ICTP and EAIFR, was conducted at the University of Rwanda, Kigali to introduce participants to the skills of RDS. Materials for the 2018 School of Research Data Science in Kigali Day 1 - Introduction, Open Science , UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R Day 4 - Research Data Management Day 5 - Research Data Management , Open Science Day 6 - Data Visualization Day 7 - Overview of Machine Learning - 20of 20Machine 20Learning 202018.pdf\">Fundamentals , 20Systems 202018.pdf\">Recommender Systems Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Day 10 - Computational Infrastructures Wrap-Up - Lecture 4 , School Close Out","title":"Kigali Foundational School in Research Data Science"},{"location":"DataKigali2018/#kigali-foundational-school-in-research-data-science","text":"22 October \u2013 2 November, 2018 University of Rwanda, Kigali, Rwanda","title":"Kigali Foundational School in Research Data Science"},{"location":"DataKigali2018/#background-and-purpose","text":"The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, ICTP and EAIFR, was conducted at the University of Rwanda, Kigali to introduce participants to the skills of RDS.","title":"Background and purpose"},{"location":"DataKigali2018/#materials-for-the-2018-school-of-research-data-science-in-kigali","text":"Day 1 - Introduction, Open Science , UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R Day 4 - Research Data Management Day 5 - Research Data Management , Open Science Day 6 - Data Visualization Day 7 - Overview of Machine Learning - 20of 20Machine 20Learning 202018.pdf\">Fundamentals , 20Systems 202018.pdf\">Recommender Systems Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Day 10 - Computational Infrastructures Wrap-Up - Lecture 4 , School Close Out","title":"Materials for the 2018 School of Research Data Science in Kigali"},{"location":"DataKigali2018/slides/readme/","text":"Slides folder","title":"Readme"},{"location":"DataKigali2018/slides/DataVizMaterials/readme/","text":"download the folder for data visualization materials","title":"Readme"},{"location":"DataSanJose2019/","text":"San Jos\u00e9 School of Research Data Science December 2 \u2013 13, 2018 CeNAT, San Jos\u00e9, Costa Rica Background and purpose The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, CONARE and CeNAT, was conducted at the CeNAT, San Jos\u00e9 to introduce participants to the skills of RDS. Materials for the 2019 School of Research Data Science in San Jos\u00e9 Day 1 - Introduction , Open Science , UNIX Shell Day 2 - Version Control with Git , Introduction to Python Day 3 - Introduction to Python Day 4 - Author Carpentry , Intro to Research Data Management Day 5 - Research Data Management , Data Management Plans Day 6 - 20using 20Seaborn.md\">Data Visualization Day 6 - Open Science , Information Security Day 7 - Overview of Machine Learning Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Lecture 4 Day 10 - Computational Infrastructures Wrap-Up - Lecture 5 , Lecture 6 School Close Out","title":"San Jos\u00e9 School of Research Data Science"},{"location":"DataSanJose2019/#san-jose-school-of-research-data-science","text":"December 2 \u2013 13, 2018 CeNAT, San Jos\u00e9, Costa Rica","title":"San Jos\u00e9 School of Research Data Science"},{"location":"DataSanJose2019/#background-and-purpose","text":"The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, CONARE and CeNAT, was conducted at the CeNAT, San Jos\u00e9 to introduce participants to the skills of RDS.","title":"Background and purpose"},{"location":"DataSanJose2019/#materials-for-the-2019-school-of-research-data-science-in-san-jose","text":"Day 1 - Introduction , Open Science , UNIX Shell Day 2 - Version Control with Git , Introduction to Python Day 3 - Introduction to Python Day 4 - Author Carpentry , Intro to Research Data Management Day 5 - Research Data Management , Data Management Plans Day 6 - 20using 20Seaborn.md\">Data Visualization Day 6 - Open Science , Information Security Day 7 - Overview of Machine Learning Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Lecture 4 Day 10 - Computational Infrastructures Wrap-Up - Lecture 5 , Lecture 6 School Close Out","title":"Materials for the 2019 School of Research Data Science in San Jos\u00e9"},{"location":"DataSanJose2019/CI/00-Pre-Introduction-Login/","text":"Install an SSH-Cleint Download and install a SSH Client. We recommend PuTTY, but any ssh client is acceptable. https://www.putty.org/ Direct link to executable After opening the executable you should see the following screen: Authentication for CI Exercises You will receive login credentials at the beginning of this session. To authenticate (prove you who you say you are and establish what you are authorized to do) the bridgekeeper (login nodes) requires three bits of information: From Monty Python and the Holy Grail: BRIDGEKEEPER: Hee hee heh. Stop! What... is your name?\\ ARTHUR: It is 'Arthur', King of the Britons.\\ BRIDGEKEEPER: What... is your quest?\\ ARTHUR: To seek the Holy Grail.\\ BRIDGEKEEPER: What... is the air-speed velocity of an unladen swallow?\\ ARTHUR: What do you mean? An African or European swallow?\\ BRIDGEKEEPER: Huh? I-- I don't know that. Auuuuuuuugh!\\ BEDEVERE: How do know so much about swallows?\\ ARTHUR: Well, you have to know these things when you're a king, you know. or... 1) Tell me who you are. 2) Tell me something only you know. 3) Show me someething only you have. Why do you need both a password and a key? What is the role of the password in the public-private key scheme? Where you will work You will be logging into training.osgconnect.net for the CyberInfrastructure exercises. To confirm you have the proper authentication and authorization to do the exercises tomorrow and Friday we will test logins today. Due to the local network firewall setup (another security mechanism) and key installation, we will go to Brazil first (thanks to Raphael for setting up temporary VM). First be sure you are on the wireless network Eventos CeNAT . Replace XX with your osguser ID and use the password you have been supplied with the following command. ssh -o PreferredAuthentications=password osguserXX@200.145.46.31 If you are using putty, you should fill the Host Name (or IP address) with the value 200.145.46.31 as seen below: After hitting the Open button you may see the following message: You should hit the Yes button. Login on our submission node using the following command along with the password you have been supplied. $ ssh training.osgconnect.net The authenticity of host 'training.osgconnect.net (128.135.158.220)' can't be established. ECDSA key fingerprint is SHA256:gielJSpIiZisxGna5ocHtiK+0zAqFTdcEkLBOgnDUsg. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added 'training.osgconnect.net,128.135.158.220' (ECDSA) to the list of known hosts. Enter passphrase for key '/home/osguser01/.ssh/id_rsa': You may get a message asking you to establish the authenticity of this connection. Answer \"yes\". When you login to the machine you will be in your \"home directory\". We recommend that you work in this directory as nobody else can modify the files here (what security concept we covered today does this recommendation satisfy?).","title":"00 Pre Introduction Login"},{"location":"DataSanJose2019/CI/00-Pre-Introduction-Login/#install-an-ssh-cleint","text":"Download and install a SSH Client. We recommend PuTTY, but any ssh client is acceptable. https://www.putty.org/ Direct link to executable After opening the executable you should see the following screen:","title":"Install an SSH-Cleint"},{"location":"DataSanJose2019/CI/00-Pre-Introduction-Login/#authentication-for-ci-exercises","text":"You will receive login credentials at the beginning of this session. To authenticate (prove you who you say you are and establish what you are authorized to do) the bridgekeeper (login nodes) requires three bits of information: From Monty Python and the Holy Grail: BRIDGEKEEPER: Hee hee heh. Stop! What... is your name?\\ ARTHUR: It is 'Arthur', King of the Britons.\\ BRIDGEKEEPER: What... is your quest?\\ ARTHUR: To seek the Holy Grail.\\ BRIDGEKEEPER: What... is the air-speed velocity of an unladen swallow?\\ ARTHUR: What do you mean? An African or European swallow?\\ BRIDGEKEEPER: Huh? I-- I don't know that. Auuuuuuuugh!\\ BEDEVERE: How do know so much about swallows?\\ ARTHUR: Well, you have to know these things when you're a king, you know. or... 1) Tell me who you are. 2) Tell me something only you know. 3) Show me someething only you have. Why do you need both a password and a key? What is the role of the password in the public-private key scheme?","title":"Authentication for CI Exercises"},{"location":"DataSanJose2019/CI/00-Pre-Introduction-Login/#where-you-will-work","text":"You will be logging into training.osgconnect.net for the CyberInfrastructure exercises. To confirm you have the proper authentication and authorization to do the exercises tomorrow and Friday we will test logins today. Due to the local network firewall setup (another security mechanism) and key installation, we will go to Brazil first (thanks to Raphael for setting up temporary VM). First be sure you are on the wireless network Eventos CeNAT . Replace XX with your osguser ID and use the password you have been supplied with the following command. ssh -o PreferredAuthentications=password osguserXX@200.145.46.31 If you are using putty, you should fill the Host Name (or IP address) with the value 200.145.46.31 as seen below: After hitting the Open button you may see the following message: You should hit the Yes button. Login on our submission node using the following command along with the password you have been supplied. $ ssh training.osgconnect.net The authenticity of host 'training.osgconnect.net (128.135.158.220)' can't be established. ECDSA key fingerprint is SHA256:gielJSpIiZisxGna5ocHtiK+0zAqFTdcEkLBOgnDUsg. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added 'training.osgconnect.net,128.135.158.220' (ECDSA) to the list of known hosts. Enter passphrase for key '/home/osguser01/.ssh/id_rsa': You may get a message asking you to establish the authenticity of this connection. Answer \"yes\". When you login to the machine you will be in your \"home directory\". We recommend that you work in this directory as nobody else can modify the files here (what security concept we covered today does this recommendation satisfy?).","title":"Where you will work"},{"location":"DataSanJose2019/CI/01-Introduction/","text":"High Throughput Computing and Condor Introduction Preliminaries You will receive login credentials at the beginning of this session. You might want to refer to the online Condor 8.6.13 manual . You may enjoy browsing the Condor web page . Which Condor? We will be using Condor 8.6.13, which is a recent production version of Condor. Condor has two coexisting types of releases at any given time: stable and development. Condor 8.2.X and 7.8.x are considered stable releases, and you can know they are stable because the second digits (a 2 or a 8 in these cases) are even numbers. In a given stable series, all versions have the same features (for example 7.8.0 and 7.8.1 have the same set of features) and differ only in bug fixes. Where you will work Today you will log into training.osgconnect.net for all of your exercises. Login on submission node using the directions from yesterdays security session. They can be found here . When you login to the machine you will be in your \"home directory\". We recommend that you work in this directory as nobody else can modify the files here. You can always return to your home directory by running the command $ cd ~ The Exercises Throughout the Condor exercises, you will be given a fair amount of guidance. In several spots, there are suggestions for extra exercises to do \"on your own\" or as \"challenges\". Since you aren't being graded, there is no extra credit for doing them, but we encourage you to try them out. If you prefer, you can come back to the extra credit after you've completed the basic exercises. If you simply cruise through the exercises, you'll probably have free time--we encourage you to delve in more deeply. For all of the exercises, we'll assume that you are logged into user-training.osgconnect.net. You should have received your name and password for user-training.osgconnect.net at the beginning of the Computation Infrastructures lecture.","title":"High Throughput Computing and Condor Introduction"},{"location":"DataSanJose2019/CI/01-Introduction/#high-throughput-computing-and-condor-introduction","text":"","title":"High Throughput Computing and Condor Introduction"},{"location":"DataSanJose2019/CI/01-Introduction/#preliminaries","text":"You will receive login credentials at the beginning of this session. You might want to refer to the online Condor 8.6.13 manual . You may enjoy browsing the Condor web page .","title":"Preliminaries"},{"location":"DataSanJose2019/CI/01-Introduction/#which-condor","text":"We will be using Condor 8.6.13, which is a recent production version of Condor. Condor has two coexisting types of releases at any given time: stable and development. Condor 8.2.X and 7.8.x are considered stable releases, and you can know they are stable because the second digits (a 2 or a 8 in these cases) are even numbers. In a given stable series, all versions have the same features (for example 7.8.0 and 7.8.1 have the same set of features) and differ only in bug fixes.","title":"Which Condor?"},{"location":"DataSanJose2019/CI/01-Introduction/#where-you-will-work","text":"Today you will log into training.osgconnect.net for all of your exercises. Login on submission node using the directions from yesterdays security session. They can be found here . When you login to the machine you will be in your \"home directory\". We recommend that you work in this directory as nobody else can modify the files here. You can always return to your home directory by running the command $ cd ~","title":"Where you will work"},{"location":"DataSanJose2019/CI/01-Introduction/#the-exercises","text":"Throughout the Condor exercises, you will be given a fair amount of guidance. In several spots, there are suggestions for extra exercises to do \"on your own\" or as \"challenges\". Since you aren't being graded, there is no extra credit for doing them, but we encourage you to try them out. If you prefer, you can come back to the extra credit after you've completed the basic exercises. If you simply cruise through the exercises, you'll probably have free time--we encourage you to delve in more deeply. For all of the exercises, we'll assume that you are logged into user-training.osgconnect.net. You should have received your name and password for user-training.osgconnect.net at the beginning of the Computation Infrastructures lecture.","title":"The Exercises"},{"location":"DataSanJose2019/CI/02-OurJobManager/","text":"Our Condor Installation Objective This exercise should help you understand the basics of how Condor is installed, what Condor processes (a.k.a. daemons) are running, and what they do. Login to the Condor submit computer Before you start, make sure you are logged into user-training.osgconnect.net $ hostname training.osgconnect.net You should have been given your name and password when you arrived this afternoon. If you don't know them, talk to Rob. Looking at our Condor installation How do you know what version of Condor you are using? Try condor_version : $ condor_version $CondorVersion: 8.6.13 Jan 16 2019 $ $CondorPlatform: X86_64-CentOS_7.6 $ Note that the \"CondorPlatform\" reports the type of computer we built it on, not the computer we're running on. It was built on CentOS_6.8, but you might notice that we're running on Scientific Linux 6.8, which is a free clone of Red Hat Enterprise Linux. Extra Tip: The OS version Do you know how to find the OS version? You can usually look in /etc/os-release to find out: $ cat /etc/os-release Or you can run: $ hostnamectl Where is Condor installed? # Show the location of the condor_q binary $ which condor_q /usr/bin/condor_q # Show which RPM installed Condor $ rpm -q condor condor-8.6.11-1.osg34.el6.x86_64 Condor has some configuration files that it needs to find. They are in the standard location, /etc/condor $ ls /etc/condor condor_config condor_ssh_to_job_sshd_config_template ganglia.d condor_config.local config.d Condor has some directories that it keeps records of jobs in. Remember that each submission computer keeps track of all jobs submitted to it. That's in the local directory: $ condor_config_val -v LOCAL_DIR LOCAL_DIR = /var # at: /etc/condor/condor_config, line 26 # raw: LOCAL_DIR = /var $ ls -CF /var/lib/condor execute/ spool/ spool.q1/ spool.q2/ spool.q3/ spool.q4/ spool.q5/ The spool directory is where Condor keeps the jobs you submit, while the execute directory is where Condor keeps running jobs. Since this is a submission-only computer, it should be empty. Check if Condor is running. Your output will differ slightly, but you should see condor_master with the other Condor daemons listed under it: $ ps auwx --forest | grep condor_ | grep -v grep condor 2299245 0.0 0.1 50972 7348 ? Ss Jul10 0:08 condor_master -pidfile /var/run/condor/condor_master.pid root 2299287 0.0 0.1 25924 5072 ? S Jul10 1:54 \\_ condor_procd -A /var/run/condor/procd_pipe -L /var/log/condor/ProcLog -R 1000000 -S 60 -C 499 condor 2299288 0.0 0.1 50596 7796 ? Ss Jul10 0:16 \\_ condor_shared_port -f condor 2299289 0.0 0.2 70020 9100 ? Ss Jul10 0:13 \\_ condor_collector -f condor 2299290 0.0 0.5 116132 23872 ? Ss Jul10 6:19 \\_ condor_schedd -f condor 2299291 0.0 0.1 51056 7956 ? Ss Jul10 0:59 \\_ condor_negotiator -f For this version of Condor there are four processes running: the condor_master, the condor_schedd, the condor_procd, and condor_schedd. In general, you might see many different Condor processes. Here's a list of the processes: condor_master : This program runs constantly and ensures that all other parts of Condor are running. If they hang or crash, it restarts them. condor_schedd : If this program is running, it allows jobs to be submitted from this computer--that is, your computer is a \"submit machine\". This will advertise jobs to the central manager so that it knows about them. It will contact a condor_startd on other execute machines for each job that needs to be started. condor_procd: This process helps Condor track process (from jobs) that it creates condor_collector: This program is part of the Condor central manager. It collects information about all computers in the pool as well as which users want to run jobs. It is what normally responds to the condor_status command. At the school, it is running on a different computer, and you can figure out which one: Other daemons include: condor_negotiator: This program is part of the Condor central manager. It decides what jobs should be run where. It is run on the same computer as the collector. condor_startd: If this program is running, it allows jobs to be started up on this computer--that is, your computer is an \"execute machine\". This advertises your computer to the central manager so that it knows about this computer. It will start up the jobs that run. condor_shadow: For each job that has been submitted from this computer, there is one condor_shadow running. It will watch over the job as it runs remotely. In some cases it will provide some assistance (see the standard universe later.) You may or may not see any condor_shadow processes running, depending on what is happening on the computer when you try it out. condor_shared_port: Used to assist Condor with networking by allowing multiple Condor processes to share a single network port. condor_q You can find out what jobs have been submitted on your computer with the condor_q command: $ condor_q -- Schedd: user-training.osgconnect.net : <128.135.158.195:9618?... @ 08/12/18 16:10:58 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE HOLD TOTAL JOB_IDS 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended The output that you see will be different depending on what jobs are running. Notice what we can see from this: ID : We can see each jobs cluster and process number. For the first job, the cluster is 60256 and the process is 0. OWNER : We can see who owns the job. SUBMITTED : We can see when the job was submitted RUN_TIME : We can see how long the job has been running. ST : We can see what the current state of the job is. I is idle, R is running. PRI : We can see the priority of the job. SIZE : We can see the memory consumption of the job. CMD : We can see the program that is being executed. Extra Tip What else can you find out with condor_q? Try any one of: man condor_q condor_q -help condor_q from the online manual Double bonus points How do you use the -constraint or -format options to condor_q ? When would you want them? When would you use the -l option? This might be an easier exercise to try once you submit some jobs. condor_status You can find out what computers are in your Condor pool. (A pool is similar to a cluster, but it doesn't have the connotation that all computers are dedicated full-time to computation: some may be desktop computers owned by users.) To look, use condor_status: $ condor_status -pool flock.opensciencegrid.org Name OpSys Arch State Activity LoadAv Mem ActvtyTime slot1@amundsen.grid.uchicago.edu LINUX X86_64 Owner Idle 0.000 32768 1+02:46:31 slot2@amundsen.grid.uchicago.edu LINUX X86_64 Owner Idle 0.000 32768 5+01:05:58 slot1@c2 LINUX X86_64 Unclaimed Idle 0.000 48289 3+10:04:49 slot1@dhcp-10-1-202-3 LINUX X86_64 Unclaimed Idle 0.000 3251 0+08:10:13 slot1_1@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 6144 0+01:09:46 slot1_2@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 6144 0+00:46:46 slot1_3@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 2048 0+00:53:08 slot1_4@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 1024 0+05:48:14 slot1_5@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.000 6144 0+00:16:48 slot1_6@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 2816 0+13:16:34 ... Let's look at exactly what you can see: Name : The name of the computer. Sometimes this gets chopped off, like above. OpSys : The operating system, though not at the granularity you may wish: It says \"Linux\" instead of which distribution and version of Linux. Arch : The architecture, such as INTEL or PPC. State : The state is often Claimed (when it is running a Condor job) or Unclaimed (when it is not running a Condor job). It can be in a few other states as well, such as Matched. Activity : This is usually something like Busy or Idle. Sometimes you may see a computer that is Claimed, but no job has yet begun on the computer. Then it is Claimed/Idle. Hopefully this doesn't last very long. LoadAv : The load average on the computer. Mem : The computers memory in megabytes. ActvtyTime : How long the computer has been doing what it's been doing. Extra credit What else can you find out with condor_status? Try any one of: man condor_status condor_status -help condor_status from the online manual Note in particular the options like -master and -schedd . When would these be useful? When would the -l option be useful?","title":"Our Condor Installation"},{"location":"DataSanJose2019/CI/02-OurJobManager/#our-condor-installation","text":"","title":"Our Condor Installation"},{"location":"DataSanJose2019/CI/02-OurJobManager/#objective","text":"This exercise should help you understand the basics of how Condor is installed, what Condor processes (a.k.a. daemons) are running, and what they do.","title":"Objective"},{"location":"DataSanJose2019/CI/02-OurJobManager/#login-to-the-condor-submit-computer","text":"Before you start, make sure you are logged into user-training.osgconnect.net $ hostname training.osgconnect.net You should have been given your name and password when you arrived this afternoon. If you don't know them, talk to Rob.","title":"Login to the Condor submit computer"},{"location":"DataSanJose2019/CI/02-OurJobManager/#looking-at-our-condor-installation","text":"How do you know what version of Condor you are using? Try condor_version : $ condor_version $CondorVersion: 8.6.13 Jan 16 2019 $ $CondorPlatform: X86_64-CentOS_7.6 $ Note that the \"CondorPlatform\" reports the type of computer we built it on, not the computer we're running on. It was built on CentOS_6.8, but you might notice that we're running on Scientific Linux 6.8, which is a free clone of Red Hat Enterprise Linux.","title":"Looking at our Condor installation"},{"location":"DataSanJose2019/CI/02-OurJobManager/#extra-tip-the-os-version","text":"Do you know how to find the OS version? You can usually look in /etc/os-release to find out: $ cat /etc/os-release Or you can run: $ hostnamectl Where is Condor installed? # Show the location of the condor_q binary $ which condor_q /usr/bin/condor_q # Show which RPM installed Condor $ rpm -q condor condor-8.6.11-1.osg34.el6.x86_64 Condor has some configuration files that it needs to find. They are in the standard location, /etc/condor $ ls /etc/condor condor_config condor_ssh_to_job_sshd_config_template ganglia.d condor_config.local config.d Condor has some directories that it keeps records of jobs in. Remember that each submission computer keeps track of all jobs submitted to it. That's in the local directory: $ condor_config_val -v LOCAL_DIR LOCAL_DIR = /var # at: /etc/condor/condor_config, line 26 # raw: LOCAL_DIR = /var $ ls -CF /var/lib/condor execute/ spool/ spool.q1/ spool.q2/ spool.q3/ spool.q4/ spool.q5/ The spool directory is where Condor keeps the jobs you submit, while the execute directory is where Condor keeps running jobs. Since this is a submission-only computer, it should be empty. Check if Condor is running. Your output will differ slightly, but you should see condor_master with the other Condor daemons listed under it: $ ps auwx --forest | grep condor_ | grep -v grep condor 2299245 0.0 0.1 50972 7348 ? Ss Jul10 0:08 condor_master -pidfile /var/run/condor/condor_master.pid root 2299287 0.0 0.1 25924 5072 ? S Jul10 1:54 \\_ condor_procd -A /var/run/condor/procd_pipe -L /var/log/condor/ProcLog -R 1000000 -S 60 -C 499 condor 2299288 0.0 0.1 50596 7796 ? Ss Jul10 0:16 \\_ condor_shared_port -f condor 2299289 0.0 0.2 70020 9100 ? Ss Jul10 0:13 \\_ condor_collector -f condor 2299290 0.0 0.5 116132 23872 ? Ss Jul10 6:19 \\_ condor_schedd -f condor 2299291 0.0 0.1 51056 7956 ? Ss Jul10 0:59 \\_ condor_negotiator -f For this version of Condor there are four processes running: the condor_master, the condor_schedd, the condor_procd, and condor_schedd. In general, you might see many different Condor processes. Here's a list of the processes: condor_master : This program runs constantly and ensures that all other parts of Condor are running. If they hang or crash, it restarts them. condor_schedd : If this program is running, it allows jobs to be submitted from this computer--that is, your computer is a \"submit machine\". This will advertise jobs to the central manager so that it knows about them. It will contact a condor_startd on other execute machines for each job that needs to be started. condor_procd: This process helps Condor track process (from jobs) that it creates condor_collector: This program is part of the Condor central manager. It collects information about all computers in the pool as well as which users want to run jobs. It is what normally responds to the condor_status command. At the school, it is running on a different computer, and you can figure out which one: Other daemons include: condor_negotiator: This program is part of the Condor central manager. It decides what jobs should be run where. It is run on the same computer as the collector. condor_startd: If this program is running, it allows jobs to be started up on this computer--that is, your computer is an \"execute machine\". This advertises your computer to the central manager so that it knows about this computer. It will start up the jobs that run. condor_shadow: For each job that has been submitted from this computer, there is one condor_shadow running. It will watch over the job as it runs remotely. In some cases it will provide some assistance (see the standard universe later.) You may or may not see any condor_shadow processes running, depending on what is happening on the computer when you try it out. condor_shared_port: Used to assist Condor with networking by allowing multiple Condor processes to share a single network port.","title":"Extra Tip: The OS version"},{"location":"DataSanJose2019/CI/02-OurJobManager/#condor_q","text":"You can find out what jobs have been submitted on your computer with the condor_q command: $ condor_q -- Schedd: user-training.osgconnect.net : <128.135.158.195:9618?... @ 08/12/18 16:10:58 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE HOLD TOTAL JOB_IDS 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended The output that you see will be different depending on what jobs are running. Notice what we can see from this: ID : We can see each jobs cluster and process number. For the first job, the cluster is 60256 and the process is 0. OWNER : We can see who owns the job. SUBMITTED : We can see when the job was submitted RUN_TIME : We can see how long the job has been running. ST : We can see what the current state of the job is. I is idle, R is running. PRI : We can see the priority of the job. SIZE : We can see the memory consumption of the job. CMD : We can see the program that is being executed.","title":"condor_q"},{"location":"DataSanJose2019/CI/02-OurJobManager/#extra-tip","text":"What else can you find out with condor_q? Try any one of: man condor_q condor_q -help condor_q from the online manual","title":"Extra Tip"},{"location":"DataSanJose2019/CI/02-OurJobManager/#double-bonus-points","text":"How do you use the -constraint or -format options to condor_q ? When would you want them? When would you use the -l option? This might be an easier exercise to try once you submit some jobs.","title":"Double bonus points"},{"location":"DataSanJose2019/CI/02-OurJobManager/#condor_status","text":"You can find out what computers are in your Condor pool. (A pool is similar to a cluster, but it doesn't have the connotation that all computers are dedicated full-time to computation: some may be desktop computers owned by users.) To look, use condor_status: $ condor_status -pool flock.opensciencegrid.org Name OpSys Arch State Activity LoadAv Mem ActvtyTime slot1@amundsen.grid.uchicago.edu LINUX X86_64 Owner Idle 0.000 32768 1+02:46:31 slot2@amundsen.grid.uchicago.edu LINUX X86_64 Owner Idle 0.000 32768 5+01:05:58 slot1@c2 LINUX X86_64 Unclaimed Idle 0.000 48289 3+10:04:49 slot1@dhcp-10-1-202-3 LINUX X86_64 Unclaimed Idle 0.000 3251 0+08:10:13 slot1_1@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 6144 0+01:09:46 slot1_2@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 6144 0+00:46:46 slot1_3@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 2048 0+00:53:08 slot1_4@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 1024 0+05:48:14 slot1_5@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.000 6144 0+00:16:48 slot1_6@dhcp-10-1-202-3 LINUX X86_64 Claimed Busy 0.990 2816 0+13:16:34 ... Let's look at exactly what you can see: Name : The name of the computer. Sometimes this gets chopped off, like above. OpSys : The operating system, though not at the granularity you may wish: It says \"Linux\" instead of which distribution and version of Linux. Arch : The architecture, such as INTEL or PPC. State : The state is often Claimed (when it is running a Condor job) or Unclaimed (when it is not running a Condor job). It can be in a few other states as well, such as Matched. Activity : This is usually something like Busy or Idle. Sometimes you may see a computer that is Claimed, but no job has yet begun on the computer. Then it is Claimed/Idle. Hopefully this doesn't last very long. LoadAv : The load average on the computer. Mem : The computers memory in megabytes. ActvtyTime : How long the computer has been doing what it's been doing.","title":"condor_status"},{"location":"DataSanJose2019/CI/02-OurJobManager/#extra-credit","text":"What else can you find out with condor_status? Try any one of: man condor_status condor_status -help condor_status from the online manual Note in particular the options like -master and -schedd . When would these be useful? When would the -l option be useful?","title":"Extra credit"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/","text":"Submitting your first Condor job Objective The objective of this exercise to have you run and understand your first Condor job, as well as run small sets of jobs in a parameter sweep. This is an important exercise because it is the basis for everything that follows. If there is anything you don't understand in this exercise, please ask before you continue on. Because this is an important foundation, please seriously consider doing the \u201cOn Your Own\u201d section. First you need a job Before you can submit a job to Condor, you need a job. We will quickly write a small program in C. If you aren't an expert C programmer, fear not. We will hold your hand throughout this process. Create a file called simple.c using your favorite editor. Put it anywhere you like in your home directory. In that file, put the following text. Copy and paste is a good choice: $ mkdir -p ~/condor-test $ cd ~/condor-test Use your preferred text editor to create this C program. (Shown below with nano.) $ nano simple.c Paste in the following C code. #include <stdio.h> int main(int argc, char **argv) { int sleep_time; int input; int failure; if (argc != 3) { printf(\"Usage: simple &lt;sleep-time&gt; &lt;integer&gt;\\n\"); failure = 1; } else { sleep_time = atoi(argv[1]); input = atoi(argv[2]); printf(\"Thinking really hard for %d seconds...\\n\", sleep_time); sleep(sleep_time); printf(\"We calculated: %d\\n\", input * 2); failure = 0; } return failure; } Now compile that program: $ gcc -o simple simple.c $ ls -lh simple -rwxrwxr-x 1 roy roy 595K Jun 20 11:12 simple Finally, run the program and tell it to sleep for four seconds and calculate 10 * 2: $ ./simple 4 10 Thinking really hard for 4 seconds... We calculated: 20 Great! You just had a job run locally on the machine you are logged into (user-training.osgconnect.net). The next step is to run this job on a remote computer - and this is a job you can tell Condor to run! Although it clearly isn't an interesting job, it models some of the aspects of a real scientific program: it takes a while to run and it does a calculation. Think back to the lecture. I said that our first step was to have a job to run. Now we'll work on running it in Condor, and eventually running lots of copies of it. Submitting your job Now that you have a job, you just have to tell Condor to run it. Put the following text into a file called submit : Universe = vanilla Executable = simple Arguments = 4 10 +ProjectName = \"ConnectTrain\" Log = simple.log Output = simple.out Error = simple.error requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ON_EXIT Queue Let's examine each of these lines: Universe: The vanilla universe means a plain old job. Later on, we'll encounter some special universes. Executable: The name of your program Arguments: These are the arguments you want. They will be the same arguments we typed above. Log: This is the name of a file where Condor will record information about your job's execution. While it's not required, it is a really good idea to have a log. If something goes wrong you can refer to this log to help figure out the problem. Output: Where Condor should put the standard output from your job. Error: Where Condor should put the standard error from your job. Our job isn't likely to have any, but we'll put it there to be safe. should_transfer_files: Tell Condor that it should transfer files, instead of relying on a shared filesystem. While your home directories (on the glite-tutor computers) are mounted on NFS, you do not have user accounts on the worker nodes, so your jobs cannot access files on NFS. In addition, NFS isn't available between the local UI computers and the remote worker nodes. Therefore we will have Condor transfer files to the remote computer. when_to_transfer_output: A technical detail about when files should be transported back to the computer from which you submitted your job. Don't worry about the details for now. If you're really curious, you can read all the details in the Condor manual . Next, tell Condor to run your job: $ condor_submit submit Submitting job(s). 1 job(s) submitted to cluster 16. Now, watch your job run (insert your username in the command below instead of USER . If you forgot your username use the whoami command. Note that most of your output will be different than the example, the important column to watch is the ST column - the job state): # Note the job state of 'I' means the job is idle - not yet running $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:08 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 2056.0 osguser99 7/19 03:40 0+00:00:00 I 0 0.0 simple 4 10 Total for query: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended Total for all users: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended # After some time your job will enter the 'R' state which means it is currently running $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:14 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 2056.0 osguser99 7/19 03:40 0+00:00:02 R 0 0.0 simple 4 10 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended # When your job disappears from the queue that means it completed. $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:21 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD Total for query: 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Total for all users: 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Tip : While you are waiting for your job to run and complete you can check out \"A few tips and tricks\" to learn how to user condor_q more effectively. When my job was done, it was no longer listed. Because I told Condor to log information about my job, I can see what happened: $ cat simple.log 000 (032.000.000) 08/18 15:18:13 Job submitted from host: <10.0.0.252:9645> ... 001 (032.000.000) 08/18 15:18:32 Job executing on host: <172.16.200.1:9250> ... 006 (032.000.000) 08/18 15:18:32 Image size of job updated: 7 0 - MemoryUsage of job (MB) 0 - ResidentSetSize of job (KB) ... 005 (032.000.000) 08/18 15:18:33 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 56 - Run Bytes Sent By Job 7059 - Run Bytes Received By Job 56 - Total Bytes Sent By Job 7059 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 15 7 17605109 Memory (MB) : 0 1 1900 That looks good: the job started up quickly, though you will often see slightly slower startups. Condor doesn't optimize for fast job startup, but for high throughput, The job ran for four seconds. Now take a look at the job's output: $ cat simple.out Thinking really hard for 4 seconds... We calculated: 20 Excellent! We ran our sophisticated scientific job on a Condor pool! We've only run one job though. Can we run more? Doing a parameter sweep If you only ever had to run a single job, you probably wouldn't need Condor. But we would like to have our program calculate a whole set of values for different inputs. How can we do that? Let's change our submit file to look like this: Universe = vanilla Executable = simple +ProjectName = \"ConnectTrain\" Arguments = 4 10 Log = simple.$(Process).log Output = simple.$(Process).out Error = simple.$(Process).error requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ON_EXIT Queue Arguments = 4 11 Queue Arguments = 4 12 Queue There are two important differences to notice here. First, the Log, Output and Error lines have the $(Process) macro in them. This means that the output and error files will be named according to the process number of the job. You'll see what this looks like in a moment. Second, we told Condor to run the same job an extra two times by adding extra Arguments and Queue statements. We are doing a parameter sweep on the values 10, 11, and 12. Let's see what happens: $ condor_submit submit Submitting job(s)... 3 job(s) submitted to cluster 18. $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 34.0 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 0 34 34.1 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 1 34 34.2 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 2 34 3 jobs; 0 completed, 0 removed, 4 idle, 0 running, 0 held, 0 suspended $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 34.0 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 0 34 34.1 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 1 34 34.2 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 2 34 3 jobs; 0 completed, 0 removed, 0 idle, 4 running, 0 held, 0 suspended $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended $ ls simple*out simple.0.out simple.1.out simple.2.out simple.out $ cat simple.0.out Thinking really hard for 4 seconds... We calculated: 20 $ cat simple.1.out Thinking really hard for 4 seconds... We calculated: 22 $ cat simple.2.out Thinking really hard for 4 seconds... We calculated: 24 Notice that we had three jobs with the same cluster number, but different process numbers. They have the same cluster number because they were all submitted from the same submit file. When the jobs ran, they created three different output files, each with the desired output. You are now ready to submit lots of jobs! Although this example was simple, Condor has many, many options so you can get a wide variety of behaviors. You can find many of these if you look at the documentation for condor_submit . On your own Now that you've gotten your feet wet, try a few things on your own. Just one log file There's no reason to have a separate log file for each job. Change your submit file so that it uses a single log file. Does it all still work? New outputs for each run You might have noticed that the output files were over-written when you re-ran the jobs. (That is, simple.1.out was just re-written.) That was okay for a simple exercise, but it might be very bad if you had wanted to keep around the results. Maybe you changed a parameter or rebuilt your program, and you want to compare the outputs. Just like you used $(Process) , you can also use $(Cluster) . This will be a number from your job ID. For example, it would be 34 from the above example. Change your submit file to use $(Cluster) and $(Process) . If you do two job submissions, will you have separate output files? Lots of jobs Instead of specifying the Arguments multiple times with multiple queue statements, try this: Arguments = $(Process) $(Cluster) queue 10 What does it mean? What happens? Does it work as you expect? (An aside: you might wish to be able to do math, something like $(Process)+1 . Unfortunately, you can't do that.) Challenges If you have time and feel comfortable with the technical background, try these extra challenges. You'll need to peruse the Condor manual (particularly the manual page for condor_submit ) to find answers. Feel free to ask Rob--he'd love to give you hints! Make another scientific program (probably just modify simple.c) that takes its input from a file. Now submit 3 copies of this program where each input file is in a separate directory. Use the initialdir option described in the manual . This will let you specify a directory for the input to the program. You can run specify the initialdir with $(Process) . You can specify extra files to copy with transfer_input_files . Now you're really learning the basics of running something like a real scientific job! Condor can send you email when a job finishes. How can you control this? You know that your job should never run for more than four hours. If it does, then the job should be killed because there is a problem. How can you tell Condor to do this for you?","title":"Submitting your first Condor job"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#submitting-your-first-condor-job","text":"","title":"Submitting your first Condor job"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#objective","text":"The objective of this exercise to have you run and understand your first Condor job, as well as run small sets of jobs in a parameter sweep. This is an important exercise because it is the basis for everything that follows. If there is anything you don't understand in this exercise, please ask before you continue on. Because this is an important foundation, please seriously consider doing the \u201cOn Your Own\u201d section.","title":"Objective"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#first-you-need-a-job","text":"Before you can submit a job to Condor, you need a job. We will quickly write a small program in C. If you aren't an expert C programmer, fear not. We will hold your hand throughout this process. Create a file called simple.c using your favorite editor. Put it anywhere you like in your home directory. In that file, put the following text. Copy and paste is a good choice: $ mkdir -p ~/condor-test $ cd ~/condor-test Use your preferred text editor to create this C program. (Shown below with nano.) $ nano simple.c Paste in the following C code. #include <stdio.h> int main(int argc, char **argv) { int sleep_time; int input; int failure; if (argc != 3) { printf(\"Usage: simple &lt;sleep-time&gt; &lt;integer&gt;\\n\"); failure = 1; } else { sleep_time = atoi(argv[1]); input = atoi(argv[2]); printf(\"Thinking really hard for %d seconds...\\n\", sleep_time); sleep(sleep_time); printf(\"We calculated: %d\\n\", input * 2); failure = 0; } return failure; } Now compile that program: $ gcc -o simple simple.c $ ls -lh simple -rwxrwxr-x 1 roy roy 595K Jun 20 11:12 simple Finally, run the program and tell it to sleep for four seconds and calculate 10 * 2: $ ./simple 4 10 Thinking really hard for 4 seconds... We calculated: 20 Great! You just had a job run locally on the machine you are logged into (user-training.osgconnect.net). The next step is to run this job on a remote computer - and this is a job you can tell Condor to run! Although it clearly isn't an interesting job, it models some of the aspects of a real scientific program: it takes a while to run and it does a calculation. Think back to the lecture. I said that our first step was to have a job to run. Now we'll work on running it in Condor, and eventually running lots of copies of it.","title":"First you need a job"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#submitting-your-job","text":"Now that you have a job, you just have to tell Condor to run it. Put the following text into a file called submit : Universe = vanilla Executable = simple Arguments = 4 10 +ProjectName = \"ConnectTrain\" Log = simple.log Output = simple.out Error = simple.error requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ON_EXIT Queue Let's examine each of these lines: Universe: The vanilla universe means a plain old job. Later on, we'll encounter some special universes. Executable: The name of your program Arguments: These are the arguments you want. They will be the same arguments we typed above. Log: This is the name of a file where Condor will record information about your job's execution. While it's not required, it is a really good idea to have a log. If something goes wrong you can refer to this log to help figure out the problem. Output: Where Condor should put the standard output from your job. Error: Where Condor should put the standard error from your job. Our job isn't likely to have any, but we'll put it there to be safe. should_transfer_files: Tell Condor that it should transfer files, instead of relying on a shared filesystem. While your home directories (on the glite-tutor computers) are mounted on NFS, you do not have user accounts on the worker nodes, so your jobs cannot access files on NFS. In addition, NFS isn't available between the local UI computers and the remote worker nodes. Therefore we will have Condor transfer files to the remote computer. when_to_transfer_output: A technical detail about when files should be transported back to the computer from which you submitted your job. Don't worry about the details for now. If you're really curious, you can read all the details in the Condor manual . Next, tell Condor to run your job: $ condor_submit submit Submitting job(s). 1 job(s) submitted to cluster 16. Now, watch your job run (insert your username in the command below instead of USER . If you forgot your username use the whoami command. Note that most of your output will be different than the example, the important column to watch is the ST column - the job state): # Note the job state of 'I' means the job is idle - not yet running $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:08 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 2056.0 osguser99 7/19 03:40 0+00:00:00 I 0 0.0 simple 4 10 Total for query: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended Total for all users: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended # After some time your job will enter the 'R' state which means it is currently running $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:14 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 2056.0 osguser99 7/19 03:40 0+00:00:02 R 0 0.0 simple 4 10 Total for query: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended Total for all users: 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended # When your job disappears from the queue that means it completed. $ condor_q YOUR_USER_ID -nobatch -- Schedd: user-training.osgconnect.net : <192.170.227.119:9618?... @ 07/19/17 03:41:21 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD Total for query: 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Total for all users: 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Tip : While you are waiting for your job to run and complete you can check out \"A few tips and tricks\" to learn how to user condor_q more effectively. When my job was done, it was no longer listed. Because I told Condor to log information about my job, I can see what happened: $ cat simple.log 000 (032.000.000) 08/18 15:18:13 Job submitted from host: <10.0.0.252:9645> ... 001 (032.000.000) 08/18 15:18:32 Job executing on host: <172.16.200.1:9250> ... 006 (032.000.000) 08/18 15:18:32 Image size of job updated: 7 0 - MemoryUsage of job (MB) 0 - ResidentSetSize of job (KB) ... 005 (032.000.000) 08/18 15:18:33 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 56 - Run Bytes Sent By Job 7059 - Run Bytes Received By Job 56 - Total Bytes Sent By Job 7059 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 15 7 17605109 Memory (MB) : 0 1 1900 That looks good: the job started up quickly, though you will often see slightly slower startups. Condor doesn't optimize for fast job startup, but for high throughput, The job ran for four seconds. Now take a look at the job's output: $ cat simple.out Thinking really hard for 4 seconds... We calculated: 20 Excellent! We ran our sophisticated scientific job on a Condor pool! We've only run one job though. Can we run more?","title":"Submitting your job"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#doing-a-parameter-sweep","text":"If you only ever had to run a single job, you probably wouldn't need Condor. But we would like to have our program calculate a whole set of values for different inputs. How can we do that? Let's change our submit file to look like this: Universe = vanilla Executable = simple +ProjectName = \"ConnectTrain\" Arguments = 4 10 Log = simple.$(Process).log Output = simple.$(Process).out Error = simple.$(Process).error requirements = (HAS_MODULES =?= true) && (OSGVO_OS_STRING == \"RHEL 6\") && (OpSys == \"LINUX\") should_transfer_files = YES when_to_transfer_output = ON_EXIT Queue Arguments = 4 11 Queue Arguments = 4 12 Queue There are two important differences to notice here. First, the Log, Output and Error lines have the $(Process) macro in them. This means that the output and error files will be named according to the process number of the job. You'll see what this looks like in a moment. Second, we told Condor to run the same job an extra two times by adding extra Arguments and Queue statements. We are doing a parameter sweep on the values 10, 11, and 12. Let's see what happens: $ condor_submit submit Submitting job(s)... 3 job(s) submitted to cluster 18. $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 34.0 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 0 34 34.1 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 1 34 34.2 kagross 8/18 15:28 0+00:00:00 I 0 0.0 simple 2 34 3 jobs; 0 completed, 0 removed, 4 idle, 0 running, 0 held, 0 suspended $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 34.0 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 0 34 34.1 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 1 34 34.2 kagross 8/18 15:28 0+00:00:00 R 0 0.0 simple 2 34 3 jobs; 0 completed, 0 removed, 0 idle, 4 running, 0 held, 0 suspended $ condor_q USER -nobatch -- Submitter: frontal.cci.ucad.sn : <10.0.0.252:9645> : frontal.cci.ucad.sn ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended $ ls simple*out simple.0.out simple.1.out simple.2.out simple.out $ cat simple.0.out Thinking really hard for 4 seconds... We calculated: 20 $ cat simple.1.out Thinking really hard for 4 seconds... We calculated: 22 $ cat simple.2.out Thinking really hard for 4 seconds... We calculated: 24 Notice that we had three jobs with the same cluster number, but different process numbers. They have the same cluster number because they were all submitted from the same submit file. When the jobs ran, they created three different output files, each with the desired output. You are now ready to submit lots of jobs! Although this example was simple, Condor has many, many options so you can get a wide variety of behaviors. You can find many of these if you look at the documentation for condor_submit .","title":"Doing a parameter sweep"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#on-your-own","text":"Now that you've gotten your feet wet, try a few things on your own.","title":"On your own"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#just-one-log-file","text":"There's no reason to have a separate log file for each job. Change your submit file so that it uses a single log file. Does it all still work?","title":"Just one log file"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#new-outputs-for-each-run","text":"You might have noticed that the output files were over-written when you re-ran the jobs. (That is, simple.1.out was just re-written.) That was okay for a simple exercise, but it might be very bad if you had wanted to keep around the results. Maybe you changed a parameter or rebuilt your program, and you want to compare the outputs. Just like you used $(Process) , you can also use $(Cluster) . This will be a number from your job ID. For example, it would be 34 from the above example. Change your submit file to use $(Cluster) and $(Process) . If you do two job submissions, will you have separate output files?","title":"New outputs for each run"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#lots-of-jobs","text":"Instead of specifying the Arguments multiple times with multiple queue statements, try this: Arguments = $(Process) $(Cluster) queue 10 What does it mean? What happens? Does it work as you expect? (An aside: you might wish to be able to do math, something like $(Process)+1 . Unfortunately, you can't do that.)","title":"Lots of jobs"},{"location":"DataSanJose2019/CI/03-FirstManagedJob/#challenges","text":"If you have time and feel comfortable with the technical background, try these extra challenges. You'll need to peruse the Condor manual (particularly the manual page for condor_submit ) to find answers. Feel free to ask Rob--he'd love to give you hints! Make another scientific program (probably just modify simple.c) that takes its input from a file. Now submit 3 copies of this program where each input file is in a separate directory. Use the initialdir option described in the manual . This will let you specify a directory for the input to the program. You can run specify the initialdir with $(Process) . You can specify extra files to copy with transfer_input_files . Now you're really learning the basics of running something like a real scientific job! Condor can send you email when a job finishes. How can you control this? You know that your job should never run for more than four hours. If it does, then the job should be killed because there is a problem. How can you tell Condor to do this for you?","title":"Challenges"},{"location":"DataSanJose2019/CI/04-TipsandTricks/","text":"A few tips and tricks Objective This exercise will teach you a few nifty commands to help you use Condor more easily. Tips for condor_q condor_q can show you your job ClassAd. Recall back to the lecture and the discussion of ClassAds. For instance, you can look at the ClassAd for a single job: $ condor_q -l YOUR_JOB_CLUSTER_NUMBER MaxHosts = 1 User = \"kagross@frontal.cci.ucad.sn\" OnExitHold = false CoreSize = 0 MachineAttrCpus0 = 1 WantRemoteSyscalls = false MyType = \"Job\" Rank = 0.0 CumulativeSuspensionTime = 0 MinHosts = 1 PeriodicHold = false PeriodicRemove = false Err = \"simple.49.error\" ProcId = 49 EnteredCurrentStatus = 1408374244 UserLog = \"/home/kagross/condor-test/s ... output trimmed ... There are some interesting parts you can check out. How many CPUs is the job requesting. (This can be more than one, but for the exercises we will do today it will be 1) $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep RequestCpus RequestCpus = 1 Where is the user log for this job? This is helpful when you assist someone else in debugging and they're not sure. $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep UserLog UserLog = \"/home/kagross/condor-test/simple.47.log\" What are the job's requirements? Condor automatically fills some in for you to make sure your job runs on a reasonable computer in our cluster, but you can override any of these. I've broken the output into multiple lines to explain it to you. $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep Requirements Requirements =( TARGET.Arch == \"X86_64\" ) <em># Run on a 64-bit computer && ( TARGET.OpSys == \"LINUX\" ) <em># Make sure you run on Linux && ( TARGET.Disk >= RequestDisk ) <em># Make sure the default disk Condor is on has enough disk space. && ( TARGET.Memory >= RequestMemory ) <em># Make sure the computer has enough memory && ( TARGET.HasFileTransfer ) <em># Only run on a computer that can accept your files. What else can you find that's interesting in the ClassAd? Removing jobs If you submit a job that you realize has a problem, you can remove it with condor_rm . For example: $ condor_q -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 29.0 roy 6/21 15:23 0+00:00:00 I 0 0.7 simple 60 10 1 jobs; 0 completed, 0 removed, 2 idle, 0 running, 0 held, 0 suspended $ condor_rm YOUR_JOB_CLUSTER_NUMBER Job 29.0 marked for removal $ condor_q -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended A few tips: You can remove all of your jobs with the -all option. You can't remove other users jobs. There are fancy options to condor_rm . Historical information You can see information about jobs that completed and are no longer in the queue with the condor_history command. It's rare that you want to see all the jobs, so try looking at jobs for just you: $ condor_history USER For example: $ condor_history kagross 9.9 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 9 9 9.8 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 8 9 9.11 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 11 9 9.7 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 7 9 9.5 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 5 9 9.6 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 6 9 9.3 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 3 9 9.2 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 2 9 9.1 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 1 9 9.0 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 9.4 kagross 7/31 12:44 0+00:00:01 C 7/31 12:44 /home/kagross/simple 4 9 8.0 kagross 7/31 12:42 0+00:00:07 C 7/31 12:42 /home/kagross/simple 4 10 ...","title":"A few tips and tricks"},{"location":"DataSanJose2019/CI/04-TipsandTricks/#a-few-tips-and-tricks","text":"","title":"A few tips and tricks"},{"location":"DataSanJose2019/CI/04-TipsandTricks/#objective","text":"This exercise will teach you a few nifty commands to help you use Condor more easily.","title":"Objective"},{"location":"DataSanJose2019/CI/04-TipsandTricks/#tips-for-condor_q","text":"condor_q can show you your job ClassAd. Recall back to the lecture and the discussion of ClassAds. For instance, you can look at the ClassAd for a single job: $ condor_q -l YOUR_JOB_CLUSTER_NUMBER MaxHosts = 1 User = \"kagross@frontal.cci.ucad.sn\" OnExitHold = false CoreSize = 0 MachineAttrCpus0 = 1 WantRemoteSyscalls = false MyType = \"Job\" Rank = 0.0 CumulativeSuspensionTime = 0 MinHosts = 1 PeriodicHold = false PeriodicRemove = false Err = \"simple.49.error\" ProcId = 49 EnteredCurrentStatus = 1408374244 UserLog = \"/home/kagross/condor-test/s ... output trimmed ... There are some interesting parts you can check out. How many CPUs is the job requesting. (This can be more than one, but for the exercises we will do today it will be 1) $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep RequestCpus RequestCpus = 1 Where is the user log for this job? This is helpful when you assist someone else in debugging and they're not sure. $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep UserLog UserLog = \"/home/kagross/condor-test/simple.47.log\" What are the job's requirements? Condor automatically fills some in for you to make sure your job runs on a reasonable computer in our cluster, but you can override any of these. I've broken the output into multiple lines to explain it to you. $ condor_q -l YOUR_JOB_CLUSTER_NUMBER | grep Requirements Requirements =( TARGET.Arch == \"X86_64\" ) <em># Run on a 64-bit computer && ( TARGET.OpSys == \"LINUX\" ) <em># Make sure you run on Linux && ( TARGET.Disk >= RequestDisk ) <em># Make sure the default disk Condor is on has enough disk space. && ( TARGET.Memory >= RequestMemory ) <em># Make sure the computer has enough memory && ( TARGET.HasFileTransfer ) <em># Only run on a computer that can accept your files. What else can you find that's interesting in the ClassAd?","title":"Tips for condor_q"},{"location":"DataSanJose2019/CI/04-TipsandTricks/#removing-jobs","text":"If you submit a job that you realize has a problem, you can remove it with condor_rm . For example: $ condor_q -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 29.0 roy 6/21 15:23 0+00:00:00 I 0 0.7 simple 60 10 1 jobs; 0 completed, 0 removed, 2 idle, 0 running, 0 held, 0 suspended $ condor_rm YOUR_JOB_CLUSTER_NUMBER Job 29.0 marked for removal $ condor_q -- Submitter: osg-ss-submit.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : osg-ss-submit.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended A few tips: You can remove all of your jobs with the -all option. You can't remove other users jobs. There are fancy options to condor_rm .","title":"Removing jobs"},{"location":"DataSanJose2019/CI/04-TipsandTricks/#historical-information","text":"You can see information about jobs that completed and are no longer in the queue with the condor_history command. It's rare that you want to see all the jobs, so try looking at jobs for just you: $ condor_history USER For example: $ condor_history kagross 9.9 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 9 9 9.8 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 8 9 9.11 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 11 9 9.7 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 7 9 9.5 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 5 9 9.6 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 6 9 9.3 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 3 9 9.2 kagross 7/31 12:44 0+00:00:02 C 7/31 12:44 /home/kagross/simple 2 9 9.1 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 1 9 9.0 kagross 7/31 12:44 0+00:00:03 C 7/31 12:44 /home/kagross/simple 9.4 kagross 7/31 12:44 0+00:00:01 C 7/31 12:44 /home/kagross/simple 4 9 8.0 kagross 7/31 12:42 0+00:00:07 C 7/31 12:42 /home/kagross/simple 4 10 ...","title":"Historical information"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/","text":"Visualisation using Seaborn The notes listed here are based on this DataCamp tutorial on Seaborn by Karlijn Willems and this CODATA-RDA module on visualisation by Sara El Jadid . During this module we'll be making use of Seaborn , which provides a high-level interface to draw statistical graphics. Seaborn vs Matplotlib Seaborn is complimentary to Matplotlib and it specifically targets statistical data visualization. But it goes even further than that: Seaborn extends Matplotlib and that\u2019s why it can address the two biggest frustrations of working with Matplotlib. Or, as Michael Waskom says in the \u201c introduction to Seaborn \u201d: \u201cIf matplotlib \u201ctries to make easy things easy and hard things possible\u201d, seaborn tries to make a well-defined set of hard things easy too.\u201d One of these hard things or frustrations had to do with the default Matplotlib parameters. Seaborn works with different parameters, which undoubtedly speaks to those users that don\u2019t use the default looks of the Matplotlib plots. During this module we'll also be making some use of Pandas to extract features of the data that we need. Getting started In the first instance please get yourself set up with a notebook on the Google colab site. Please go to https://colab.research.google.com/notebooks/welcome.ipynb and then click on File and New Python 3 notebook. OR log into the Kabre jupyter server and then click on New and then New Python 3. We'll start with importing a set of libraries that will be useful for us and the gapminder data set. import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set(style=\"darkgrid\") The last line is a choice about how things look - you may want to leave that out. Now we'll read in the data. We will again use the gapminder data set but with the columns labelled differently. Please do not use the version you have used previously. The version is stored on a github repository which has been shortened using bit.ly. url=\"http://bit.ly/2PbVBcR\" #url=\"https://raw.githubusercontent.com/CODATA-RDA-DataScienceSchools/Materials/master/docs/DataSanJose2019/slides/Visualisation/gapminder.csv\" gapminder=pd.read_csv(url) So we are using pandas to read in the data set. The gapminder data set is a Data Frame . Exploring the gapminder data set The gapminder data set is a set of socioeconomic data about populations, GDP per capita and expected life span for a large number of countries over a number of years. We can have a look at it using the head command. gapminder.head() You should get something like this. Unnamed: 0 country continent year lifeExp pop gdpPercap 0 1 Afghanistan Asia 1952 28.801 8425333 779.445314 1 2 Afghanistan Asia 1957 30.332 9240934 820.853030 2 3 Afghanistan Asia 1962 31.997 10267083 853.100710 3 4 Afghanistan Asia 1967 34.020 11537966 836.197138 4 5 Afghanistan Asia 1972 36.088 13079460 739.981106 So it is a combination of categorical data (countries and continents) and quantitative data (year, lifeExp etc.). It's also nice (though unrealistic) that it doesn't have missing values or malformed data e.g. Ireland is written sometimes as \"Ireland\" and sometimes \"ireland\" and sometimes \"Republic of Ireland\" or even \"Eire\"!! Dealing with those kinds of issues is not what we're going to focus on here. We can do a statistical summary of the numerical data using the describe function gapminder.describe() Unnamed: 0 year lifeExp pop gdpPercap count 1704.000000 1704.00000 1704.000000 1.704000e+03 1704.000000 mean 852.500000 1979.50000 59.474439 2.960121e+07 7215.327081 std 492.046746 17.26533 12.917107 1.061579e+08 9857.454543 min 1.000000 1952.00000 23.599000 6.001100e+04 241.165877 25% 426.750000 1965.75000 48.198000 2.793664e+06 1202.060309 50% 852.500000 1979.50000 60.712500 7.023596e+06 3531.846989 75% 1278.250000 1993.25000 70.845500 1.958522e+07 9325.462346 max 1704.000000 2007.00000 82.603000 1.318683e+09 113523.132900 One can find the names of the continents by executing the following. list(set(gapminder.continent)) We note that gapminder.continent gives us the list with the column corresponding to the continent entry. The set command converts the list into a set (which only has unique entries) and then list turns that back into a list again. We can list these entries alphabetically from the command sorted(list(set(gapminder.continent))) Exercise What does the function sorted do? Do the same for the countries. The gsapminder data set also presents lots of questions such as Is there a relationship between gdpPercap (roughly a measure of the average wealth of each person in a country) and their average life span? Is the average life span changing over time? How does picture change over different countries or comntinents? Visualisation allows us to explore all of this! Getting started with seaborn Let us start with doing box plots which count the number of entries that we have for each continent. sns.countplot(x=\"continent\", data=gapminder) You should get the folllowing. Note that generically seaborn generally looks like this. sns. (x= , y= , ... , data =< a data frame>) We use countplot here to just count entries and plot them as a box plot. Exercise What happens if we do the following? sns.countplot(x=\"Continent\", data=gapminder) What does that tell us? Printing out You can save a figure as a PNG or as a PDF then in the same cell as the command you run to plot use the savefig command. sns.countplot(x=\"continent\", data=gapminder) plt.savefig(\"Histogram.png\") plt.savefig(\"Histogram.pdf\") Looking at 1-d distributions We can use the command catplot to just look at the distribution of life expenctancies. sns.catplot(y=\"lifeExp\", data=gapminder) You should get something like this. The points are jittered i.e. randomly moved in the horizontal axis to make things clearer. We can switch that off if we wish. sns.catplot(y=\"lifeExp\", data=gapminder,jitter=False) This scatterplot is not very informative! We can create a box plot of the data sns.boxplot(y=\"lifeExp\", data=gapminder) This should give the following. Exercise One can use another type of plot called a violin plot which tries to summarise the distribution better than a boxplot. The width of the violin plot represents how big the distribution is at that value. It is quite useful for picking out multi-modal (one with a distribution that has more than one peak). The command in seaborn is violinplot. Try and implement this for this data. Diving deeper into the data Just looking at the life expectancy for all of the countries isn't very informative. The first thing we can do is ask how does this vary across continents. Seaborn does this easily by introducing an x-axis which is the continent. Again, let's try with just the points. sns.catplot(x=\"continent\", y=\"lifeExp\", data=gapminder) Exercise Repeat this using box plots (and violin plots if you wish). Repeat the above steps using GDP per capita (gdpPercap) instead of life expectancy. You can also try the swarmplot function as another way to represent this data. Can we just draw a distribution or a histogram? What about dividing it as continents? Yes! But we'll get to that in a bit. Ordering Plotting the box plots with the continents in alphabetical order is quite easy. orderedContinents = sorted(list(set(gapminder.continent))) orderedContinents orderedContinents is a list with the continents ordered. sns.boxplot(x=\"continent\", y=\"lifeExp\", order=orderedContinents, data=gapminder) On the other hand we may want to order the box plots in ascending order of the medians of the life expectancy. This is more involved but is a good exercise in manipulating the data. #Create an empty dictionary medianLifeExps = {} # Loop through all the continents for val in gapminder.continent: # Create a new key which is the median life expectancy of that continent # gapminder[gapminder.continent == val] pulls out the continent in that loop # the .lifeExp.median() part then computes the median # of the remaining life expectancy data key = gapminder[gapminder.continent==val].lifeExp.median() # create a new entry in the dictionary with the continent as the value and the key # as the median. medianLifeExps[key] = val # Create a sorted list of the medians (in ascending order) sortedKeys = sorted(medianLifeExps) # Finally return the list of continents in that order orderedMedianContinents = [] for m in orderedMedians: orderedMedianContinents.append(medianLifeExps[m]) sns.boxplot(x=\"continent\", y=\"lifeExp\", order=orderedMedianContinents, data=gapminder) Exercise Do the same plot for GDP per capita. Line and scatter plots Instead of having a categorical variable on the horizontal access we now do scatter and line plots. Let's start with the whole data set. sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data=gapminder) This is hard to grasp as a whole, so we'll just consider one country - China. We can select data corresponding to China by the following. gapminder[gapminder.country==\"China\"] sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) Exercise Do the same for your country - if it isn't listed in gapminder then pick another. A line plot works in the same way. It's possible to overlay these in the same cell. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) Expressing more variables with different attributes It is hard to make out the points from the lines. We can change the colour (color) of the points accordingly. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",color=\"red\",data= gapminder[gapminder.country==\"China\"]) We can use the colour of the points to represent a different column - e.g. the year. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\", data= gapminder[gapminder.country==\"China\"]) The problem here is that only a certain number of years have been picked out. We need to tell seaborn how many years there are and how to set a palette of colours for this (the default palette has six colours). # Find the number of years (why only set?) n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) # We use a rainbow-like palette but there are others. sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"]) We can use the size of the point to represent an additional column - the population. n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",size=\"pop\", palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"]) The problem now is that there is too much detail in the legend - so we'll switch that off. n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",size=\"pop\", palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"], legend=False) How useful is adding these attributes? Exercise Pick another country and try this out. Costa Rica is interesting. Does anybody have an explanation? Summarising We will now examine how life expenctancy has varied over time. sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"country\",data= gapminder,legend=False) There are so many countries here I haven't even tried to construct a palette! Looking at this many countries are generally increasing but some are not following that trend. We could explore those outliers but here we will focus on trying to summarise what is going on (is a particular continent not going with the trend of increasing life span over time?) To do this we need to use another pandas command groupby which creates a new data frame for particular variables. Once we have created that new data frame we can then plot the data. # First create a new data frame which has the medians, by year and continent, of the life expectancy medianGapminder = gapminder.groupby(['continent','year']).lifeExp.median() # Now plot it sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"continent\",data= medianGapminder) What happened? The groupby command makes continent and year indices of the data (you can see this if you print out the data frame). Having created the data frame we need to reset the indices. # First create a new data frame which has the medians, by year and continent, of the life expectancy medianGapminder = gapminder.groupby(['continent','year']).lifeExp.median() # Now plot it sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"continent\",data= medianGapminder.reset_index()) Exercise Repeat this using the GDP per capita. Use a different statistical summary apart from the median. Regression The line plots just \"join the dots\". It is more intesting to try and fit the data to a curve. We also want to do the fit and distinguish between the different continents. We can do this using the lmplot command. sns.lmplot(x=\"year\",y=\"lifeExp\",hue=\"continent\", data= medianGapminder.reset_index()) This does a simple linear regression. We can do more sophisticated types of fit, for example Loess (or Lowess). sns.lmplot(x=\"year\",y=\"lifeExp\",hue=\"continent\", lowess=True, data= medianGapminder.reset_index()) Exercise Repeat the above using the GDP per capita. More regression Now that we know how to fit curves through data with a number of different variables we can go back to the case of where we plotted the life expectancy against the GDP per capita. Instead of just doing a scatter plot we can now do regression (curve fitting) as function of the continent as well so we can see how the life expectancy varies between GDP per capita and the continent. sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True,data=gapminder) The problem here is that points are too large - we cannot see the trend. When you have many points make the point size smaller, indeed way smaller (someone described it as 'dust size') to see the trend better. Since Seaborn is based on Matplotlib we need to use a slightly different notation in the arguments to what was used previously. # scatter_kws is passed onto the underlying matplotlib plotting routine. sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True,data=gapminder,scatter_kws={\"s\":5}) The GDP per capita varies over a wide range and it would be good in the first instance to plot the x-axis on a logarithmic scale. Again since Seaborn is based on Matplotlib we need to use a slightly different notation to what was used previously. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xscale=\"log\") We now get a much better spread of the data but it's still hard to see how the different continents are behaving. To do that we make use of facet plots . These are simply plots of different but related variables that are organised on the same screen for easy comparison. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\") We note that the axes of all of these plots are the same so we can do a valid comparison. Still these plots are quite squashed as thy try and fit to the width of the page. Instead we can wrap them around. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", col_wrap=2, lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\") Finally, we can adjust the colour of the individual plots. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", hue=\"continent\", col_wrap=2, lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\") Exercise Do the same type of plots for life expectancy against year. Distributions We can plot the distribution of a list of data (one column from a data frame) using a kernal density approach and/or a histogram. sns.distplot(gapminder.lifeExp) distplot only allows a single column from a data frame! You can also add the raw data into this plot as well (although this isn't very useful in this case as there's so much data). sns.distplot(gapminder.lifeExp,rug=True) Exercise Do the same type of plot with the GDP per capita data. Again we would like to break this down in separate continents. Again we will make use of facet plots. lmplot is designed to create facet plots but distplot isn't so we need to use a specific function called FacetGrid to do this. In the call below we also adjust the height and aspect (the height/width ratio) of the figures. # Create a facet of the gapminder data based on the continents ordered alphabetically (orderedContinents) g = sns.FacetGrid(gapminder, row=\"continent\", row_order=orderedContinents, height=2, aspect=4) # Plot on the facets the distribution of the life expctancy data, but don't plot the histogram. g.map(sns.distplot, \"lifeExp\", hist=False) Finally the facet plot can also be used with just one facet! # Create a facet plot with one facet but colour on continent. g = sns.FacetGrid(gapminder, hue=\"continent\",height=2, aspect=4) # Plot the distributions with no histogram g.map(sns.distplot, \"lifeExp\", hist=False) # Give the colour scheme in a legend g.add_legend() Exercise Do the same type of plot with the GDP per capita data (Longer) Select the data from the gapminder data set for a particular continent and now create facet plots for those countries.","title":"Visualisation using Seaborn"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#visualisation-using-seaborn","text":"The notes listed here are based on this DataCamp tutorial on Seaborn by Karlijn Willems and this CODATA-RDA module on visualisation by Sara El Jadid . During this module we'll be making use of Seaborn , which provides a high-level interface to draw statistical graphics.","title":"Visualisation using Seaborn"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#seaborn-vs-matplotlib","text":"Seaborn is complimentary to Matplotlib and it specifically targets statistical data visualization. But it goes even further than that: Seaborn extends Matplotlib and that\u2019s why it can address the two biggest frustrations of working with Matplotlib. Or, as Michael Waskom says in the \u201c introduction to Seaborn \u201d: \u201cIf matplotlib \u201ctries to make easy things easy and hard things possible\u201d, seaborn tries to make a well-defined set of hard things easy too.\u201d One of these hard things or frustrations had to do with the default Matplotlib parameters. Seaborn works with different parameters, which undoubtedly speaks to those users that don\u2019t use the default looks of the Matplotlib plots. During this module we'll also be making some use of Pandas to extract features of the data that we need.","title":"Seaborn vs Matplotlib"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#getting-started","text":"In the first instance please get yourself set up with a notebook on the Google colab site. Please go to https://colab.research.google.com/notebooks/welcome.ipynb and then click on File and New Python 3 notebook.","title":"Getting started"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#or","text":"log into the Kabre jupyter server and then click on New and then New Python 3. We'll start with importing a set of libraries that will be useful for us and the gapminder data set. import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set(style=\"darkgrid\") The last line is a choice about how things look - you may want to leave that out. Now we'll read in the data. We will again use the gapminder data set but with the columns labelled differently. Please do not use the version you have used previously. The version is stored on a github repository which has been shortened using bit.ly. url=\"http://bit.ly/2PbVBcR\" #url=\"https://raw.githubusercontent.com/CODATA-RDA-DataScienceSchools/Materials/master/docs/DataSanJose2019/slides/Visualisation/gapminder.csv\" gapminder=pd.read_csv(url) So we are using pandas to read in the data set. The gapminder data set is a Data Frame .","title":"OR"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exploring-the-gapminder-data-set","text":"The gapminder data set is a set of socioeconomic data about populations, GDP per capita and expected life span for a large number of countries over a number of years. We can have a look at it using the head command. gapminder.head() You should get something like this. Unnamed: 0 country continent year lifeExp pop gdpPercap 0 1 Afghanistan Asia 1952 28.801 8425333 779.445314 1 2 Afghanistan Asia 1957 30.332 9240934 820.853030 2 3 Afghanistan Asia 1962 31.997 10267083 853.100710 3 4 Afghanistan Asia 1967 34.020 11537966 836.197138 4 5 Afghanistan Asia 1972 36.088 13079460 739.981106 So it is a combination of categorical data (countries and continents) and quantitative data (year, lifeExp etc.). It's also nice (though unrealistic) that it doesn't have missing values or malformed data e.g. Ireland is written sometimes as \"Ireland\" and sometimes \"ireland\" and sometimes \"Republic of Ireland\" or even \"Eire\"!! Dealing with those kinds of issues is not what we're going to focus on here. We can do a statistical summary of the numerical data using the describe function gapminder.describe() Unnamed: 0 year lifeExp pop gdpPercap count 1704.000000 1704.00000 1704.000000 1.704000e+03 1704.000000 mean 852.500000 1979.50000 59.474439 2.960121e+07 7215.327081 std 492.046746 17.26533 12.917107 1.061579e+08 9857.454543 min 1.000000 1952.00000 23.599000 6.001100e+04 241.165877 25% 426.750000 1965.75000 48.198000 2.793664e+06 1202.060309 50% 852.500000 1979.50000 60.712500 7.023596e+06 3531.846989 75% 1278.250000 1993.25000 70.845500 1.958522e+07 9325.462346 max 1704.000000 2007.00000 82.603000 1.318683e+09 113523.132900 One can find the names of the continents by executing the following. list(set(gapminder.continent)) We note that gapminder.continent gives us the list with the column corresponding to the continent entry. The set command converts the list into a set (which only has unique entries) and then list turns that back into a list again. We can list these entries alphabetically from the command sorted(list(set(gapminder.continent)))","title":"Exploring the gapminder data set"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise","text":"What does the function sorted do? Do the same for the countries. The gsapminder data set also presents lots of questions such as Is there a relationship between gdpPercap (roughly a measure of the average wealth of each person in a country) and their average life span? Is the average life span changing over time? How does picture change over different countries or comntinents? Visualisation allows us to explore all of this!","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#getting-started-with-seaborn","text":"Let us start with doing box plots which count the number of entries that we have for each continent. sns.countplot(x=\"continent\", data=gapminder) You should get the folllowing. Note that generically seaborn generally looks like this. sns. (x= , y= , ... , data =< a data frame>) We use countplot here to just count entries and plot them as a box plot.","title":"Getting started with seaborn"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_1","text":"What happens if we do the following? sns.countplot(x=\"Continent\", data=gapminder) What does that tell us?","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#printing-out","text":"You can save a figure as a PNG or as a PDF then in the same cell as the command you run to plot use the savefig command. sns.countplot(x=\"continent\", data=gapminder) plt.savefig(\"Histogram.png\") plt.savefig(\"Histogram.pdf\")","title":"Printing out"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#looking-at-1-d-distributions","text":"We can use the command catplot to just look at the distribution of life expenctancies. sns.catplot(y=\"lifeExp\", data=gapminder) You should get something like this. The points are jittered i.e. randomly moved in the horizontal axis to make things clearer. We can switch that off if we wish. sns.catplot(y=\"lifeExp\", data=gapminder,jitter=False) This scatterplot is not very informative! We can create a box plot of the data sns.boxplot(y=\"lifeExp\", data=gapminder) This should give the following.","title":"Looking at 1-d distributions"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_2","text":"One can use another type of plot called a violin plot which tries to summarise the distribution better than a boxplot. The width of the violin plot represents how big the distribution is at that value. It is quite useful for picking out multi-modal (one with a distribution that has more than one peak). The command in seaborn is violinplot. Try and implement this for this data.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#diving-deeper-into-the-data","text":"Just looking at the life expectancy for all of the countries isn't very informative. The first thing we can do is ask how does this vary across continents. Seaborn does this easily by introducing an x-axis which is the continent. Again, let's try with just the points. sns.catplot(x=\"continent\", y=\"lifeExp\", data=gapminder)","title":"Diving deeper into the data"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_3","text":"Repeat this using box plots (and violin plots if you wish). Repeat the above steps using GDP per capita (gdpPercap) instead of life expectancy. You can also try the swarmplot function as another way to represent this data. Can we just draw a distribution or a histogram? What about dividing it as continents? Yes! But we'll get to that in a bit.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#ordering","text":"Plotting the box plots with the continents in alphabetical order is quite easy. orderedContinents = sorted(list(set(gapminder.continent))) orderedContinents orderedContinents is a list with the continents ordered. sns.boxplot(x=\"continent\", y=\"lifeExp\", order=orderedContinents, data=gapminder) On the other hand we may want to order the box plots in ascending order of the medians of the life expectancy. This is more involved but is a good exercise in manipulating the data. #Create an empty dictionary medianLifeExps = {} # Loop through all the continents for val in gapminder.continent: # Create a new key which is the median life expectancy of that continent # gapminder[gapminder.continent == val] pulls out the continent in that loop # the .lifeExp.median() part then computes the median # of the remaining life expectancy data key = gapminder[gapminder.continent==val].lifeExp.median() # create a new entry in the dictionary with the continent as the value and the key # as the median. medianLifeExps[key] = val # Create a sorted list of the medians (in ascending order) sortedKeys = sorted(medianLifeExps) # Finally return the list of continents in that order orderedMedianContinents = [] for m in orderedMedians: orderedMedianContinents.append(medianLifeExps[m]) sns.boxplot(x=\"continent\", y=\"lifeExp\", order=orderedMedianContinents, data=gapminder)","title":"Ordering"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_4","text":"Do the same plot for GDP per capita.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#line-and-scatter-plots","text":"Instead of having a categorical variable on the horizontal access we now do scatter and line plots. Let's start with the whole data set. sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data=gapminder) This is hard to grasp as a whole, so we'll just consider one country - China. We can select data corresponding to China by the following. gapminder[gapminder.country==\"China\"] sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"])","title":"Line and scatter plots"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_5","text":"Do the same for your country - if it isn't listed in gapminder then pick another. A line plot works in the same way. It's possible to overlay these in the same cell. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"])","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#expressing-more-variables-with-different-attributes","text":"It is hard to make out the points from the lines. We can change the colour (color) of the points accordingly. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",color=\"red\",data= gapminder[gapminder.country==\"China\"]) We can use the colour of the points to represent a different column - e.g. the year. sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\", data= gapminder[gapminder.country==\"China\"]) The problem here is that only a certain number of years have been picked out. We need to tell seaborn how many years there are and how to set a palette of colours for this (the default palette has six colours). # Find the number of years (why only set?) n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) # We use a rainbow-like palette but there are others. sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"]) We can use the size of the point to represent an additional column - the population. n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",size=\"pop\", palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"]) The problem now is that there is too much detail in the legend - so we'll switch that off. n = len(set(gapminder.year)) sns.lineplot(x=\"gdpPercap\",y=\"lifeExp\",data= gapminder[gapminder.country==\"China\"]) sns.scatterplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"year\",size=\"pop\", palette=sns.color_palette(\"rainbow_r\",n), data= gapminder[gapminder.country==\"China\"], legend=False) How useful is adding these attributes?","title":"Expressing more variables with different attributes"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_6","text":"Pick another country and try this out. Costa Rica is interesting. Does anybody have an explanation?","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#summarising","text":"We will now examine how life expenctancy has varied over time. sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"country\",data= gapminder,legend=False) There are so many countries here I haven't even tried to construct a palette! Looking at this many countries are generally increasing but some are not following that trend. We could explore those outliers but here we will focus on trying to summarise what is going on (is a particular continent not going with the trend of increasing life span over time?) To do this we need to use another pandas command groupby which creates a new data frame for particular variables. Once we have created that new data frame we can then plot the data. # First create a new data frame which has the medians, by year and continent, of the life expectancy medianGapminder = gapminder.groupby(['continent','year']).lifeExp.median() # Now plot it sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"continent\",data= medianGapminder) What happened? The groupby command makes continent and year indices of the data (you can see this if you print out the data frame). Having created the data frame we need to reset the indices. # First create a new data frame which has the medians, by year and continent, of the life expectancy medianGapminder = gapminder.groupby(['continent','year']).lifeExp.median() # Now plot it sns.lineplot(x=\"year\",y=\"lifeExp\",hue=\"continent\",data= medianGapminder.reset_index())","title":"Summarising"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_7","text":"Repeat this using the GDP per capita. Use a different statistical summary apart from the median.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#regression","text":"The line plots just \"join the dots\". It is more intesting to try and fit the data to a curve. We also want to do the fit and distinguish between the different continents. We can do this using the lmplot command. sns.lmplot(x=\"year\",y=\"lifeExp\",hue=\"continent\", data= medianGapminder.reset_index()) This does a simple linear regression. We can do more sophisticated types of fit, for example Loess (or Lowess). sns.lmplot(x=\"year\",y=\"lifeExp\",hue=\"continent\", lowess=True, data= medianGapminder.reset_index())","title":"Regression"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_8","text":"Repeat the above using the GDP per capita.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#more-regression","text":"Now that we know how to fit curves through data with a number of different variables we can go back to the case of where we plotted the life expectancy against the GDP per capita. Instead of just doing a scatter plot we can now do regression (curve fitting) as function of the continent as well so we can see how the life expectancy varies between GDP per capita and the continent. sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True,data=gapminder) The problem here is that points are too large - we cannot see the trend. When you have many points make the point size smaller, indeed way smaller (someone described it as 'dust size') to see the trend better. Since Seaborn is based on Matplotlib we need to use a slightly different notation in the arguments to what was used previously. # scatter_kws is passed onto the underlying matplotlib plotting routine. sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True,data=gapminder,scatter_kws={\"s\":5}) The GDP per capita varies over a wide range and it would be good in the first instance to plot the x-axis on a logarithmic scale. Again since Seaborn is based on Matplotlib we need to use a slightly different notation to what was used previously. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",hue=\"continent\", lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xscale=\"log\") We now get a much better spread of the data but it's still hard to see how the different continents are behaving. To do that we make use of facet plots . These are simply plots of different but related variables that are organised on the same screen for easy comparison. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\") We note that the axes of all of these plots are the same so we can do a valid comparison. Still these plots are quite squashed as thy try and fit to the width of the page. Instead we can wrap them around. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", col_wrap=2, lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\") Finally, we can adjust the colour of the individual plots. ax = sns.lmplot(x=\"gdpPercap\",y=\"lifeExp\",col=\"continent\", hue=\"continent\", col_wrap=2, lowess=True, data=gapminder,scatter_kws={\"s\":5}) ax.set(xlim=(100,200000),xscale=\"log\")","title":"More regression"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_9","text":"Do the same type of plots for life expectancy against year.","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#distributions","text":"We can plot the distribution of a list of data (one column from a data frame) using a kernal density approach and/or a histogram. sns.distplot(gapminder.lifeExp) distplot only allows a single column from a data frame! You can also add the raw data into this plot as well (although this isn't very useful in this case as there's so much data). sns.distplot(gapminder.lifeExp,rug=True)","title":"Distributions"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_10","text":"Do the same type of plot with the GDP per capita data. Again we would like to break this down in separate continents. Again we will make use of facet plots. lmplot is designed to create facet plots but distplot isn't so we need to use a specific function called FacetGrid to do this. In the call below we also adjust the height and aspect (the height/width ratio) of the figures. # Create a facet of the gapminder data based on the continents ordered alphabetically (orderedContinents) g = sns.FacetGrid(gapminder, row=\"continent\", row_order=orderedContinents, height=2, aspect=4) # Plot on the facets the distribution of the life expctancy data, but don't plot the histogram. g.map(sns.distplot, \"lifeExp\", hist=False) Finally the facet plot can also be used with just one facet! # Create a facet plot with one facet but colour on continent. g = sns.FacetGrid(gapminder, hue=\"continent\",height=2, aspect=4) # Plot the distributions with no histogram g.map(sns.distplot, \"lifeExp\", hist=False) # Give the colour scheme in a legend g.add_legend()","title":"Exercise"},{"location":"DataSanJose2019/slides/Visualisation/Visualisation using Seaborn/#exercise_11","text":"Do the same type of plot with the GDP per capita data (Longer) Select the data from the gapminder data set for a particular continent and now create facet plots for those countries.","title":"Exercise"},{"location":"DataSaoPaulo2018/","text":"Sao Paulo School of Research Data Science December 3 \u2013 14, 2018 ICTP-SAIFR, Sao Paulo, Brazil Background and purpose The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, ICTP and SAIFR, was conducted at the ICTP-SAIFR, Sao Paulo to introduce participants to the skills of RDS. Materials for the 2018 School of Research Data Science in Sao Paulo Day 1 - Introduction, Open Science , UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R , Reports Knitr Day 4 - Data Visualization Day 5 - Research Data Management , Data Management Plans Day 6 - Open Science , Information Security Day 7 - Overview of Machine Learning Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Lecture 4 Day 10 - Computational Infrastructures Wrap-Up - Lecture 5 , Lecture 6 School Close Out","title":"Sao Paulo School of Research Data Science"},{"location":"DataSaoPaulo2018/#sao-paulo-school-of-research-data-science","text":"December 3 \u2013 14, 2018 ICTP-SAIFR, Sao Paulo, Brazil","title":"Sao Paulo School of Research Data Science"},{"location":"DataSaoPaulo2018/#background-and-purpose","text":"The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA, ICTP and SAIFR, was conducted at the ICTP-SAIFR, Sao Paulo to introduce participants to the skills of RDS.","title":"Background and purpose"},{"location":"DataSaoPaulo2018/#materials-for-the-2018-school-of-research-data-science-in-sao-paulo","text":"Day 1 - Introduction, Open Science , UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R , Reports Knitr Day 4 - Data Visualization Day 5 - Research Data Management , Data Management Plans Day 6 - Open Science , Information Security Day 7 - Overview of Machine Learning Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Lecture 4 Day 10 - Computational Infrastructures Wrap-Up - Lecture 5 , Lecture 6 School Close Out","title":"Materials for the 2018 School of Research Data Science in Sao Paulo"},{"location":"DataSaoPaulo2018/word_clouds/Readme/","text":"Code and instructions to create the word clouds can be found in here","title":"Readme"},{"location":"DataTrieste2019/","text":"Trieste School of Research Data Science 5-16 August, 2019 ICTP, Trieste, Italy Background and purpose The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA and ICTP, was conducted at the ICTP, Trieste to introduce participants to the skills of RDS. Materials for the 2019 School of Research Data Science in Trieste Day 1 - Introduction Open Science UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R Day 4 - Data Visualization Day 5 - Research Data Management , Data Management Plans Day 6 - Open Science , Information Security Day 7 - Overview of Machine Learning Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Lecture 4 Day 10 - Computational Infrastructures Wrap-Up - Lecture 5 , Lecture 6 School Close Out","title":"Trieste School of Research Data Science"},{"location":"DataTrieste2019/#trieste-school-of-research-data-science","text":"5-16 August, 2019 ICTP, Trieste, Italy","title":"Trieste School of Research Data Science"},{"location":"DataTrieste2019/#background-and-purpose","text":"The goal of this workshop is to train researchers in Research Data Science (RDS). RDS refers to the principles and practice of Open Science and research data management and curation, the use of a range of data platforms and infrastructures, large scale analysis, statistics, visualization and modelling techniques, software development and data annotation. These are important tools for extracting useful information from data and these tools are useful in every research area. A 10-day workshop, organized by CODATA, RDA and ICTP, was conducted at the ICTP, Trieste to introduce participants to the skills of RDS.","title":"Background and purpose"},{"location":"DataTrieste2019/#materials-for-the-2019-school-of-research-data-science-in-trieste","text":"Day 1 - Introduction Open Science UNIX Shell Day 2 - Version Control with Git , Introduction to R Day 3 - Introduction to R Day 4 - Data Visualization Day 5 - Research Data Management , Data Management Plans Day 6 - Open Science , Information Security Day 7 - Overview of Machine Learning Day 8 - Artificial Neural Networks Day 9 - Computational Infrastructures - Lecture 1 , Lecture 2 , Lecture 3 , Lecture 4 Day 10 - Computational Infrastructures Wrap-Up - Lecture 5 , Lecture 6 School Close Out","title":"Materials for the 2019 School of Research Data Science in Trieste"}]}