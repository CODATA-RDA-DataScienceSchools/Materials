{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e3bfd18b",
      "metadata": {
        "id": "e3bfd18b"
      },
      "source": [
        "# BUILDING A CLASSIFACTION MODEL\n",
        "\n",
        "# Instructor: Ekpe Okorafor\n",
        "\n",
        "# The CODATA-RDA School for Research Data Science\n",
        "\n",
        "\n",
        "## Introduction:\n",
        "\n",
        "Building classification models is one of the most important data science use cases. Classification models are models that predict a categorical label. A few examples of this include predicting whether a customer will churn or whether a bank loan will default. In this guide, you will learn how to build and evaluate a classification model in Python. We will train the logistic regression algorithm, which is one of the oldest yet most powerful classification algorithms.\n",
        "\n",
        "## 0.\tData\n",
        "\n",
        "In this exercise, we will use a fictitious dataset of loan applicants containing about 614 observations and 12 variables, as described below:\n",
        "\n",
        "1. **Gender:** Whether the applicant is a male (\"Male\") or a female (\"Female\")\n",
        "2. **Marital_status:** Whether the applicant is married or not (\"Yes\") or not (\"No\")    \n",
        "3. **Dependent:** Total number of dependents    \n",
        "4. **Education:** Whether the applicant is a graduate (“Graduate”) or not (“Not Graduate\")    \n",
        "5. **Self_employed:** Whether the applicant is a self-employed (\"Yes\") or not (“No”)\n",
        "6. **ApplicantIncome:** Monthly Income of the applicant (in USD)\n",
        "7. **CoapplicantIncome:** Monthly Income of the coapplicant (in USD)\n",
        "8. **Loan_amount:** Loan amount (in USD) for which the application was submitted\n",
        "9. **Loan_amount_term:** Terms of the loan in months\n",
        "10. **Credit_history:** Whether the applicant has a credit history (\"1\") or not (\"0\")\n",
        "11. **Property_area:** Where property is located – rural (“Rural”), semiurban (“Semiurban”) or urban (\"Urban\")\n",
        "12. **Loan_status:** Whether the loan application was approved (\"Y\") or not (\"N\")\n",
        "\n",
        "Let's start by loading the required libraries and the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b48718d",
      "metadata": {
        "scrolled": true,
        "id": "8b48718d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "dat = pd.read_csv('data.csv')\n",
        "\n",
        "# Display a summary of the dataframe\n",
        "dat.info()\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "dat.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a686132",
      "metadata": {
        "id": "6a686132"
      },
      "source": [
        "The output shows that the dataset has four numerical (labeled as int). If the other six character variables are labeled as object, we will convert these into factor variables using the line of code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38f76f81",
      "metadata": {
        "id": "38f76f81"
      },
      "outputs": [],
      "source": [
        "# Display the initial data types\n",
        "print(\"Initial data types:\")\n",
        "print(dat.dtypes)\n",
        "\n",
        "# Convert columns from 'object' to 'int64'\n",
        "for col in dat.select_dtypes(include='object').columns:\n",
        "    dat[col] = dat[col].astype('category')\n",
        "\n",
        "\n",
        "# Display the data types after conversion\n",
        "print(\"\\nData types after conversion:\")\n",
        "print(dat.dtypes)\n",
        "\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "dat.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ca91f9a",
      "metadata": {
        "id": "9ca91f9a"
      },
      "source": [
        "Great! We have 5 numerical and the other variables are now labelled as objects.\n",
        "\n",
        "Now, let's get on with building this classification model.\n",
        "\n",
        "We will proceed as follow:\n",
        "\n",
        "Step 1: Check continuous variables\n",
        "Step 2: Check factor variables\n",
        "Step 3: Summary statistic\n",
        "Step 4: Train/test set\n",
        "Step 5: Build the model\n",
        "Step 6: Assess the performance of the model\n",
        "\n",
        "## 1.\tStep 1) Check continuous variables\n",
        "\n",
        "In the first step, you can see the distribution of the continuous variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "690e0ae7",
      "metadata": {
        "id": "690e0ae7"
      },
      "outputs": [],
      "source": [
        " # Select numeric columns\n",
        "continuous = dat.select_dtypes(include='number')\n",
        "\n",
        "\n",
        "# Display a summary of the numeric columns\n",
        "summary = continuous.describe()\n",
        "\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84eca9db",
      "metadata": {
        "id": "84eca9db"
      },
      "source": [
        "The code above selects the columns that are numeric type, and then display a summary of those columns.\n",
        "\n",
        "From the above table, you can see that the data have totally different scales. 'ApplicantIncome' & 'CoapplicantIncome' have large outliers ( i.e., look at the last quartile and maximum value).\n",
        "\n",
        "**You can deal with it following two steps:**\n",
        "\n",
        " 1: Plot the distribution of the variables with the outliers (ApplicantIncome & CoapplicantIncome)\n",
        "\n",
        " 2: Standardize the continuous variables\n",
        "\n",
        "Let's go ahead and plot the distribution.\n",
        "\n",
        "### 1. Plot the distribution\n",
        "\n",
        "Let's look closer at the distribution of 'ApplicantIncome'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f71eb77",
      "metadata": {
        "id": "5f71eb77"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting histogram with kernel density curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(continuous['ApplicantIncome'], kde=True, color='#FF6666', alpha=0.2)\n",
        "plt.title('Distribution of ApplicantIncome')\n",
        "plt.xlabel('ApplicantIncome')\n",
        "plt.ylabel('Density')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de2b5d96",
      "metadata": {
        "id": "de2b5d96"
      },
      "source": [
        "This is histogram plot with a kernel density curve for the 'ApplicantIncome' column.\n",
        "\n",
        "We can see that the variable has some outliers. You can partially tackle this problem by deleting the top 0.02 percent of the ApplicantIncome.\n",
        "\n",
        "To compute the 98th percentile of the 'ApplicantIncome' column and display it, you can use the 'numpy' library in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd645bb8",
      "metadata": {
        "id": "cd645bb8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Compute the 98th percentile of ApplicantIncome\n",
        "applicant_income = np.percentile(dat['ApplicantIncome'], 98)\n",
        "\n",
        "# Display the result\n",
        "applicant_income\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31d4c239",
      "metadata": {
        "id": "31d4c239"
      },
      "source": [
        "98 percent of the population makes under $19666.04 per month.\n",
        "\n",
        "You can drop the observations above this threshold.\n",
        "\n",
        "\n",
        "To filter the DataFrame in Python to drop observations where 'ApplicantIncome' is above the 98th percentile, you can use the 'pandas' library.\n",
        "\n",
        "**Compute the 98th Percentile of ApplicantIncome and Drop Observations Above the Threshold**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b04f955",
      "metadata": {
        "id": "2b04f955"
      },
      "outputs": [],
      "source": [
        " # Compute the 98th percentile of ApplicantIncome\n",
        "applicant_income = np.percentile(dat['ApplicantIncome'], 98)\n",
        "\n",
        "# Drop observations above this threshold\n",
        "dat_drop = dat[dat['ApplicantIncome'] < applicant_income]\n",
        "\n",
        "# Display the dimensions of the resulting DataFrame\n",
        "dat_drop.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf5abb9d",
      "metadata": {
        "id": "bf5abb9d"
      },
      "source": [
        "Compare this with the original “dat”. Observe that some rows have been dropped.\n",
        "\n",
        "### 2. Standardize the continuous variables\n",
        "\n",
        "You can standardize each column to improve the performance, especially if your data does not have the same scale.\n",
        "\n",
        "To standardize the numeric columns in a DataFrame in Python, you can use the StandardScaler from the sklearn.preprocessing module.\n",
        "\n",
        "**Standardize Numeric Columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04ce7d04",
      "metadata": {
        "id": "04ce7d04"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize numeric columns\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Selecet only the numeric columns\n",
        "numeric_columns = dat_drop.select_dtypes(include='number').columns\n",
        "\n",
        "#Standardize the numeric columns\n",
        "dat_drop[numeric_columns] = scaler.fit_transform(dat_drop[numeric_columns])\n",
        "\n",
        "# Display the first few rows of the rescaled DataFrame\n",
        "dat_rescale = dat_drop\n",
        "dat_rescale.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6ea0a80",
      "metadata": {
        "id": "c6ea0a80"
      },
      "source": [
        "## 2.\tStep 2) Check factor variables\n",
        "\n",
        "This step has two objectives:\n",
        " - Check the level in each categorical column\n",
        " - Define new levels\n",
        "\n",
        "We will divide this step into three parts:\n",
        " - Select the categorical columns\n",
        " - Store the bar chart of each column in a list\n",
        " - Print the graphs\n",
        "\n",
        "To select categorical (factor) columns from a DataFrame in Python using pandas, you can filter columns based on their data type after rescaling and standardizing the DataFrame.\n",
        "\n",
        "\n",
        "We can select the factor columns with the code below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8b842b",
      "metadata": {
        "id": "6f8b842b"
      },
      "outputs": [],
      "source": [
        "# Select categorical columns\n",
        "factor = dat_rescale.select_dtypes(include='category')\n",
        "\n",
        "# Count the number of categorical columns\n",
        "num_factor_columns = factor.shape[1]\n",
        "\n",
        "num_factor_columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e4b41c",
      "metadata": {
        "id": "35e4b41c"
      },
      "source": [
        "The dataset contains 7 categorical variables\n",
        "\n",
        "The second step is more skilled. You want to plot a bar chart for each column in the data frame factor. It is more convenient to automate the process, especially in situations where there are lots of columns.\n",
        "\n",
        "\n",
        "To create bar charts for each categorical column in a pandas DataFrame in Python, you can use matplotlib or seaborn for plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9158d461",
      "metadata": {
        "id": "9158d461"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'factor' DataFrame is already defined with categorical columns\n",
        "\n",
        "# Calculate appropriate figure height\n",
        "num_columns = len(factor.columns)\n",
        "fig_height = num_columns * 4  # Adjust the multiplier as needed to ensure sufficient space\n",
        "\n",
        "# Create graphs for each column\n",
        "plt.figure(figsize=(4, fig_height))\n",
        "\n",
        "for i, col in enumerate(factor.columns):\n",
        "    plt.subplot(num_columns, 1, i + 1)\n",
        "    factor[col].value_counts().plot(kind='bar')\n",
        "    plt.title(f'Bar Chart of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.xticks(rotation=90)\n",
        "\n",
        "# Adjust layout to avoid warning and overlapping\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plots\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c21fc68c",
      "metadata": {
        "id": "c21fc68c"
      },
      "source": [
        "This Python code automates the creation of bar charts for each categorical column in the factor DataFrame, similar to the R code provided. Adjust the DataFrame name (factor) and plot customization according to your specific dataset and visualization preferences.\n",
        "\n",
        "**Step 1:** Import necessary libraries (pandas for data manipulation and matplotlib.pyplot for plotting).\n",
        "\n",
        "**Step 2:** Assuming factor is a DataFrame containing categorical columns.\n",
        "\n",
        "**Step 3:** Use a for loop to iterate through each column (col) in factor.columns.\n",
        "\n",
        "**Step 4:** Create a subplot for each column using plt.subplot. Adjust the subplot dimensions (len(factor.columns), 1) based on the number of columns in factor.\n",
        "\n",
        "**Step 5:** Plot the bar chart for each column using value_counts().plot(kind='bar').\n",
        "\n",
        "**Step 6:** Customize plot titles, labels, and rotation of x-axis labels (plt.title, plt.xlabel, plt.ylabel, plt.xticks(rotation=90)).\n",
        "\n",
        "**Step 7:** Use plt.tight_layout() to improve subplot spacing, and plt.show() to display the plots.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64f1dc28",
      "metadata": {
        "id": "64f1dc28"
      },
      "source": [
        "## 3.\tStep 3) Summary Statistic\n",
        "\n",
        "It is time to check some statistics about our target variables. In the graph below, you count the percentage of individuals with loan approval given their gender.\n",
        "\n",
        "1. Calculate the percentage of loan approval by gender.\n",
        "\n",
        "2. Plot the results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baad4555",
      "metadata": {
        "id": "baad4555"
      },
      "outputs": [],
      "source": [
        "#import pandas as pd\n",
        "#import seaborn as sns\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'dat_rescale' DataFrame is already defined and contains 'Gender' and 'Loan_status' columns\n",
        "\n",
        "# Calculate the percentage of loan approval given gender\n",
        "loan_status_percentage = dat_rescale.pivot_table(index='Gender', columns='Loan_status', aggfunc='size', fill_value=0)\n",
        "loan_status_percentage = loan_status_percentage.div(loan_status_percentage.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Reset the index to convert pivot table to DataFrame for plotting\n",
        "loan_status_percentage = loan_status_percentage.reset_index()\n",
        "\n",
        "# Melt the DataFrame for easier plotting with seaborn\n",
        "loan_status_percentage_melted = loan_status_percentage.melt(id_vars='Gender', value_vars=loan_status_percentage.columns[1:],\n",
        "                                                            var_name='Loan_status', value_name='Percentage')\n",
        "\n",
        "# Plot the percentage of loan approval by gender\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=loan_status_percentage_melted, x='Gender', y='Percentage', hue='Loan_status')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Percentage of Loan Approval by Gender')\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Percentage')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Loan Status')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f3e9072",
      "metadata": {
        "id": "5f3e9072"
      },
      "source": [
        "Next, check if the level of education affects their loan approval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "274bae83",
      "metadata": {
        "id": "274bae83"
      },
      "outputs": [],
      "source": [
        "# Assuming 'dat_rescale' DataFrame is already defined and contains 'Education' and 'Loan_status' columns\n",
        "\n",
        "# Calculate the percentage of loan approval given education level\n",
        "loan_status_education_percentage = dat_rescale.pivot_table(index='Education', columns='Loan_status', aggfunc='size', fill_value=0)\n",
        "loan_status_education_percentage = loan_status_education_percentage.div(loan_status_education_percentage.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Reset the index to convert pivot table to DataFrame for plotting\n",
        "loan_status_education_percentage = loan_status_education_percentage.reset_index()\n",
        "\n",
        "# Melt the DataFrame for easier plotting with seaborn\n",
        "loan_status_education_percentage_melted = loan_status_education_percentage.melt(id_vars='Education', value_vars=loan_status_education_percentage.columns[1:],\n",
        "                                                            var_name='Loan_status', value_name='Percentage')\n",
        "\n",
        "# Plot the percentage of loan approval by education level\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=loan_status_education_percentage_melted, x='Education', y='Percentage', hue='Loan_status')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Percentage of Loan Approval by Education Level')\n",
        "plt.xlabel('Education Level')\n",
        "plt.ylabel('Percentage')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Loan Status')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "693d988c",
      "metadata": {
        "id": "693d988c"
      },
      "source": [
        "Next, check if the property area affects their loan approval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5ffaad8",
      "metadata": {
        "id": "a5ffaad8"
      },
      "outputs": [],
      "source": [
        "# Calculate the percentage of loan approval given property area\n",
        "loan_status_area_percentage = dat_rescale.pivot_table(index='Property_area', columns='Loan_status', aggfunc='size', fill_value=0)\n",
        "loan_status_area_percentage = loan_status_area_percentage.div(loan_status_area_percentage.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Reset the index to convert pivot table to DataFrame for plotting\n",
        "loan_status_area_percentage = loan_status_area_percentage.reset_index()\n",
        "\n",
        "# Melt the DataFrame for easier plotting with seaborn\n",
        "loan_status_area_percentage_melted = loan_status_area_percentage.melt(id_vars='Property_area', value_vars=loan_status_area_percentage.columns[1:],\n",
        "                                                            var_name='Loan_status', value_name='Percentage')\n",
        "\n",
        "# Plot the percentage of loan approval by property area\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=loan_status_area_percentage_melted, x='Property_area', y='Percentage', hue='Loan_status')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Percentage of Loan Approval by Property Area')\n",
        "plt.xlabel('Property Area')\n",
        "plt.ylabel('Percentage')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Loan Status')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38515700",
      "metadata": {
        "id": "38515700"
      },
      "source": [
        "Check if the applicant's income is related to the loan amount.\n",
        "\n",
        "Do a scatter plot to visually inspect the relationship between the applicant's income and the loan amount, and the Pearson correlation coefficient to quantify this relationship. Adjust the DataFrame and column names as needed to fit your specific data context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7300b528",
      "metadata": {
        "id": "7300b528"
      },
      "outputs": [],
      "source": [
        "#import pandas as pd\n",
        "#import seaborn as sns\n",
        "#import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Assuming 'dat_rescale' DataFrame is already defined and contains 'ApplicantIncome' and 'LoanAmount' columns\n",
        "\n",
        "# Scatter plot to visualize the relationship\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=dat_rescale, x='ApplicantIncome', y='Loan_amount')\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Relationship between Applicant Income and Loan Amount')\n",
        "plt.xlabel('Applicant Income')\n",
        "plt.ylabel('Loan Amount')\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Pearson correlation coefficient\n",
        "correlation, p_value = pearsonr(dat_rescale['ApplicantIncome'], dat_rescale['Loan_amount'])\n",
        "print(f\"Pearson correlation coefficient: {correlation}\")\n",
        "print(f\"P-value: {p_value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c60f1789",
      "metadata": {
        "id": "c60f1789"
      },
      "source": [
        "**Non-linearity**\n",
        "\n",
        "Before you run the model, you can see if the applicant income is related to loan amount.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "556a06fa",
      "metadata": {
        "scrolled": true,
        "id": "556a06fa"
      },
      "outputs": [],
      "source": [
        "#import pandas as pd\n",
        "#import seaborn as sns\n",
        "#import matplotlib.pyplot as plt\n",
        "#import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Assuming 'dat_rescale' DataFrame is already defined and contains 'ApplicantIncome', 'Loan_amount', and 'Loan_status' columns\n",
        "\n",
        "# Scatter plot with polynomial regression line\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.scatterplot(data=dat_rescale, x='ApplicantIncome', y='Loan_amount', hue='Loan_status', size=0.5)\n",
        "\n",
        "# Polynomial regression fit (degree=2)\n",
        "sns.lmplot(data=dat_rescale, x='ApplicantIncome', y='Loan_amount', hue='Loan_status', order=2, ci=True, aspect=2)\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Relationship between Applicant Income and Loan Amount with Polynomial Regression')\n",
        "plt.xlabel('Applicant Income')\n",
        "plt.ylabel('Loan Amount')\n",
        "plt.grid(True)\n",
        "plt.legend(title='Loan Status')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ad3ce00",
      "metadata": {
        "id": "7ad3ce00"
      },
      "source": [
        "This code will provide you with a scatter plot to visually inspect the relationship between the applicant's income and the loan amount, with a polynomial regression line (degree 2) to account for non-linearity, colored by loan status. Adjust the DataFrame and column names as needed to fit your specific data context.\n",
        "\n",
        "\n",
        "In a nutshell, you can test interaction terms in the model to pick up the non-linearity effect between the applicant income and other features. It is important to detect under which condition the applicant income differs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67bfda7b",
      "metadata": {
        "id": "67bfda7b"
      },
      "source": [
        "**Correlation**\n",
        "\n",
        "The next check is to visualize the correlation between the variables. You convert the factor level type to numeric so that you can plot a heat map containing the coefficient of correlation computed with the Spearman method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4818856",
      "metadata": {
        "scrolled": true,
        "id": "b4818856"
      },
      "outputs": [],
      "source": [
        "#import pandas as pd\n",
        "#import seaborn as sns\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'dat_rescale' DataFrame is already defined\n",
        "\n",
        "# Convert factor level columns to numeric\n",
        "dat_numeric = dat_rescale.copy()\n",
        "for col in dat_numeric.select_dtypes(include=['category', 'object']).columns:\n",
        "    dat_numeric[col] = dat_numeric[col].astype('category').cat.codes\n",
        "\n",
        "# Compute the correlation matrix using the Spearman method\n",
        "corr_matrix = dat_numeric.corr(method='spearman')\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1,\n",
        "            linewidths=.5, annot_kws={\"size\": 8}, cbar_kws={\"shrink\": .8})\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Spearman Correlation Heatmap')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5294d0cd",
      "metadata": {
        "id": "5294d0cd"
      },
      "source": [
        "**Class Discussion:**\n",
        "\n",
        "What observations can you make based on the results above?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2a538e2",
      "metadata": {
        "id": "e2a538e2"
      },
      "source": [
        "## 4.\tStep 4) Train/test set\n",
        "\n",
        "Any supervised machine learning task requires splitting the data between a train set and a test set.\n",
        "\n",
        "To split your data into a training set and a test set in Python, you can use the train_test_split function from sklearn.model_selection.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9722790d",
      "metadata": {
        "id": "9722790d"
      },
      "outputs": [],
      "source": [
        "#import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Assuming 'dat_rescale' DataFrame is already defined\n",
        "\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "random_seed = 1234\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "trainData, testData = train_test_split(dat_rescale, test_size=0.3, stratify=dat_rescale['Loan_status'], random_state=random_seed)\n",
        "\n",
        "\n",
        "# Print the dimensions of the train and test sets\n",
        "print(f'Train Data Dimensions: {trainData.shape}')\n",
        "print(f'Test Data Dimensions: {testData.shape}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af325f80",
      "metadata": {
        "id": "af325f80"
      },
      "source": [
        "The train dataset contains 70 percent of the data (420 observations of 12 variables) while the test data contains the remaining 30 percent (181 observations of 12 variables)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e139ef90",
      "metadata": {
        "id": "e139ef90"
      },
      "source": [
        "## 5.\tStep 5) Build the model\n",
        "\n",
        "To fit a logistic regression model in Python using statsmodels, and print the summary of the trained model, you can follow these steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5203859f",
      "metadata": {
        "id": "5203859f"
      },
      "outputs": [],
      "source": [
        "#import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "#import numpy as np\n",
        "\n",
        "\n",
        "# Assuming 'trainData' DataFrame is already defined and contains 'Loan_status' and other predictor columns\n",
        "\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "# Convert categorical variables into dummy/indicator variables\n",
        "X = pd.get_dummies(trainData.drop('Loan_status', axis=1), drop_first=True)\n",
        "y = trainData['Loan_status']\n",
        "\n",
        "\n",
        "# Standardize the numeric features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "# Step 2: Instantiate and fit the logistic regression model\n",
        "model_glm = LogisticRegression(max_iter=1000, random_state=1234)\n",
        "model_glm.fit(X_scaled, y)\n",
        "\n",
        "\n",
        "# Step 3: Print the summary (similar to R's summary function)\n",
        "# Model coefficients and intercept\n",
        "coefficients = pd.DataFrame({'Feature': X.columns, 'Coefficient': model_glm.coef_[0]})\n",
        "intercept = model_glm.intercept_[0]\n",
        "\n",
        "\n",
        "print(\"Intercept:\", intercept)\n",
        "print(\"\\nCoefficients:\")\n",
        "print(coefficients)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "965c832f",
      "metadata": {
        "id": "965c832f"
      },
      "source": [
        "## 6.\tStep 6) Assess the performance of the model\n",
        "\n",
        "Let's now evaluate the model performance on the training and data. We start by generating predictions on the training data. The algorithm will predict the Y response for the Loan_status variable.\n",
        "\n",
        "The accuracy of the model on the training data comes out to be 81 percent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d574928",
      "metadata": {
        "scrolled": true,
        "id": "2d574928"
      },
      "outputs": [],
      "source": [
        "#import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "#import numpy as np\n",
        "\n",
        "\n",
        "# Assuming 'trainData' DataFrame is already defined and contains 'Loan_status' and other predictor columns\n",
        "\n",
        "# Model evaluation\n",
        "y_pred = model_glm.predict(X_scaled)\n",
        "\n",
        "\n",
        "print(\"\\nAccuracy:\", accuracy_score(y, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred))\n",
        "\n",
        "\n",
        "# Model summary similar to R's glm summary output\n",
        "def logistic_regression_summary(model, X):\n",
        "    summary_df = pd.DataFrame({\n",
        "        'Feature': X.columns,\n",
        "        'Coefficient': model.coef_[0],\n",
        "        'Odds Ratio': np.exp(model.coef_[0])\n",
        "    })\n",
        "    return summary_df\n",
        "\n",
        "\n",
        "summary = logistic_regression_summary(model_glm, X)\n",
        "print(\"\\nLogistic Regression Summary:\\n\")\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "510b5380",
      "metadata": {
        "id": "510b5380"
      },
      "source": [
        "## 7.\tConclusion\n",
        "\n",
        "In this guide, you have learned techniques of building a classification model in Python using the powerful logistic regression algorithm."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}