# -*- coding: utf-8 -*-
"""Regression_Spark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/167ccX3XoORanyRIy6M-7zS7y1u2rLUNl

This lesson leverages with work at https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a with slight modifications and additional explanation.

#System Setup 
This first step sets up the Google Collab Cloud Environment by running some system level commands. This includes installting the Java Development Toolkit, Spark (3.1) with Hadoop (3.2), and the python module FindSpark. All necessary for the example regression analysis we will do later in the code. 

**Note on Syntax:** The "!" at the beginning of a command runs an operating system command. You'll see this later when we download the dataset also. 

**Additional Note:** This step will take about 30 seconds. It installs a significant amount of complex software.
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
!tar xf spark-3.1.1-bin-hadoop3.2.tgz
!pip install -q findspark

"""This step will set the home paths for the Java Toolkit and Spark software packages installed in the previous step. We're still in the runtime enviroment set up, this will need to be done on all cloud enviroments that provide only Operating System level setup. """

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop3.2"
print("Environment Set")

"""Here we import and bind the Spark installation with the regular Python libraries. After this Spark can be accessed and used as a regular Python library. The use of Spark at this point becomes transparent to the user and all cloud-like functionality is done in the background. """

import findspark
findspark.init()
print("Findspark Initialized")

"""Import Spark methods to bind the spark installation to the data set. Spark Context (sc) and SQL Context (sqlContext) create an RDD (Spark Data Type) and Data Frames for structured data processing."""

from pyspark import SparkConf, SparkContext
from pyspark.sql import SQLContext
sc = SparkContext()
sqlContext = SQLContext(sc)
print("PySpark libraries imported and SQL Context set")

"""# Get the data

Housing data from the Boston Area is collected from GitHub via the HTTP protocol and delivered to a local Google Colab location to be accessed. It is then read into a Data Frame and the first five rows are printed out as a sanity check. Looks like legitimate data, so we can proceed with formatting it for analysis. 
"""

!wget https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/MASS/Boston.csv
house_df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('Boston.csv')
house_df.take(5)

"""Let's take a look at the data schema to determine the data types we'll be working with. All data is in the form of integer or double, so should work well with a standard regression analysis. **Note:** the first column named "_c0" is simply a row number and will not be used in the analysis. """

house_df.printSchema()

"""Ok, let's look even closer at the data set, this will provide some base numerical analysis including the Mean, Standard Deviation, Minimum, and Maximum values for each variable. 

Here is a more detailed description of each variable:

CRIM — per capita crime rate by town.

ZN — proportion of residential land zoned for lots over 25,000 sq.ft.

INDUS — proportion of non-retail business acres per town.

CHAS — Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

NOX — nitrogen oxides concentration (parts per 10 million).

RM — average number of rooms per dwelling.

AGE — proportion of owner-occupied units built prior to 1940.

DIS — weighted mean of distances to five Boston employment centres.

RAD — index of accessibility to radial highways.

TAX — full-value property-tax rate per $10,000.

PTRATIO — pupil-teacher ratio by town.

BLACK — 1000(Bk — 0.63)² where Bk is the proportion of blacks by town.

LSTAT — lower status of the population (percent).

MV — median value of owner-occupied homes in $1000s. This is the target variable.

**Note:** This step uses the pandas Python library which is used for data manipulation and analysis. We are calling this library 'pd' for brevity. 
"""

import pandas as pd
house_df.describe().toPandas().transpose()

numeric_features = [t[0] for t in house_df.dtypes if t[1] == 'int' or t[1] == 'double']
sampled_data = house_df.select(numeric_features).sample(False,0.8).toPandas()
axs = pd.plotting.scatter_matrix(sampled_data, figsize=(10,10))

"""Look for coorelations using the Median Value target variable. Remember the correlation coefficient tells you how strong the positive (close to 1) or negative (close to -1) the relationship is. Which variable has the strongest postive correlation (increases the median value of the home)? Which variable has the strongest negative correlation (decrease the median value of the home)? Does this make real world sense based on what you know about home values? 

**Note:** We are using the six library here it provides utility functions for smoothing over the differences between the Python versions with the goal of writing Python code that is compatible on both Python versions
"""

import six
for i in house_df.columns: 
  if not ( isinstance(house_df.select(i).take(1)[0][0], six.string_types)):
    print( "Correlation to MV for ", i, house_df.stat.corr("medv", i))

"""Let's prepare the data for a machine learning analysis. For this we need only features and labels. We also look at the first 5 rows to be sure we have the data in the proper format. """

from pyspark.ml.feature import VectorAssembler
vectorAssembler = VectorAssembler(inputCols= ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'tax', 'ptratio', 'black', 'lstat'], outputCol='features')
vhouse_df = vectorAssembler.transform(house_df)
vhouse_df = vhouse_df.select(['features', 'medv'])
vhouse_df.show(5)

"""Now we'll need to split the sets into a training set and a testing set. We'll do this randomly and use 70% of the set to train and 30% for testing. """

splits = vhouse_df.randomSplit([0.7, 0.3])
train_df = splits[0]
test_df = splits[1]
print("Dataset Split")

"""Time to train the Linear Regression model and look at the Coefficients and Intercept of that model. """

from pyspark.ml.regression import LinearRegression
lr = LinearRegression(featuresCol= 'features', labelCol='medv', maxIter=10, regParam=0.3, elasticNetParam=0.8)
lr_model = lr.fit(train_df)
print("Coefficients: " + str(lr_model.coefficients))
print("Intercept: " + str(lr_model.intercept))

"""Look at the Root Mean Square Error (difference between predicted values and actual models) and R-Squared results for the model. In this case 74% (this figure may vary depending on the random split done above) of the variability can be explained using the model. """

trainingSummary = lr_model.summary
print("RMSE: %f" % trainingSummary.rootMeanSquaredError)
print("r2: %f" % trainingSummary.r2)

"""RMSE is meaningless without looking at the value of the medv variable, so let's take a look. """

train_df.describe().show()

"""Let's try some predictions using the test data set. First we'll find the R-Squared."""

lr_predictions = lr_model.transform(test_df)
lr_predictions.select("prediction", "medv", "features").show(5)

from pyspark.ml.evaluation import RegressionEvaluator
lr_evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="medv", metricName="r2")
print("R Squared (R2) on test data = %g" % lr_evaluator.evaluate(lr_predictions))

"""And now the RMSE."""

test_result = lr_model.evaluate(test_df)
print('Root Mean Squared Error (RMSE) on test data = %g' % test_result.rootMeanSquaredError)

"""And now some analytics from the training along with residuals. """

print("Number of Iterations: %d" % trainingSummary.totalIterations)
print("Objective History %s" % str(trainingSummary.objectiveHistory))
trainingSummary.residuals.show(5)

"""Here we will use this model to create some predictions."""

predictions = lr_model.transform(test_df)
predictions.select("prediction", "medv", 'features').show(5)

"""Creating a decision tree regression to determine which features are the most important. More about decision tree learning can be seen at: https://en.wikipedia.org/wiki/Decision_tree_learning. """

from pyspark.ml.regression import DecisionTreeRegressor
dt = DecisionTreeRegressor(featuresCol='features', labelCol='medv')
dt_model = dt.fit(train_df)
dt_prediction = dt_model.transform(test_df)
dt_evaluator = RegressionEvaluator(labelCol="medv", predictionCol="prediction", metricName="rmse")
rmse = dt_evaluator.evaluate(dt_prediction)
print("Root Mean Square Error (RMSE) on test data = % g" % rmse)

dt_model.featureImportances

house_df.take(1)

"""The number of rooms appears to be the most important feature to predict median value. Does this match your analysis from above?

And now a Gradient-boosted tree regression. See https://en.wikipedia.org/wiki/Gradient_boosting for a more detailed description of gradient boosting.
"""

from pyspark.ml.regression import GBTRegressor
gbt = GBTRegressor(featuresCol= 'features', labelCol = 'medv', maxIter=10)
gbt_model = gbt.fit(train_df)
gbt_predictions = gbt_model.transform(test_df)
gbt_predictions.select('prediction', 'medv', 'features').show(5)

gbt_evaluator = RegressionEvaluator(labelCol="medv", predictionCol='prediction', metricName='rmse')
rmse = gbt_evaluator.evaluate(gbt_predictions)
print("Root Mean Squared Error (RMSE) on test data = %g" % rmse)

"""Which regression method (linear, decision tree, gradient-boosted) worked best with your test data?

Now that you've done this exercise on the Google Colab notebook environment. We'll look at creating a similar notebook environment using OpenStack VMs. After we create that VM we'll repeat the steps in this exercise. I challenenge you to find a new data set, one that might be relevant to your research.
"""



